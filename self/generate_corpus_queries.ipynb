{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files under the root directory:\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\add-bindings-existing-function\\cf374970-651e-b078-cf54-7a1117d11405\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\analyze-telemetry-data\\1dcc4d8f-79e5-35b9-7fc7-750cab3fc03d\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\bring-dependency-to-functions\\350da97f-fa07-6ab9-6df0-548f2c00a651\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\configure-encrypt-at-rest-using-cmk\\ed05840e-ea3b-e367-77b5-f80ba8f6d018\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\configure-monitoring\\bcf023ad-012c-e840-ef25-dbf13657d751\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\configure-networking-how-to\\6152970d-01ac-18a7-1a87-b5e89168f86a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\consumption-plan\\91012c20-a7d6-4442-8c01-d0529994ade0\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\container-concepts\\5cae03dd-1d3c-0562-0f98-96e31f469f45\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-arc-cli\\9d3bcdbe-e7b3-6787-cbcd-103c1dd09686\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-arc-custom-container\\9b90f59f-34f0-fc9a-bdb1-f37cbd425331\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-azure-developer-cli\\a3ed70a7-c40e-b750-39ab-95f9eaf33715\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-cli-csharp\\fa74d715-575a-e7bb-0f77-5dda3a567767\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-cli-java\\1d622912-7821-bfb4-11e4-72169ac12368\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-cli-node\\43318d61-fd77-6ebb-78f3-e88bf86d482e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-cli-powershell\\30bd7cb9-e3aa-9f2d-f3b4-56cf5357467d\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-cli-python\\79f0ae77-7c48-ae83-2698-eaaea9eb2ef2\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-cli-typescript\\7906f149-afe0-bb3b-2ca7-904bd5ab256b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-vs-code-csharp\\62f699a7-275f-8332-26f7-9cc416e34b5b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-vs-code-java\\ca22e32f-d4b2-b2fa-e696-a7c9370a2ccb\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-vs-code-node\\7ecc8797-7e20-13bc-fb1d-4a4916272ca0\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-vs-code-other\\6a9e27d5-103d-84fe-c7f1-6d7afbc52fd6\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-vs-code-powershell\\dadbda4a-e801-f99e-18b1-e1d4dd111c86\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-vs-code-python\\13162222-7bce-8277-d7be-dd92fa4eed96\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-vs-code-typescript\\6fd9324e-dde3-8a06-12da-fc8bef66bbd6\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-first-function-vs-code-web\\f0614e62-3aa4-f45b-310f-8221decd4898\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\create-resources-azure-powershell\\bfc79bca-d1c8-5912-c0ab-4de04787a646\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\dedicated-plan\\3cac48f1-a41f-f5c3-93f9-ce052e3e0acb\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\deployment-zip-push\\1e5fb7b6-7ffb-a690-cd8f-8672f7c38aca\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\develop-python-worker-extensions\\dc385d8e-f6f2-817e-7896-51d27e78111e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\disable-function\\0d5cd213-f2d1-e5c8-f485-7b13a251e27f\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\dotnet-isolated-in-process-differences\\a73dc11f-a9bc-a240-6220-c86d525721d2\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\dotnet-isolated-process-guide\\2bad2db5-5cd7-ebd7-0d05-03ed117b6330\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-azure-storage-provider\\f7076227-1be1-bf72-0297-c2aef74c01cc\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-best-practice-reference\\d34f87c1-47a7-207e-c2fe-4a22b0a41353\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-billing\\e99d0994-271a-8227-0b07-b5d3cb5f104d\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-bindings\\5fb958fe-a192-31ae-6e4d-39987049b887\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-cloud-backup\\2c204e1f-5095-6f7e-d2c3-e0d2927232b3\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-code-constraints\\cb9016ff-7b0a-fc3f-3981-29efd386ee6b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-configure-durable-functions-with-credentials\\fa226d2a-4a18-3e04-f76d-1764488af67d\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-configure-managed-identity\\9ca9728d-ba33-e32f-757a-7e8e79c7b585\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-create-first-csharp\\2d357af8-d7e3-6679-24ed-d887d0b46e13\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-create-portal\\417a31a8-ccb2-31bb-4e69-854580e2293a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-custom-orchestration-status\\52fdd3e9-b916-d71a-7d1b-6b77fe941aad\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-diagnostics\\6359a97e-de26-4b85-cdfe-d59ba13717a5\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-disaster-recovery-geo-distribution\\f833fcb1-b9ac-fc4f-73af-621c48236611\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-dotnet-entities\\ac7a376c-acee-e440-b477-180b2f0c947e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-dotnet-isolated-overview\\f342b1e6-9f4f-8f25-2fbe-b23d12b2cf8b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-entities\\4001488c-8280-7d5a-eb77-f7e45bfdf4fb\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-error-handling\\c61a3bad-8095-3bb1-7d3f-97416a464010\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-eternal-orchestrations\\56da57d2-85a1-2dbc-c7bc-a9d848a93363\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-event-publishing\\b17dd497-052b-9394-a1c4-3824a217e0ad\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-extension-upgrade\\271a5855-3a53-57ad-fdd7-d5f306bdd053\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-external-events\\a553bed4-f05f-1aa2-6731-914886cde3c1\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-http-api\\3c0639c7-4770-285d-b182-ca4bce22ff92\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-http-features\\3e1d65d2-4965-58d3-66f7-74cf8dd7a109\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-instance-management\\330848a4-aee2-074e-a2e9-1cf2c9fd7e1c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-isolated-create-first-csharp\\b3626c1c-ea1e-10c5-0675-8bef3ee49198\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-monitor\\339ae936-a7fe-7969-137e-3b327fce12b1\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-monitor-python\\90759358-a0fb-8612-10aa-9caf984aae3c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-node-model-upgrade\\5b1dc364-7ee6-1797-2c25-294b2e54ad59\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-orchestrations\\2a3d1e26-a19d-bef0-6f68-30994ae124d5\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-overview\\a248fac8-5f27-f4e0-6014-0b084fd79f25\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-packages\\0a1c6df9-07cf-92ef-5236-e7a5248eb997\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-perf-and-scale\\061246a4-677a-43b9-269d-c0ac32eb0e65\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-phone-verification\\1254d07e-4455-b27d-f1f2-bdc54555cc75\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-powershell-v2-sdk-migration-guide\\adac7f34-5f1d-3d53-9049-efa218f11414\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-roslyn-analyzer\\726c3865-81fc-3b83-3c03-fc9e3e70f0d4\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-sequence\\49e3e063-ede9-9d79-2e29-f36b3b2a274a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-serialization-and-persistence\\170caebe-7bce-718b-13df-06819d7de1f4\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-singletons\\128c3c1d-0f65-a063-d289-f712ea5e75cb\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-storage-providers\\3a35d635-d8d7-896f-eedb-6f00de6d58fd\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-sub-orchestrations\\27f8370e-1199-e815-412e-a3a1512f61f1\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-task-hubs\\d4416ea5-39f3-41fe-a510-674545db8282\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-timers\\7bcd67cc-28ee-09c2-5d09-d76f1e1e564f\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-troubleshooting-guide\\b85a7216-0380-378d-eaf4-6626c082cc4e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-types-features-overview\\998595dd-3530-ae8d-4dd6-6a8482f0903c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-unit-testing\\1b6df9f0-c17e-b32f-4b3d-89c51fe5d739\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-versioning\\93e1e0ec-8284-2cea-f1c5-c78286daa2bf\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-versions\\cb57f799-e046-e46a-863b-4370b186d2e4\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-webjobs-sdk\\eecb4403-6abe-c24b-8b05-7555b205706e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\durable-functions-zero-downtime-deployment\\ed670219-b1a8-49a8-d107-15cea8429017\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\function-app-diagnostics\\9170f8c6-5350-5505-dc94-8b12c0f1c927\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\quickstart-java\\61e99f0b-7369-9f11-6b4b-56349e2d9f61\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\quickstart-js-vscode\\88890319-c74d-150f-e826-df419127becc\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\quickstart-mssql\\d6658ec1-5cbd-a04a-643b-5b79f479daf8\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\quickstart-netherite\\c421e4b0-3636-5e64-ef13-d07104d6460c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\quickstart-powershell-vscode\\a89cbd0f-cc89-97df-0cb3-36ef6122d1f0\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\quickstart-python-vscode\\50e60d0a-b75d-5b2f-555c-550b7797b140\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\durable\\quickstart-ts-vscode\\9a63cf8c-8e7b-36c1-a630-8087fdc0fa66\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\event-driven-scaling\\b4805ec5-05ff-83f6-6090-8e2b12e88c17\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\event-grid-how-tos\\fe363ef6-b68a-56f3-82a1-99cf8289c5aa\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\event-messaging-bindings\\4a6e9898-18ad-a627-dcae-0e1e1a6618d8\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\flex-consumption-how-to\\c8d41e79-f330-3f9c-62e7-194f2fe67e3c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\flex-consumption-plan\\0406b783-3788-b2c8-34de-e98c4f05a789\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\function-keys-how-to\\dd291da3-a3a0-3c27-866c-b61c861dfe1e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-add-openai-text-completion\\58eb5735-3204-878f-8144-6bf7a232c8ef\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-add-output-binding-azure-sql-vs-code\\27e34dab-87e7-ab07-fc4c-5d509258d51c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-add-output-binding-cosmos-db-vs-code\\0a1404ee-9801-c03e-bf85-2587cd407d33\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-add-output-binding-storage-queue-cli\\da3e244d-bf1b-3fae-832c-ebce27548154\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-add-output-binding-storage-queue-java\\63921b90-d217-c2a2-7945-128b16a8e2ed\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-add-output-binding-storage-queue-vs\\e967165b-da7a-1904-f647-fd76361b508e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-add-output-binding-storage-queue-vs-code\\7511d557-7c29-5d60-234f-5e31ca026372\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-app-settings\\8378028a-0f0e-311f-57cc-d185f190ce09\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-best-practices\\9be95598-c2b8-8649-9aba-36d7d3a6b57e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-bindings-error-pages\\39905770-6151-046b-52cc-03b30ae897b7\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-cli-samples\\af6df4bd-a70a-6048-f283-9aa44dec412a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-compare-logic-apps-ms-flow-webjobs\\4de26aad-f81d-d14d-adcf-61046feb7d87\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-concurrency\\0f44e44d-9cab-6dda-e188-831f8de7d6f1\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-consumption-costs\\09c177d0-7212-c0ed-3cb9-dd77c7640adb\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-container-apps-hosting\\57188890-a583-be6c-6d9f-f449b83284c0\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-continuous-deployment\\ae34d9ed-c6de-eb2b-96b4-e0d6f52344dc\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-container-registry\\f621a2f9-a2b4-3594-e176-845cf16eb6b6\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-cosmos-db-triggered-function\\00f3d4d1-bbf3-bb5c-a9cc-3b9c278c55e8\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-first-function-bicep\\2d1bb343-0410-1c8c-0d12-a28656f2b149\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-first-function-resource-manager\\9194ef7f-cbe0-c442-a0f4-79b46ed6ecd3\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-first-java-gradle\\d4f3d931-d3ff-933c-04f6-c36aa14b585a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-first-quarkus\\cab0aedd-9b23-7dc5-c32e-6f417d12475c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-function-app-portal\\4cd61ef8-6334-8d9d-bc55-448f2bef786d\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-maven-eclipse\\c15f6e4d-49aa-3ef1-38f7-5e4d97e05f72\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-maven-intellij\\bbccfcf7-2b2a-7744-b982-749b43e61165\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-maven-kotlin-intellij\\42215f6b-3a8b-ed26-092c-a598fb1d3d00\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-private-site-access\\5ec52580-2753-6478-7ddc-9dd4f0913b6f\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-scheduled-function\\0dbcbc18-f624-7ab0-ec6b-b6cf02bba599\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-serverless-api\\70372224-2a11-12c1-133f-0c1df0c77c6e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-storage-blob-triggered-function\\a43af3ae-14e3-91fb-45ee-07390722efd0\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-storage-queue-triggered-function\\05e241ab-b705-e2d3-8871-3b29ec291e08\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-vnet\\4495724d-f35c-b211-2d8e-e79867b5c4e7\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-create-your-first-function-visual-studio\\a60cb6ef-332d-86ab-af36-dba2c1b11af2\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-custom-handlers\\251a27c1-dd86-59ba-b61d-6999ad139708\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-debug-powershell-local\\e35bd4bf-cfdb-4d18-8438-42028754ed05\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-deploy-container\\dbc4bc21-d607-b211-0472-a4141bfe95cc\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-deploy-container-apps\\a47cc076-a894-f887-db90-c4b0a329c985\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-deployment-slots\\bdf4581e-3e86-944a-f75f-9b7172068435\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-deployment-technologies\\6d74e74a-80d9-8c9e-8263-8eb9ebbb41e1\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-develop-local\\3fa6d840-7c14-aeb6-b403-3d1c739938e0\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-develop-vs\\a114085c-4803-ec5c-6bbd-174d90ac6184\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-develop-vs-code\\f07eec0a-b6ae-4368-35a2-e026ef58a10b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-diagnostics\\e8abf85d-d562-ecc5-ac29-e3e2d93e1882\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-dotnet-class-library\\9b63e42d-7b16-f13d-edd3-662f4735e424\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-dotnet-dependency-injection\\b2a17b13-62be-b20c-4e37-ae083b4d2673\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-event-grid-blob-trigger\\2e118fcd-fb59-a698-7c36-c68c4326bae3\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-event-hub-cosmos-db\\15ccd6b8-f57b-a4ea-aaee-9174ccc3cde3\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-get-started\\31194c59-9565-b26b-53c6-d1f7dcddcd63\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-host-json\\9d840768-0b22-a62f-037f-36d24da4adde\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-host-json-v1\\72c71b4d-1775-ecd8-c5f8-d44fcaeb21f5\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-how-to-azure-devops\\f02f536d-d8f2-a1df-784b-c35175589a1d\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-how-to-custom-container\\02613e7b-187b-3030-1e98-f7d15664e79f\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-how-to-github-actions\\0e712ff1-dd11-11e5-f172-1b256a7d1bc7\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-how-to-use-azure-function-app-settings\\eca2af7c-2a7d-fa57-c50b-7817d069eaa7\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-how-to-use-nat-gateway\\f90435e6-2ea2-339d-38a2-f9b0a31dc677\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-hybrid-powershell\\77c0be61-aeb8-30eb-c8a1-e145faaa59eb\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-idempotent\\1e92608d-cbb6-8c6b-2bc2-0286e3c3a696\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-identity-access-azure-sql-with-managed-identity\\072bbc9c-dafb-3b5e-2d2c-d48f8b411f14\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-identity-based-connections-tutorial\\0f78ee9d-51c5-f029-04a8-8cd6e5d19d7b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-identity-based-connections-tutorial-2\\bcd4bf0e-6ebb-c39c-3755-c759db32bd2f\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-infrastructure-as-code\\f0b9f26e-044e-b2f1-be6e-8f0238e9ab4c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-integrate-storage-queue-output-binding\\29406b12-36a3-8e73-f133-2189ebd6b8a6\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-integrate-store-unstructured-data-cosmosdb\\749aaf48-068a-c14f-6ebd-31764b7d6bb2\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-kubernetes-keda\\9563a9d7-4a53-1612-edf6-338de0bb9d75\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-machine-learning-tensorflow\\54aa3ad5-5349-82af-6735-6d0e899edba2\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-manually-run-non-http\\cfe6c342-02cb-2aa4-60f3-f5595355aab9\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-monitor-log-analytics\\57ea4a4c-e145-1ce6-0d91-3c2e0e1805f5\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-monitoring\\7cec6918-7dd8-fb54-c65c-6e795e518c6a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-move-across-regions\\eab095ac-d625-4f3d-701b-eccc2a9de299\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-networking-faq\\f3df511f-ac60-d9c9-4375-907963043a06\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-networking-options\\a616b221-3f9b-8b30-a0c3-a2c27728e3ab\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-node-troubleshoot\\9d1c9afd-e9ec-4cdd-d980-2d22f476f097\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-node-upgrade-v4\\6e5770ba-a862-74e5-e3b8-8b46bd4a0965\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-openapi-definition\\14aa5dfc-0aa7-1e3f-1a0e-fa3cfe8c583a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-overview\\66b9d8b2-282d-ddf5-774f-36cf53e8f014\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-premium-plan\\e76b917a-966d-1f20-db6b-2d0cec51cf2c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-proxies\\225db34f-16ae-7be8-5bea-6cb0442278f7\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-recover-storage-account\\8b28d033-a575-93a9-8078-058f8fb09695\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-reference\\fdbe91fa-c4c1-7209-58de-0e2a4ffffa13\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-reference-csharp\\21070952-9c0b-f679-ef32-3a81bd26b933\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-reference-java\\55b70333-6a4d-14bf-eebf-1179f4ff1242\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-reference-node\\699725ad-5f91-2dca-adb3-b6020bd33457\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-reference-powershell\\4ee8e83e-27fd-914a-f209-07a7b8cf92e1\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-reference-python\\761af614-adc8-11a7-34bd-77067978c46a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-reliable-event-processing\\b382d1a5-39f7-7b8a-c084-3a062de4d3f8\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-run-local\\eb3f063e-2e12-b19d-7167-5c6eb1187cc4\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-scale\\07ca91ee-4cc8-8bc2-5996-005aa6a29b47\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-scenario-database-table-cleanup\\b00cfe12-4019-2ace-982d-b0648b6bed2a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-scenarios\\299d97b1-b927-678f-1528-15c9302216c2\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-target-based-scaling\\6ad06175-29a3-898e-ff69-c8d79668971a\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-triggers-bindings\\8b032441-76ed-ac63-abbb-4b3eecde418b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-twitter-email\\75d26354-6534-9ac0-cb16-2e9f63d62dd6\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\functions-versions\\ff0e3c17-abeb-74e7-ebe1-552bd9e94aff\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\ip-addresses\\6a894d6a-a631-fe2b-315c-d9f49f90cede\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\language-support-policy\\6c45569c-5d08-2a32-7484-90747d6ea76e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\legacy-proxies\\bc25c22b-2709-0820-d01c-f9b00c726118\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\machine-learning-pytorch\\190a69e8-1c6e-b4e3-4798-6fd915058c4c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\manage-connections\\677c0ed8-08e8-b205-7378-127e8f6e6acb\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\migrate-cosmos-db-version-3-version-4\\73123f4e-d7aa-3f04-05f8-9a2b236e96ae\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\migrate-dotnet-to-isolated-model\\8af6b738-7017-145b-dd8c-b0b688f3d15e\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\migrate-service-bus-version-4-version-5\\f41c8da3-c636-7d21-bb3a-142d8670bdbd\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\migrate-version-1-version-4\\6d916b76-70ac-a820-9e9d-e70e4984f542\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\migrate-version-3-version-4\\59231049-37e4-7fb9-4c25-62146e0b27e2\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\monitor-functions\\4ff9dd47-f557-74aa-cbdb-a2b9cd3869e9\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\openapi-apim-integrate-visual-studio\\cd334761-9208-6a07-dc37-77c62ef2696b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\opentelemetry-howto\\bdd9553f-77f6-12d7-a8df-1b976a01d148\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\performance-reliability\\9a573092-679d-0a72-5d66-83b4fbc9faf8\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\python-memory-profiler-reference\\0419e932-ef72-ef19-2b21-d8bc31ad2247\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\python-scale-performance-reference\\0024cd39-efac-f7db-60a1-4212896dee9f\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\recover-python-functions\\ff3a7488-4a3a-0d76-29ab-93447cacc2ec\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\run-functions-from-deployment-package\\e9b17ee5-fd8a-54c5-0be5-f12faab65827\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\scripts\\functions-cli-create-app-service-plan\\69b97ccc-d295-b773-31c1-79c3b106c7d8\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\scripts\\functions-cli-create-function-app-connect-to-cosmos-db\\daf3ac8a-ab11-cf2e-f2ba-6336b328d965\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\scripts\\functions-cli-create-function-app-connect-to-storage-account\\8944dd30-e935-6c29-2733-b0aca492c73c\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\scripts\\functions-cli-create-function-app-github-continuous\\a7d17bc5-c8a4-3aef-8a18-f0a8e59850f1\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\scripts\\functions-cli-create-premium-plan\\af79489a-0d08-4a3c-df2b-348a747f8ba9\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\scripts\\functions-cli-create-serverless\\fcadfcb8-ef83-af1b-fcc7-dba46da34482\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\scripts\\functions-cli-create-serverless-python\\960dbad3-63b3-34a9-c03e-c5acdc3fd075\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\scripts\\functions-cli-mount-files-storage-linux\\175b8a02-0712-4ab1-10fe-fe36e7272296\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\security-concepts\\4cb9f742-47f8-06b7-5bd9-a58b029805a5\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\set-runtime-version\\3b4517b5-e4a2-21ed-4b2a-e80ea3021a1b\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\shift-expressjs\\6137161e-05ca-f525-8e0a-d97642ce0ed9\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\start-stop-vms\\deploy\\8f77fdf8-4ef1-30a7-6b22-65c933482a25\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\start-stop-vms\\manage\\373eba05-6687-7087-211a-2ce1c1ea49cc\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\start-stop-vms\\overview\\b2cf17f9-2cfc-1d2b-2d0e-9ff179c84e8d\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\start-stop-vms\\pre-actions\\41a22f9d-26fa-32a3-e1f7-aab8deeada06\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\start-stop-vms\\remove\\6f229a69-446e-4a5d-eff5-377b55d2deb4\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\start-stop-vms\\troubleshoot\\c3749532-fa76-fd54-378b-f9f77684843f\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\storage-considerations\\c53f22ee-98f9-1942-7f42-f966217d3e74\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\streaming-logs\\405b2ca1-8db0-0692-eccc-e79601d58acd\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\supported-languages\\9f3928f5-e001-cc55-3ac3-3b1483cb0614\n",
      "./source_of_data/learncontent/azure-functions-fullpage\\update-language-versions\\5be4c615-fd11-e1ab-9d99-7b3ba259cf0c\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_all_files(root_dir):\n",
    "    \"\"\"Lists all files under all subfolders of a given root directory.\n",
    "\n",
    "    Args:\n",
    "        root_dir: The path to the root directory.\n",
    "\n",
    "    Returns:\n",
    "        A list of all file paths found under the root directory.  Returns an empty list if the root directory doesn't exist or is empty.\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    try:\n",
    "        for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if not filename.endswith('-000'):\n",
    "                    all_files.append(os.path.join(dirpath, filename))\n",
    "        return all_files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Directory '{root_dir}' not found.\")\n",
    "        return []\n",
    "    except OSError as e:\n",
    "        print(f\"Error accessing directory: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "root_directory = \"./source_of_data/learncontent/azure-functions-fullpage\" \n",
    "all_files_list = list_all_files(root_directory)\n",
    "\n",
    "if all_files_list:\n",
    "    print(\"All files under the root directory:\")\n",
    "    for file_path in all_files_list:\n",
    "        print(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first 10 files from the list\n",
    "\n",
    "corpus = []\n",
    "file_list = all_files_list[:50]\n",
    "for file_path in file_list:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        json_content = json.loads(content)\n",
    "        corpus.append({\"_id\": json_content['id'], \"title\": json_content['title'], \"text\": json_content['content'], \"metadata\": {}})\n",
    "        # print(f\"Content of {file_path}:\\n{content}\\n\")\n",
    "# save the corpus to a jsonl file\n",
    "with open('./datasets/learncorpus/corpus.jsonl', 'w') as f:\n",
    "    for item in corpus:\n",
    "        # convert item to json \n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': 'cf374970-651e-b078-cf54-7a1117d11405',\n",
       "  'title': 'Connect functions to other Azure services',\n",
       "  'text': '# Connect functions to Azure services using bindings (programming-language-csharp)\\r\\n\\r\\nWhen you create a function, language-specific trigger code is added in your project from a set of trigger templates. If you want to connect your function to other services by using input or output bindings, you have to add specific binding definitions in your function. To learn more about bindings, see [Azure Functions triggers and bindings concepts](functions-triggers-bindings).\\r\\n\\r\\n## Local development\\r\\n\\r\\nWhen you develop functions locally, you need to update the function code to add bindings. For languages that use function.json, Visual Studio Code provides tooling to add bindings to a function.\\r\\n\\r\\n### Manually add bindings based on examples\\r\\n\\r\\nWhen adding a binding to an existing function, you need to add binding-specific attributes to the function definition in code.\\r\\n\\r\\nThe following example shows the function definition after adding a [Queue Storage output binding](functions-bindings-storage-queue-output) to an [HTTP triggered function](functions-bindings-http-webhook-trigger):\\r\\n\\r\\n**Isolated process**\\r\\nBecause an HTTP triggered function also returns an HTTP response, the function returns a `MultiResponse` object, which represents both the HTTP and queue output.\\r\\n\\r\\n```csharp\\r\\n[Function(\"HttpExample\")]\\r\\npublic static MultiResponse Run([HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\")] HttpRequest req,\\r\\n    FunctionContext executionContext)\\r\\n{\\r\\n```\\r\\n\\r\\nThis example is the definition of the `MultiResponse` object that includes the output binding:\\r\\n\\r\\n```csharp\\r\\npublic class MultiResponse\\r\\n{\\r\\n    [QueueOutput(\"outqueue\",Connection = \"AzureWebJobsStorage\")]\\r\\n    public string[] Messages { get; set; }\\r\\n    public IActionResult HttpResponse { get; set; }\\r\\n}\\r\\n```\\r\\n\\r\\nWhen applying that example to your own project, you might need to change `HttpRequest` to `HttpRequestData` and `IActionResult` to `HttpResponseData`, depending on if you are using [ASP.NET Core integration](dotnet-isolated-process-guide#aspnet-core-integration) or not.\\r\\n\\r\\n**In-process**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"HttpExample\")]\\r\\npublic static async Task<IActionResult> Run(\\r\\n    [HttpTrigger(AuthorizationLevel.Anonymous, \"get\", \"post\", Route = null)] HttpRequest req, \\r\\n    [Queue(\"outqueue\"),StorageAccount(\"AzureWebJobsStorage\")] ICollector<string> msg, \\r\\n    ILogger log)\\r\\n```\\r\\n\\r\\nMessages are sent to the queue when the function completes. The way you define the output binding depends on your process model. For more information, including links to example binding code that you can refer to, see [Add bindings to a function](add-bindings-existing-function?tabs=csharp#manually-add-bindings-based-on-examples).\\r\\n\\r\\nUse the following table to find examples of specific binding types that you can use to guide you in updating an existing function. First, choose the language tab that corresponds to your project.\\r\\n\\r\\nBinding code for C# depends on the [specific process model](dotnet-isolated-process-guide#benefits-of-the-isolated-worker-model).\\r\\n\\r\\n**Isolated process**\\r\\n\\r\\n| Service | Examples | Samples |\\r\\n| --- | --- | --- |\\r\\n| Blob storage | [Trigger](functions-bindings-storage-blob-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Input](functions-bindings-storage-blob-input?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-storage-blob-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Blob%20Storage&amp;language=C%23) |\\r\\n| Azure Cosmos DB | [Trigger](functions-bindings-cosmosdb-v2-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Input](functions-bindings-cosmosdb-v2-input?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-cosmosdb-v2-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Cosmos%2CCosmos%20DB&amp;language=C%23) |\\r\\n| Azure Data Explorer | [Input](functions-bindings-azure-data-explorer-input?pivots=programming-language-csharp#examples)[Output](functions-bindings-azure-data-explorer-output?pivots=programming-language-csharp#examples) | [Link](https://github.com/Azure/Webjobs.Extensions.Kusto/tree/main/samples/samples-csharp) |\\r\\n| Azure SQL | [Trigger](functions-bindings-azure-sql-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Input](functions-bindings-azure-sql-input?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-azure-sql-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](/en-us/samples/azure-samples/azure-sql-binding-func-dotnet-todo/todo-backend-dotnet-azure-sql-bindings-azure-functions/) |\\r\\n| Event Grid | [Trigger](functions-bindings-event-grid-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-event-grid-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Event%20Grid&amp;language=C%23) |\\r\\n| Event Hubs | [Trigger](functions-bindings-event-hubs-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-event-hubs-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) |\\r\\n| IoT Hub | [Trigger](functions-bindings-event-iot-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-event-iot-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) |\\r\\n| HTTP | [Trigger](functions-bindings-http-webhook-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?language=C%23&amp;filtertext=http) |\\r\\n| Queue storage | [Trigger](functions-bindings-storage-queue-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-storage-queue-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Storage%20Queue&amp;language=C%23) |\\r\\n| RabbitMQ | [Trigger](functions-bindings-rabbitmq-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-rabbitmq-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) |\\r\\n| SendGrid | [Output](functions-bindings-sendgrid?tabs=isolated-process&amp;pivots=programming-language-csharp#example) |\\r\\n| Service Bus | [Trigger](functions-bindings-service-bus-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-service-bus-output?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Service%20Bus%20Queue&amp;language=C%23) |\\r\\n| SignalR | [Trigger](functions-bindings-signalr-service-trigger?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Input](functions-bindings-signalr-service-input?tabs=isolated-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-signalr-service-output?tabs=isolated-process&amp;pivots=programming-language-csharp) |\\r\\n| Table storage | [Input](functions-bindings-storage-table-input?tabs=isolated-process&amp;pivots=programming-language-csharp)[Output](functions-bindings-storage-table-output?tabs=isolated-process&amp;pivots=programming-language-csharp) |\\r\\n| Timer | [Trigger](functions-bindings-timer?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?language=C%23&amp;filtertext=timer) |\\r\\n| Twilio | [Output](functions-bindings-twilio?tabs=isolated-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?language=C%23&amp;filtertext=twilio) |\\r\\n\\r\\n**In-process**\\r\\n\\r\\n| Service | Examples | Samples |\\r\\n| --- | --- | --- |\\r\\n| Blob storage | [Trigger](functions-bindings-storage-blob-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Input](functions-bindings-storage-blob-input?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-storage-blob-output?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Blob%20Storage&amp;language=C%23) |\\r\\n| Azure Cosmos DB | [Trigger](functions-bindings-cosmosdb-v2-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Input](functions-bindings-cosmosdb-v2-input?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-cosmosdb-v2-output?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Cosmos%2CCosmos%20DB&amp;language=C%23) |\\r\\n| Azure Data Explorer | [Input](functions-bindings-azure-data-explorer-input?pivots=programming-language-csharp#examples)[Output](functions-bindings-azure-data-explorer-output?pivots=programming-language-csharp#examples) | [Link](https://github.com/Azure/Webjobs.Extensions.Kusto/tree/main/samples/samples-csharp) |\\r\\n| Azure SQL | [Trigger](functions-bindings-azure-sql-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Input](functions-bindings-azure-sql-input?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-azure-sql-output?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](/en-us/samples/azure-samples/azure-sql-binding-func-dotnet-todo/todo-backend-dotnet-azure-sql-bindings-azure-functions/) |\\r\\n| Event Grid | [Trigger](functions-bindings-event-grid-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-event-grid-output?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Event%20Grid&amp;language=C%23) |\\r\\n| Event Hubs | [Trigger](functions-bindings-event-hubs-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-event-hubs-output?tabs=in-process&amp;pivots=programming-language-csharp#example) |\\r\\n| IoT Hub | [Trigger](functions-bindings-event-iot-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-event-iot-output?tabs=in-process&amp;pivots=programming-language-csharp#example) |\\r\\n| HTTP | [Trigger](functions-bindings-http-webhook-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?language=C%23&amp;filtertext=http) |\\r\\n| Queue storage | [Trigger](functions-bindings-storage-queue-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-storage-queue-output?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Storage%20Queue&amp;language=C%23) |\\r\\n| RabbitMQ | [Trigger](functions-bindings-rabbitmq-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-rabbitmq-output?tabs=in-process&amp;pivots=programming-language-csharp#example) |\\r\\n| SendGrid | [Output](functions-bindings-sendgrid?tabs=in-process&amp;pivots=programming-language-csharp#example) |\\r\\n| Service Bus | [Trigger](functions-bindings-service-bus-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-service-bus-output?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?technology=Service%20Bus%20Queue&amp;language=C%23) |\\r\\n| SignalR | [Trigger](functions-bindings-signalr-service-trigger?tabs=in-process&amp;pivots=programming-language-csharp#example)[Input](functions-bindings-signalr-service-input?tabs=in-process&amp;pivots=programming-language-csharp#example)[Output](functions-bindings-signalr-service-output?tabs=in-process&amp;pivots=programming-language-csharp) |\\r\\n| Table storage | [Input](functions-bindings-storage-table-input?tabs=in-process&amp;pivots=programming-language-csharp)[Output](functions-bindings-storage-table-output?tabs=in-process&amp;pivots=programming-language-csharp) |\\r\\n| Timer | [Trigger](functions-bindings-timer?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?language=C%23&amp;filtertext=timer) |\\r\\n| Twilio | [Output](functions-bindings-twilio?tabs=in-process&amp;pivots=programming-language-csharp#example) | [Link](https://www.serverlesslibrary.net/?language=C%23&amp;filtertext=twilio) |\\r\\n\\r\\n### Visual Studio Code\\r\\n\\r\\nWhen you use Visual Studio Code to develop your function and your function uses a function.json file, the Azure Functions extension can automatically add a binding to an existing function.json file. To learn more, see [Add input and output bindings](functions-develop-vs-code#add-input-and-output-bindings).\\r\\n\\r\\n## Azure portal\\r\\n\\r\\nWhen you develop your functions in the [Azure portal](https://portal.azure.com), you add input and output bindings in the **Integrate** tab for a given function. The new bindings are added to either the function.json file or to the method attributes, depending on your language. The following articles show examples of how to add bindings to an existing function in the portal:\\r\\n\\r\\n- [Queue storage output binding](functions-integrate-storage-queue-output-binding)\\r\\n- [Azure Cosmos DB output binding](functions-integrate-store-unstructured-data-cosmosdb)\\r\\n\\r\\n# Connect functions to Azure services using bindings (programming-language-java)\\r\\n\\r\\nWhen you create a function, language-specific trigger code is added in your project from a set of trigger templates. If you want to connect your function to other services by using input or output bindings, you have to add specific binding definitions in your function. To learn more about bindings, see [Azure Functions triggers and bindings concepts](functions-triggers-bindings).\\r\\n\\r\\n## Local development\\r\\n\\r\\nWhen you develop functions locally, you need to update the function code to add bindings. For languages that use function.json, Visual Studio Code provides tooling to add bindings to a function.\\r\\n\\r\\n### Manually add bindings based on examples\\r\\n\\r\\nWhen adding a binding to an existing function, you need to add binding-specific annotations to the function definition in code.\\r\\n\\r\\nThe following example shows the function definition after adding a [Queue Storage output binding](functions-bindings-storage-queue-output) to an [HTTP triggered function](functions-bindings-http-webhook-trigger):\\r\\n\\r\\n```java\\r\\n@FunctionName(\"HttpExample\")\\r\\npublic HttpResponseMessage run(\\r\\n        @HttpTrigger(name = \"req\", methods = {HttpMethod.GET, HttpMethod.POST}, authLevel = AuthorizationLevel.ANONYMOUS) \\r\\n        HttpRequestMessage<Optional<String>> request, \\r\\n        @QueueOutput(name = \"msg\", queueName = \"outqueue\", \\r\\n        connection = \"AzureWebJobsStorage\") OutputBinding<String> msg, \\r\\n        final ExecutionContext context) {\\r\\n```\\r\\n\\r\\nFor more information, including links to example binding code that you can refer to, see [Add bindings to a function](add-bindings-existing-function?tabs=java#manually-add-bindings-based-on-examples).\\r\\n\\r\\nUse the following table to find examples of specific binding types that you can use to guide you in updating an existing function. First, choose the language tab that corresponds to your project.\\r\\n\\r\\n| Service | Examples | Samples |\\r\\n| --- | --- | --- |\\r\\n| Blob storage | [Trigger](functions-bindings-storage-blob-trigger?pivots=programming-language-java#example)[Input](functions-bindings-storage-blob-input?pivots=programming-language-java#example)[Output](functions-bindings-storage-blob-output?pivots=programming-language-java#example) | [Link](https://www.serverlesslibrary.net/?technology=Blob%20Storage&amp;language=Java) |\\r\\n| Azure Cosmos DB | [Trigger](functions-bindings-cosmosdb-v2-trigger?pivots=programming-language-java#example)[Input](functions-bindings-cosmosdb-v2-input?pivots=programming-language-java#example)[Output](functions-bindings-cosmosdb-v2-output?pivots=programming-language-java#example) | [Link](https://www.serverlesslibrary.net/?technology=Cosmos%2CCosmos%20DB&amp;language=Java) |\\r\\n| Azure Data Explorer | [Input](functions-bindings-azure-data-explorer-input?pivots=programming-language-java#examples)[Output](functions-bindings-azure-data-explorer-output?pivots=programming-language-java#examples) | [Link](https://github.com/Azure/Webjobs.Extensions.Kusto/tree/main/samples/samples-java) |\\r\\n| Azure SQL | [Trigger](functions-bindings-azure-sql-trigger?pivots=programming-language-java#example)[Input](functions-bindings-azure-sql-input?pivots=programming-language-java#example)[Output](functions-bindings-azure-sql-output?pivots=programming-language-java#example) |\\r\\n| Event Grid | [Trigger](functions-bindings-event-grid-trigger?pivots=programming-language-java#example)[Output](functions-bindings-event-grid-output?pivots=programming-language-java#example) | [Link](https://www.serverlesslibrary.net/?technology=Event%20Grid&amp;language=Java) |\\r\\n| Event Hubs | [Trigger](functions-bindings-event-hubs-trigger?pivots=programming-language-java#example)[Output](functions-bindings-event-hubs-output?pivots=programming-language-java#example) |\\r\\n| IoT Hub | [Trigger](functions-bindings-event-iot-trigger?pivots=programming-language-java#example)[Output](functions-bindings-event-iot-output?pivots=programming-language-java#example) |\\r\\n| HTTP | [Trigger](functions-bindings-http-webhook-trigger?pivots=programming-language-java#example) | [Link](https://www.serverlesslibrary.net/?language=Java&amp;filtertext=http) |\\r\\n| Queue storage | [Trigger](functions-bindings-storage-queue-trigger?pivots=programming-language-java#example)[Output](functions-bindings-storage-queue-output?pivots=programming-language-java#example) | [Link](https://www.serverlesslibrary.net/?technology=Storage%20Queue&amp;language=Java) |\\r\\n| RabbitMQ | [Trigger](functions-bindings-rabbitmq-trigger?pivots=programming-language-java#example)[Output](functions-bindings-rabbitmq-output?pivots=programming-language-java#example) |\\r\\n| SendGrid | [Output](functions-bindings-sendgrid?pivots=programming-language-java#example) |\\r\\n| Service Bus | [Trigger](functions-bindings-service-bus-trigger?pivots=programming-language-java#example)[Output](functions-bindings-service-bus-output?pivots=programming-language-java#example) | [Link](https://www.serverlesslibrary.net/?technology=Service%20Bus%20Queue&amp;language=Java) |\\r\\n| SignalR | [Trigger](functions-bindings-signalr-service-trigger?pivots=programming-language-java#example)[Input](functions-bindings-signalr-service-input?pivots=programming-language-java#example)[Output](functions-bindings-signalr-service-output?pivots=programming-language-java) |\\r\\n| Table storage | [Input](functions-bindings-storage-table-input?pivots=programming-language-java)[Output](functions-bindings-storage-table-output?pivots=programming-language-java) |\\r\\n| Timer | [Trigger](functions-bindings-timer?pivots=programming-language-java#example) | [Link](https://www.serverlesslibrary.net/?language=Java&amp;filtertext=timer) |\\r\\n| Twilio | [Output](functions-bindings-twilio?pivots=programming-language-java#example) | [Link](https://www.serverlesslibrary.net/?language=Java&amp;filtertext=twilio) |\\r\\n\\r\\n### Visual Studio Code\\r\\n\\r\\nWhen you use Visual Studio Code to develop your function and your function uses a function.json file, the Azure Functions extension can automatically add a binding to an existing function.json file. To learn more, see [Add input and output bindings](functions-develop-vs-code#add-input-and-output-bindings).\\r\\n\\r\\n## Azure portal\\r\\n\\r\\nWhen you develop your functions in the [Azure portal](https://portal.azure.com), you add input and output bindings in the **Integrate** tab for a given function. The new bindings are added to either the function.json file or to the method attributes, depending on your language. The following articles show examples of how to add bindings to an existing function in the portal:\\r\\n\\r\\n- [Queue storage output binding](functions-integrate-storage-queue-output-binding)\\r\\n- [Azure Cosmos DB output binding](functions-integrate-store-unstructured-data-cosmosdb)\\r\\n\\r\\n# Connect functions to Azure services using bindings (programming-language-javascript)\\r\\n\\r\\nWhen you create a function, language-specific trigger code is added in your project from a set of trigger templates. If you want to connect your function to other services by using input or output bindings, you have to add specific binding definitions in your function. To learn more about bindings, see [Azure Functions triggers and bindings concepts](functions-triggers-bindings).\\r\\n\\r\\n## Local development\\r\\n\\r\\nWhen you develop functions locally, you need to update the function code to add bindings. For languages that use function.json, Visual Studio Code provides tooling to add bindings to a function.\\r\\n\\r\\n### Manually add bindings based on examples\\r\\n\\r\\nWhen adding a binding to an existing function, you need to update the function code and add a definition to the function.json configuration file.\\r\\n\\r\\nThe following example shows the function definition after adding a [Queue Storage output binding](functions-bindings-storage-queue-output) to an [HTTP triggered function](functions-bindings-http-webhook-trigger):\\r\\n\\r\\n**v4**\\r\\nExample binding for Node.js model v4 not yet available.\\r\\n\\r\\n**v3**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"authLevel\": \"function\",\\r\\n      \"type\": \"httpTrigger\",\\r\\n      \"direction\": \"in\",\\r\\n      \"name\": \"req\",\\r\\n      \"methods\": [\"get\", \"post\"]\\r\\n    },\\r\\n    {\\r\\n      \"type\": \"http\",\\r\\n      \"direction\": \"out\",\\r\\n      \"name\": \"res\"\\r\\n    },\\r\\n    {\\r\\n      \"type\": \"queue\",\\r\\n      \"direction\": \"out\",\\r\\n      \"name\": \"msg\",\\r\\n      \"queueName\": \"outqueue\",\\r\\n      \"connection\": \"AzureWebJobsStorage\"\\r\\n    }\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\nThe way you define the output binding depends on the version of your Node.js model. For more information, including links to example binding code that you can refer to, see [Add bindings to a function](add-bindings-existing-function?tabs=javascript#manually-add-bindings-based-on-examples).\\r\\n\\r\\nUse the following table to find examples of specific binding types that you can use to guide you in updating an existing function. First, choose the language tab that corresponds to your project.\\r\\n\\r\\n| Service | Examples | Samples |\\r\\n| --- | --- | --- |\\r\\n| Blob storage | [Trigger](functions-bindings-storage-blob-trigger?pivots=programming-language-javascript#example)[Input](functions-bindings-storage-blob-input?pivots=programming-language-javascript#example)[Output](functions-bindings-storage-blob-output?pivots=programming-language-javascript#example) | [Link](https://www.serverlesslibrary.net/?technology=Blob%20Storage&amp;language=JavaScript) |\\r\\n| Azure Cosmos DB | [Trigger](functions-bindings-cosmosdb-v2-trigger?pivots=programming-language-javascript#example)[Input](functions-bindings-cosmosdb-v2-input?pivots=programming-language-javascript#example)[Output](functions-bindings-cosmosdb-v2-output?pivots=programming-language-javascript#example) | [Link](https://www.serverlesslibrary.net/?technology=Cosmos%2CCosmos%20DB&amp;language=JavaScript) |\\r\\n| Azure Data Explorer | [Input](functions-bindings-azure-data-explorer-input?pivots=programming-language-javascript#examples)[Output](functions-bindings-azure-data-explorer-output?pivots=programming-language-javascript#examples) |\\r\\n| Azure SQL | [Trigger](functions-bindings-azure-sql-trigger?pivots=programming-language-javascript#example)[Input](functions-bindings-azure-sql-input?pivots=programming-language-javascript#example)[Output](functions-bindings-azure-sql-output?pivots=programming-language-javascript#example) | [Link](https://github.com/Azure/Webjobs.Extensions.Kusto/tree/main/samples/samples-node) |\\r\\n| Event Grid | [Trigger](functions-bindings-event-grid-trigger?pivots=programming-language-javascript#example)[Output](functions-bindings-event-grid-output?pivots=programming-language-javascript#example) | [Link](https://www.serverlesslibrary.net/?technology=Event%20Grid&amp;language=JavaScript) |\\r\\n| Event Hubs | [Trigger](functions-bindings-event-hubs-trigger?pivots=programming-language-javascript#example)[Output](functions-bindings-event-hubs-output?pivots=programming-language-javascript#example) |\\r\\n| IoT Hub | [Trigger](functions-bindings-event-iot-trigger?pivots=programming-language-javascript#example)[Output](functions-bindings-event-iot-output?pivots=programming-language-javascript#example) |\\r\\n| HTTP | [Trigger](functions-bindings-http-webhook-trigger?pivots=programming-language-javascript#example) | [Link](https://www.serverlesslibrary.net/?language=JavaScript&amp;filtertext=http) |\\r\\n| Queue storage | [Trigger](functions-bindings-storage-queue-trigger?pivots=programming-language-javascript#example)[Output](functions-bindings-storage-queue-output?pivots=programming-language-javascript#example) | [Link](https://www.serverlesslibrary.net/?technology=Storage%20Queue&amp;language=JavaScript) |\\r\\n| RabbitMQ | [Trigger](functions-bindings-rabbitmq-trigger?pivots=programming-language-javascript#example)[Output](functions-bindings-rabbitmq-output?pivots=programming-language-javascript#example) |\\r\\n| SendGrid | [Output](functions-bindings-sendgrid?pivots=programming-language-javascript#example) |\\r\\n| Service Bus | [Trigger](functions-bindings-service-bus-trigger?pivots=programming-language-javascript#example)[Output](functions-bindings-service-bus-output?pivots=programming-language-javascript#example) | [Link](https://www.serverlesslibrary.net/?technology=Service%20Bus%20Queue&amp;language=JavaScript) |\\r\\n| SignalR | [Trigger](functions-bindings-signalr-service-trigger?pivots=programming-language-javascript#example)[Input](functions-bindings-signalr-service-input?pivots=programming-language-javascript#example)[Output](functions-bindings-signalr-service-output?pivots=programming-language-javascript) |\\r\\n| Table storage | [Input](functions-bindings-storage-table-input?pivots=programming-language-javascript)[Output](functions-bindings-storage-table-output?pivots=programming-language-javascript) |\\r\\n| Timer | [Trigger](functions-bindings-timer?pivots=programming-language-javascript#example) | [Link](https://www.serverlesslibrary.net/?language=JavaScript&amp;filtertext=timer) |\\r\\n| Twilio | [Output](functions-bindings-twilio?pivots=programming-language-javascript#example) | [Link](https://www.serverlesslibrary.net/?language=JavaScript&amp;filtertext=twilio) |\\r\\n\\r\\n### Visual Studio Code\\r\\n\\r\\nWhen you use Visual Studio Code to develop your function and your function uses a function.json file, the Azure Functions extension can automatically add a binding to an existing function.json file. To learn more, see [Add input and output bindings](functions-develop-vs-code#add-input-and-output-bindings).\\r\\n\\r\\n## Azure portal\\r\\n\\r\\nWhen you develop your functions in the [Azure portal](https://portal.azure.com), you add input and output bindings in the **Integrate** tab for a given function. The new bindings are added to either the function.json file or to the method attributes, depending on your language. The following articles show examples of how to add bindings to an existing function in the portal:\\r\\n\\r\\n- [Queue storage output binding](functions-integrate-storage-queue-output-binding)\\r\\n- [Azure Cosmos DB output binding](functions-integrate-store-unstructured-data-cosmosdb)\\r\\n\\r\\n# Connect functions to Azure services using bindings (programming-language-powershell)\\r\\n\\r\\nWhen you create a function, language-specific trigger code is added in your project from a set of trigger templates. If you want to connect your function to other services by using input or output bindings, you have to add specific binding definitions in your function. To learn more about bindings, see [Azure Functions triggers and bindings concepts](functions-triggers-bindings).\\r\\n\\r\\n## Local development\\r\\n\\r\\nWhen you develop functions locally, you need to update the function code to add bindings. For languages that use function.json, Visual Studio Code provides tooling to add bindings to a function.\\r\\n\\r\\n### Manually add bindings based on examples\\r\\n\\r\\nWhen adding a binding to an existing function, you need to update the function code and add a definition to the function.json configuration file.\\r\\n\\r\\nThe following example shows the function definition after adding a [Queue Storage output binding](functions-bindings-storage-queue-output) to an [HTTP triggered function](functions-bindings-http-webhook-trigger):\\r\\n\\r\\n```powershell\\r\\n$outputMsg = $name\\r\\nPush-OutputBinding -name msg -Value $outputMsg\\r\\n```\\r\\n\\r\\nFor more information, including links to example binding code that you can refer to, see [Add bindings to a function](add-bindings-existing-function?tabs=powershell#manually-add-bindings-based-on-examples).\\r\\n\\r\\nUse the following table to find examples of specific binding types that you can use to guide you in updating an existing function. First, choose the language tab that corresponds to your project.\\r\\n\\r\\n| Service | Examples | Samples |\\r\\n| --- | --- | --- |\\r\\n| Blob storage | [Trigger](functions-bindings-storage-blob-trigger?pivots=programming-language-powershell#example)[Input](functions-bindings-storage-blob-input?pivots=programming-language-powershell#example)[Output](functions-bindings-storage-blob-output?pivots=programming-language-powershell#example) | [Link](https://www.serverlesslibrary.net/?technology=Blob%20Storage&amp;language=PowerShell) |\\r\\n| Azure Cosmos DB | [Trigger](functions-bindings-cosmosdb-v2-trigger?pivots=programming-language-powershell#example)[Input](functions-bindings-cosmosdb-v2-input?pivots=programming-language-powershell#example)[Output](functions-bindings-cosmosdb-v2-output?pivots=programming-language-powershell#example) | [Link](https://www.serverlesslibrary.net/?technology=Cosmos%2CCosmos%20DB&amp;language=PowerShell) |\\r\\n| Azure SQL | [Trigger](functions-bindings-azure-sql-trigger?pivots=programming-language-powershell#example)[Input](functions-bindings-azure-sql-input?pivots=programming-language-powershell#example)[Output](functions-bindings-azure-sql-output?pivots=programming-language-powershell#example) |\\r\\n| Event Grid | [Trigger](functions-bindings-event-grid-trigger?pivots=programming-language-powershell#example)[Output](functions-bindings-event-grid-output?pivots=programming-language-powershell#example) | [Link](https://www.serverlesslibrary.net/?technology=Event%20Grid&amp;language=PowerShell) |\\r\\n| Event Hubs | [Trigger](functions-bindings-event-hubs-trigger?pivots=programming-language-powershell#example)[Output](functions-bindings-event-hubs-output?pivots=programming-language-powershell#example) |\\r\\n| IoT Hub | [Trigger](functions-bindings-event-iot-trigger?pivots=programming-language-powershell#example)[Output](functions-bindings-event-iot-output?pivots=programming-language-powershell#example) |\\r\\n| HTTP | [Trigger](functions-bindings-http-webhook-trigger?pivots=programming-language-powershell#example) | [Link](https://www.serverlesslibrary.net/?language=PowerShell&amp;filtertext=http) |\\r\\n| Queue storage | [Trigger](functions-bindings-storage-queue-trigger?pivots=programming-language-powershell#example)[Output](functions-bindings-storage-queue-output?pivots=programming-language-powershell#example) | [Link](https://www.serverlesslibrary.net/?technology=Storage%20Queue&amp;language=PowerShell) |\\r\\n| RabbitMQ | [Trigger](functions-bindings-rabbitmq-trigger?pivots=programming-language-powershell#example)[Output](functions-bindings-rabbitmq-output?pivots=programming-language-powershell#example) |\\r\\n| SendGrid | [Output](functions-bindings-sendgrid?pivots=programming-language-powershell#example) |\\r\\n| Service Bus | [Trigger](functions-bindings-service-bus-trigger?pivots=programming-language-powershell#example)[Output](functions-bindings-service-bus-output?pivots=programming-language-powershell#example) | [Link](https://www.serverlesslibrary.net/?technology=Service%20Bus%20Queue&amp;language=PowerShell) |\\r\\n| SignalR | [Trigger](functions-bindings-signalr-service-trigger?pivots=programming-language-powershell#example)[Input](functions-bindings-signalr-service-input?pivots=programming-language-powershell#example)[Output](functions-bindings-signalr-service-output?pivots=programming-language-powershell) |\\r\\n| Table storage | [Input](functions-bindings-storage-table-input?pivots=programming-language-powershell)[Output](functions-bindings-storage-table-output?pivots=programming-language-powershell) |\\r\\n| Timer | [Trigger](functions-bindings-timer?pivots=programming-language-powershell#example) | [Link](https://www.serverlesslibrary.net/?language=PowerShell&amp;filtertext=timer) |\\r\\n',\n",
       "  'metadata': {}},\n",
       " {'_id': '1dcc4d8f-79e5-35b9-7fc7-750cab3fc03d',\n",
       "  'title': 'Analyze Azure Functions telemetry in Application Insights',\n",
       "  'text': '# Analyze Azure Functions telemetry in Application Insights\\r\\n\\r\\nAzure Functions integrates with Application Insights to better enable you to monitor your function apps. Application Insights collects telemetry data generated by your function app, including information your app writes to logs. Application Insights integration is typically enabled when your function app is created. If your function app doesn\\'t have the instrumentation key set, you must first [enable Application Insights integration](configure-monitoring#enable-application-insights-integration).\\r\\n\\r\\nBy default, the data collected from your function app is stored in Application Insights. In the [Azure portal](https://portal.azure.com), Application Insights provides an extensive set of visualizations of your telemetry data. You can drill into error logs and query events and metrics. This article provides basic examples of how to view and query your collected data. To learn more about exploring your function app data in Application Insights, see [What is Application Insights?](/en-us/azure/azure-monitor/app/app-insights-overview).\\r\\n\\r\\nTo be able to view Application Insights data from a function app, you must have at least Contributor role permissions on the function app. You also need to have the [Monitoring Reader permission](/en-us/azure/azure-monitor/roles-permissions-security#monitoring-reader) on the Application Insights instance. You have these permissions by default for any function app and Application Insights instance that you create.\\r\\n\\r\\nTo learn more about data retention and potential storage costs, see [Data collection, retention, and storage in Application Insights](/en-us/previous-versions/azure/azure-monitor/app/data-retention-privacy).\\r\\n\\r\\n## Viewing telemetry in Monitor tab\\r\\n\\r\\nWith [Application Insights integration enabled](configure-monitoring#enable-application-insights-integration), you can view telemetry data in the **Monitor** tab.\\r\\n\\r\\n1. In the function app page, select a function that has run at least once after Application Insights was configured. Then, select **Monitor** from the left pane. Select **Refresh** periodically, until the list of function invocations appears.\\r\\n\\r\\n    Note\\r\\n\\r\\n    It can take up to five minutes for the list to appear while the telemetry client batches data for transmission to the server. The delay doesn\\'t apply to the [Live Metrics Stream](/en-us/azure/azure-monitor/app/live-stream). That service connects to the Functions host when you load the page, so logs are streamed directly to the page.\\r\\n2. To see the logs for a particular function invocation, select the **Date (UTC)** column link for that invocation. The logging output for that invocation appears in a new page.\\r\\n3. Choose **Run in Application Insights** to view the source of the query that retrieves the Azure Monitor log data in Azure Log. If this is your first time using Azure Log Analytics in your subscription, you\\'re asked to enable it.\\r\\n4. After you enable Log Analytics, the following query is displayed. You can see that the query results are limited to the last 30 days (`where timestamp > ago(30d)`), and the results show no more than 20 rows (`take 20`). In contrast, the invocation details list for your function is for the last 30 days with no limit.\\r\\n\\r\\nFor more information, see Query telemetry data later in this article.\\r\\n\\r\\n## View telemetry in Application Insights\\r\\n\\r\\nTo open Application Insights from a function app in the [Azure portal](https://portal.azure.com):\\r\\n\\r\\n1. Browse to your function app in the portal.\\r\\n2. Select **Application Insights** under **Settings** in the left page.\\r\\n3. If this is your first time using Application Insights with your subscription, you\\'ll be prompted to enable it. To do this, select **Turn on Application Insights**, and then select **Apply** on the next page.\\r\\n\\r\\nFor information about how to use Application Insights, see the [Application Insights documentation](/en-us/azure/azure-monitor/app/app-insights-overview). This section shows some examples of how to view data in Application Insights. If you\\'re already familiar with Application Insights, you can go directly to [the sections about how to configure and customize the telemetry data](configure-monitoring#configure-log-levels).\\r\\n\\r\\nThe following areas of Application Insights can be helpful when evaluating the behavior, performance, and errors in your functions:\\r\\n\\r\\n| Investigate | Description |\\r\\n| --- | --- |\\r\\n| **[Failures](/en-us/azure/azure-monitor/app/asp-net-exceptions)** | Create charts and alerts based on function failures and server exceptions. The **Operation Name** is the function name. Failures in dependencies aren\\'t shown unless you implement custom telemetry for dependencies. |\\r\\n| **[Performance](/en-us/azure/azure-monitor/app/performance-counters)** | Analyze performance issues by viewing resource utilization and throughput per **Cloud role instances**. This performance data can be useful for debugging scenarios where functions are bogging down your underlying resources. |\\r\\n| **[Metrics](/en-us/azure/azure-monitor/essentials/metrics-charts)** | Create charts and alerts that are based on metrics. Metrics include the number of function invocations, execution time, and success rates. |\\r\\n| **[Live Metrics](/en-us/azure/azure-monitor/app/live-stream)** | View metrics data as it\\'s created in near real time. |\\r\\n\\r\\n## Query telemetry data\\r\\n\\r\\n[Application Insights Analytics](/en-us/azure/azure-monitor/logs/log-query-overview) gives you access to all telemetry data in the form of tables in a database. Analytics provides a query language for extracting, manipulating, and visualizing the data.\\r\\n\\r\\nChoose **Logs** to explore or query for logged events.\\r\\n\\r\\nHere\\'s a query example that shows the distribution of requests per worker over the last 30 minutes.\\r\\n\\r\\n```kusto\\r\\nrequests\\r\\n| where timestamp > ago(30m) \\r\\n| summarize count() by cloud_RoleInstance, bin(timestamp, 1m)\\r\\n| render timechart\\r\\n```\\r\\n\\r\\nThe tables that are available are shown in the **Schema** tab on the left. You can find data generated by function invocations in the following tables:\\r\\n\\r\\n| Table | Description |\\r\\n| --- | --- |\\r\\n| **traces** | Logs created by the runtime, scale controller, and traces from your function code. For Flex Consumption plan hosting, `traces` also includes logs created during code deployment. |\\r\\n| **requests** | One request for each function invocation. |\\r\\n| **exceptions** | Any exceptions thrown by the runtime. |\\r\\n| **customMetrics** | The count of successful and failing invocations, success rate, and duration. |\\r\\n| **customEvents** | Events tracked by the runtime, for example: HTTP requests that trigger a function. |\\r\\n| **performanceCounters** | Information about the performance of the servers that the functions are running on. |\\r\\n\\r\\nThe other tables are for availability tests, and client and browser telemetry. You can implement custom telemetry to add data to them.\\r\\n\\r\\nWithin each table, some of the Functions-specific data is in a `customDimensions` field. For example, the following query retrieves all traces that have log level `Error`.\\r\\n\\r\\n```kusto\\r\\ntraces \\r\\n| where customDimensions.LogLevel == \"Error\"\\r\\n```\\r\\n\\r\\nThe runtime provides the `customDimensions.LogLevel` and `customDimensions.Category` fields. You can provide additional fields in logs that you write in your function code. For an example in C#, see [Structured logging](functions-dotnet-class-library#structured-logging) in the .NET class library developer guide.\\r\\n\\r\\n## Query function invocations\\r\\n\\r\\nEvery function invocation is assigned a unique ID. `InvocationId` is included in the custom dimension and can be used to correlate all the logs from a particular function execution.\\r\\n\\r\\n```kusto\\r\\ntraces\\r\\n| project customDimensions[\"InvocationId\"], message\\r\\n```\\r\\n\\r\\n## Telemetry correlation\\r\\n\\r\\nLogs from different functions can be correlated using `operation_Id`. Use the following query to return all the logs for a specific logical operation.\\r\\n\\r\\n```kusto\\r\\ntraces\\r\\n| where operation_Id == \\'45fa5c4f8097239efe14a2388f8b4e29\\'\\r\\n| project timestamp, customDimensions[\"InvocationId\"], message\\r\\n| order by timestamp\\r\\n```\\r\\n\\r\\n## Sampling percentage\\r\\n\\r\\nSampling configuration can be used to reduce the volume of telemetry. Use the following query to determine if sampling is operational or not. If you see that `RetainedPercentage` for any type is less than 100, then that type of telemetry is being sampled.\\r\\n\\r\\n```kusto\\r\\nunion requests,dependencies,pageViews,browserTimings,exceptions,traces\\r\\n| where timestamp > ago(1d)\\r\\n| summarize RetainedPercentage = 100/avg(itemCount) by bin(timestamp, 1h), itemType\\r\\n```\\r\\n\\r\\n## Query scale controller logs\\r\\n\\r\\n*This feature is in preview.*\\r\\n\\r\\nAfter enabling both [scale controller logging](configure-monitoring#configure-scale-controller-logs) and [Application Insights integration](configure-monitoring#enable-application-insights-integration), you can use the Application Insights log search to query for the emitted scale controller logs. Scale controller logs are saved in the `traces` collection under the **ScaleControllerLogs** category.\\r\\n\\r\\nThe following query can be used to search for all scale controller logs for the current function app within the specified time period:\\r\\n\\r\\n```kusto\\r\\ntraces \\r\\n| extend CustomDimensions = todynamic(tostring(customDimensions))\\r\\n| where CustomDimensions.Category == \"ScaleControllerLogs\"\\r\\n```\\r\\n\\r\\nThe following query expands on the previous query to show how to get only logs indicating a change in scale:\\r\\n\\r\\n```kusto\\r\\ntraces \\r\\n| extend CustomDimensions = todynamic(tostring(customDimensions))\\r\\n| where CustomDimensions.Category == \"ScaleControllerLogs\"\\r\\n| where message == \"Instance count changed\"\\r\\n| extend Reason = CustomDimensions.Reason\\r\\n| extend PreviousInstanceCount = CustomDimensions.PreviousInstanceCount\\r\\n| extend NewInstanceCount = CustomDimensions.CurrentInstanceCount\\r\\n```\\r\\n\\r\\n## Query Flex Consumption code deployment logs\\r\\n\\r\\nThe following query can be used to search for all code deployment logs for the current function app within the specified time period:\\r\\n\\r\\n```kusto\\r\\ntraces\\r\\n| extend deploymentId = customDimensions.deploymentId\\r\\n| where deploymentId != \\'\\'\\r\\n| project timestamp, deploymentId, message, severityLevel, customDimensions, appName\\r\\n```\\r\\n\\r\\n## Consumption plan-specific metrics\\r\\n\\r\\nWhen running in a [Consumption plan](consumption-plan), the execution *cost* of a single function execution is measured in *GB-seconds*. Execution cost is calculated by combining its memory usage with its execution time. To learn more, see [Estimating Consumption plan costs](functions-consumption-costs).\\r\\n\\r\\nThe following telemetry queries are specific to metrics that impact the cost of running functions in the Consumption plan.\\r\\n\\r\\n#### Determine memory usage\\r\\n\\r\\nUnder **Monitoring**, select **Logs (Analytics)**, then copy the following telemetry query and paste it into the query window and select **Run**. This query returns the total memory usage at each sampled time.\\r\\n\\r\\n```\\r\\nperformanceCounters\\r\\n| where name == \"Private Bytes\"\\r\\n| project timestamp, name, value\\r\\n```\\r\\n\\r\\nThe results look like the following example:\\r\\n\\r\\n| timestamp [UTC] | name | value |\\r\\n| --- | --- | --- |\\r\\n| 9/12/2019, 1:05:14.947 AM | Private Bytes | 209,932,288 |\\r\\n| 9/12/2019, 1:06:14.994 AM | Private Bytes | 212,189,184 |\\r\\n| 9/12/2019, 1:06:30.010 AM | Private Bytes | 231,714,816 |\\r\\n| 9/12/2019, 1:07:15.040 AM | Private Bytes | 210,591,744 |\\r\\n| 9/12/2019, 1:12:16.285 AM | Private Bytes | 216,285,184 |\\r\\n| 9/12/2019, 1:12:31.376 AM | Private Bytes | 235,806,720 |\\r\\n\\r\\n#### Determine duration\\r\\n\\r\\nAzure Monitor tracks metrics at the resource level, which for Functions is the function app. Application Insights integration emits metrics on a per-function basis. Here\\'s an example analytics query to get the average duration of a function:\\r\\n\\r\\n```\\r\\ncustomMetrics\\r\\n| where name contains \"Duration\"\\r\\n| extend averageDuration = valueSum / valueCount\\r\\n| summarize averageDurationMilliseconds=avg(averageDuration) by name\\r\\n```\\r\\n\\r\\n| name | averageDurationMilliseconds |\\r\\n| --- | --- |\\r\\n| QueueTrigger AvgDurationMs | 16.087 |\\r\\n| QueueTrigger MaxDurationMs | 90.249 |\\r\\n| QueueTrigger MinDurationMs | 8.522 |',\n",
       "  'metadata': {}},\n",
       " {'_id': '350da97f-fa07-6ab9-6df0-548f2c00a651',\n",
       "  'title': 'Bring dependencies and third-party libraries to Azure Functions',\n",
       "  'text': '# Bring dependencies or third party library to Azure Functions (programming-language-python)\\r\\n\\r\\nIn this article, you learn to bring in third-party dependencies into your functions apps. Examples of third-party dependencies are json files, binary files and machine learning models.\\r\\n\\r\\nIn this article, you learn how to:\\r\\n\\r\\n- Bring in dependencies via Functions Code project\\r\\n\\r\\n- Bring in dependencies via mounting Azure Fileshare\\r\\n\\r\\n## Bring in dependencies from the project directory\\r\\n\\r\\nOne of the simplest ways to bring in dependencies is to put the files/artifact together with the functions app code in Functions project directory structure. Here\\'s an example of the directory samples in a Python functions project:\\r\\n\\r\\n```\\r\\n<project_root>/\\r\\n | - my_first_function/\\r\\n | | - __init__.py\\r\\n | | - function.json\\r\\n | | - example.py\\r\\n | - dependencies/\\r\\n | | - dependency1\\r\\n | - .funcignore\\r\\n | - host.json\\r\\n | - local.settings.json\\r\\n```\\r\\n\\r\\nBy putting the dependencies in a folder inside functions app project directory, the dependencies folder will get deployed together with the code. As a result, your function code can access the dependencies in the cloud via file system api.\\r\\n\\r\\n### Accessing the dependencies in your code\\r\\n\\r\\nHere\\'s an example to access and execute `ffmpeg` dependency that is put into `<project_root>/ffmpeg_lib` directory.\\r\\n\\r\\n```python\\r\\nimport logging\\r\\n\\r\\nimport azure.functions as func\\r\\nimport subprocess\\r\\n\\r\\nFFMPEG_RELATIVE_PATH = \"../ffmpeg_lib/ffmpeg\"\\r\\n\\r\\ndef main(req: func.HttpRequest,\\r\\n         context: func.Context) -> func.HttpResponse:\\r\\n    logging.info(\\'Python HTTP trigger function processed a request.\\')\\r\\n\\r\\n    command = req.params.get(\\'command\\')\\r\\n    # If no command specified, set the command to help\\r\\n    if not command:\\r\\n        command = \"-h\"\\r\\n\\r\\n    # context.function_directory returns the current directory in which functions is executed \\r\\n    ffmpeg_path = \"/\".join([str(context.function_directory), FFMPEG_RELATIVE_PATH])\\r\\n\\r\\n    try:\\r\\n        byte_output  = subprocess.check_output([ffmpeg_path, command])\\r\\n        return func.HttpResponse(byte_output.decode(\\'UTF-8\\').rstrip(),status_code=200)\\r\\n    except Exception as e:\\r\\n        return func.HttpResponse(\"Unexpected exception happened when executing ffmpeg. Error message:\" + str(e),status_code=200)\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nYou may need to use `chmod` to provide `Execute` rights to the ffmpeg binary in a Linux environment\\r\\n\\r\\n## Bring dependencies by mounting a file share\\r\\n\\r\\nWhen running your function app on Linux, there\\'s another way to bring in third-party dependencies. Functions lets you mount a file share hosted in Azure Files. Consider this approach when you want to decouple dependencies or artifacts from your application code.\\r\\n\\r\\nFirst, you need to create an Azure Storage Account. In the account, you also need to create file share in Azure files. To create these resources, follow this [guide](../storage/files/storage-how-to-use-files-portal)\\r\\n\\r\\nAfter you created the storage account and file share, use the [az webapp config storage-account add](/en-us/cli/azure/webapp/config/storage-account#az-webapp-config-storage-account-add) command to attach the file share to your functions app, as shown in the following example.\\r\\n\\r\\n```azurecli\\r\\naz webapp config storage-account add \\\\\\r\\n  --name < Function-App-Name > \\\\\\r\\n  --resource-group < Resource-Group > \\\\\\r\\n  --subscription < Subscription-Id > \\\\\\r\\n  --custom-id < Unique-Custom-Id > \\\\\\r\\n  --storage-type AzureFiles \\\\\\r\\n  --account-name < Storage-Account-Name > \\\\\\r\\n  --share-name < File-Share-Name >  \\\\\\r\\n  --access-key < Storage-Account-AccessKey > \\\\\\r\\n  --mount-path </path/to/mount>\\r\\n```\\r\\n\\r\\n| Flag | Value |\\r\\n| --- | --- |\\r\\n| custom-id | Any unique string |\\r\\n| storage-type | Only AzureFiles is supported currently |\\r\\n| share-name | Pre-existing share |\\r\\n| mount-path | Path at which the share will be accessible inside the container. Value has to be of the format `/dir-name` and it can\\'t start with `/home` |\\r\\n\\r\\nMore commands to modify/delete the file share configuration can be found [here](/en-us/cli/azure/webapp/config/storage-account#az-webapp-config-storage-account-update)\\r\\n\\r\\n### Uploading the dependencies to Azure Files\\r\\n\\r\\nOne option to upload your dependency into Azure Files is through Azure portal. Refer to this [guide](../storage/files/storage-how-to-use-files-portal#upload-a-file) for instruction to upload dependencies using portal. Other options to upload your dependencies into Azure Files are through [Azure CLI](../storage/files/storage-how-to-use-files-portal#upload-a-file) and [PowerShell](../storage/files/storage-how-to-use-files-portal#upload-a-file).\\r\\n\\r\\n### Accessing the dependencies in your code\\r\\n\\r\\nAfter your dependencies are uploaded in the file share, you can access the dependencies from your code. The mounted share is available at the specified *mount-path*, such as `/path/to/mount`. You can access the target directory by using file system APIs.\\r\\n\\r\\nThe following example shows HTTP trigger code that accesses the `ffmpeg` library, which is stored in a mounted file share.\\r\\n\\r\\n```python\\r\\nimport logging\\r\\n\\r\\nimport azure.functions as func\\r\\nimport subprocess \\r\\n\\r\\nFILE_SHARE_MOUNT_PATH = os.environ[\\'FILE_SHARE_MOUNT_PATH\\']\\r\\nFFMPEG = \"ffmpeg\"\\r\\n\\r\\ndef main(req: func.HttpRequest) -> func.HttpResponse:\\r\\n    logging.info(\\'Python HTTP trigger function processed a request.\\')\\r\\n\\r\\n    command = req.params.get(\\'command\\')\\r\\n    # If no command specified, set the command to help\\r\\n    if not command:\\r\\n        command = \"-h\"\\r\\n\\r\\n    try:\\r\\n        byte_output  = subprocess.check_output([\"/\".join(FILE_SHARE_MOUNT_PATH, FFMPEG), command])\\r\\n        return func.HttpResponse(byte_output.decode(\\'UTF-8\\').rstrip(),status_code=200)\\r\\n    except Exception as e:\\r\\n        return func.HttpResponse(\"Unexpected exception happened when executing ffmpeg. Error message:\" + str(e),status_code=200)\\r\\n```\\r\\n\\r\\nWhen you deploy this code to a function app in Azure, you need to [create an app setting](functions-how-to-use-azure-function-app-settings#settings) with a key name of `FILE_SHARE_MOUNT_PATH` and value of the mounted file share path, which for this example is `/azure-files-share`. To do local debugging, you need to populate the `FILE_SHARE_MOUNT_PATH` with the file path where your dependencies are stored in your local machine. Here\\'s an example to set `FILE_SHARE_MOUNT_PATH` using `local.settings.json`:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"IsEncrypted\": false,\\r\\n  \"Values\": {\\r\\n    \"AzureWebJobsStorage\": \"\",\\r\\n    \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\\r\\n    \"FILE_SHARE_MOUNT_PATH\" : \"PATH_TO_LOCAL_FFMPEG_DIR\"\\r\\n  }\\r\\n}\\r\\n\\r\\n```\\r\\n\\r\\n# Bring dependencies or third party library to Azure Functions (programming-language-java)\\r\\n\\r\\nIn this article, you learn to bring in third-party dependencies into your functions apps. Examples of third-party dependencies are json files, binary files and machine learning models.\\r\\n\\r\\nIn this article, you learn how to:\\r\\n\\r\\n- Bring in dependencies via Functions Code project\\r\\n\\r\\n## Bring in dependencies from the project directory\\r\\n\\r\\nOne of the simplest ways to bring in dependencies is to put the files/artifact together with the functions app code in functions project directory structure. Here\\'s an example of the directory samples in a Java functions project:\\r\\n\\r\\n```\\r\\n<project_root>/\\r\\n | - src/\\r\\n | | - main/java/com/function\\r\\n | | | - Function.java\\r\\n | | - test/java/com/function\\r\\n | - artifacts/\\r\\n | | - dependency1\\r\\n | - host.json\\r\\n | - local.settings.json\\r\\n | - pom.xml\\r\\n```\\r\\n\\r\\nFor Java specifically, you need to specifically include the artifacts into the build/target folder when copying resources. Here\\'s an example on how to do it in Maven:\\r\\n\\r\\n```xml\\r\\n...\\r\\n<execution>\\r\\n    <id>copy-resources</id>\\r\\n    <phase>package</phase>\\r\\n    <goals>\\r\\n        <goal>copy-resources</goal>\\r\\n    </goals>\\r\\n    <configuration>\\r\\n        <overwrite>true</overwrite>\\r\\n        <outputDirectory>${stagingDirectory}</outputDirectory>\\r\\n        <resources>\\r\\n            <resource>\\r\\n                <directory>${project.basedir}</directory>\\r\\n                <includes>\\r\\n                    <include>host.json</include>\\r\\n                    <include>local.settings.json</include>\\r\\n                    <include>artifacts/**</include>\\r\\n                </includes>\\r\\n            </resource>\\r\\n        </resources>\\r\\n    </configuration>\\r\\n</execution>\\r\\n...\\r\\n```\\r\\n\\r\\nBy putting the dependencies in a folder inside functions app project directory, the dependencies folder will get deployed together with the code. As a result, your function code can access the dependencies in the cloud via file system api.\\r\\n\\r\\n### Accessing the dependencies in your code\\r\\n\\r\\nHere\\'s an example to access and execute `ffmpeg` dependency that is put into `<project_root>/ffmpeg_lib` directory.\\r\\n\\r\\n```java\\r\\npublic class Function {\\r\\n    final static String BASE_PATH = \"BASE_PATH\";\\r\\n    final static String FFMPEG_PATH = \"/artifacts/ffmpeg/ffmpeg.exe\";\\r\\n    final static String HELP_FLAG = \"-h\";\\r\\n    final static String COMMAND_QUERY = \"command\";\\r\\n\\r\\n    @FunctionName(\"HttpExample\")\\r\\n    public HttpResponseMessage run(\\r\\n            @HttpTrigger(\\r\\n                name = \"req\",\\r\\n                methods = {HttpMethod.GET, HttpMethod.POST},\\r\\n                authLevel = AuthorizationLevel.ANONYMOUS)\\r\\n                HttpRequestMessage<Optional<String>> request,\\r\\n            final ExecutionContext context) throws IOException{\\r\\n        context.getLogger().info(\"Java HTTP trigger processed a request.\");\\r\\n\\r\\n        // Parse query parameter\\r\\n        String flags = request.getQueryParameters().get(COMMAND_QUERY);\\r\\n\\r\\n        if (flags == null || flags.isBlank()) {\\r\\n            flags = HELP_FLAG;\\r\\n        }\\r\\n\\r\\n        Runtime rt = Runtime.getRuntime();\\r\\n        String[] commands = { System.getenv(BASE_PATH) + FFMPEG_PATH, flags};\\r\\n        Process proc = rt.exec(commands);\\r\\n\\r\\n        BufferedReader stdInput = new BufferedReader(new \\r\\n        InputStreamReader(proc.getInputStream()));\\r\\n\\r\\n        String out = stdInput.lines().collect(Collectors.joining(\"\\\\n\"));\\r\\n        if(out.isEmpty()) {\\r\\n            BufferedReader stdError = new BufferedReader(new \\r\\n                InputStreamReader(proc.getErrorStream()));\\r\\n            out = stdError.lines().collect(Collectors.joining(\"\\\\n\"));\\r\\n        }\\r\\n        return request.createResponseBuilder(HttpStatus.OK).body(out).build();\\r\\n\\r\\n    }\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nTo get this snippet of code to work in Azure, you need to specify a custom application setting of \"BASE\\\\_PATH\" with value of \"/home/site/wwwroot\"',\n",
       "  'metadata': {}},\n",
       " {'_id': 'ed05840e-ea3b-e367-77b5-f80ba8f6d018',\n",
       "  'title': 'Encrypt your application source at rest',\n",
       "  'text': '# Encrypt your application data at rest using customer-managed keys\\r\\n\\r\\nEncrypting your function app\\'s application data at rest requires an Azure Storage Account and an Azure Key Vault. These services are used when you run your app from a deployment package.\\r\\n\\r\\n- [Azure Storage provides encryption at rest](../storage/common/storage-service-encryption). You can use system-provided keys or your own, customer-managed keys. This is where your application data is stored when it\\'s not running in a function app in Azure.\\r\\n- [Running from a deployment package](run-functions-from-deployment-package) is a deployment feature of App Service. It allows you to deploy your site content from an Azure Storage Account using a Shared Access Signature (SAS) URL.\\r\\n- [Key Vault references](../app-service/app-service-key-vault-references) are a security feature of App Service. It allows you to import secrets at runtime as application settings. Use this to encrypt the SAS URL of your Azure Storage Account.\\r\\n\\r\\n## Set up encryption at rest\\r\\n\\r\\n### Create an Azure Storage account\\r\\n\\r\\nFirst, [create an Azure Storage account](../storage/common/storage-account-create) and [encrypt it with customer managed keys](../storage/common/customer-managed-keys-overview). Once the storage account is created, use the [Azure Storage Explorer](../vs-azure-tools-storage-manage-with-storage-explorer) to upload package files.\\r\\n\\r\\nNext, use the Storage Explorer to [generate an SAS](../vs-azure-tools-storage-manage-with-storage-explorer?tabs=windows#generate-a-sas-in-storage-explorer).\\r\\n\\r\\nNote\\r\\n\\r\\nSave this SAS URL, this is used later to enable secure access of the deployment package at runtime.\\r\\n\\r\\n### Configure running from a package from your storage account\\r\\n\\r\\nOnce you upload your file to Blob storage and have an SAS URL for the file, set the `WEBSITE_RUN_FROM_PACKAGE` application setting to the SAS URL. The following example does it by using Azure CLI:\\r\\n\\r\\n```\\r\\naz webapp config appsettings set --name <app-name> --resource-group <resource-group-name> --settings WEBSITE_RUN_FROM_PACKAGE=\"<your-SAS-URL>\"\\r\\n```\\r\\n\\r\\nAdding this application setting causes your function app to restart. After the app has restarted, browse to it and make sure that the app has started correctly using the deployment package. If the application didn\\'t start correctly, see the [Run from package troubleshooting guide](run-functions-from-deployment-package#troubleshooting).\\r\\n\\r\\n### Encrypt the application setting using Key Vault references\\r\\n\\r\\nNow you can replace the value of the `WEBSITE_RUN_FROM_PACKAGE` application setting with a Key Vault reference to the SAS-encoded URL. This keeps the SAS URL encrypted in Key Vault, which provides an extra layer of security.\\r\\n\\r\\n1. Use the following [`az keyvault create`](/en-us/cli/azure/keyvault#az-keyvault-create) command to create a Key Vault instance.\\r\\n\\r\\n    ```azurecli\\r\\n    az keyvault create --name \"Contoso-Vault\" --resource-group <group-name> --location eastus    \\r\\n    ```\\r\\n2. Follow [these instructions to grant your app access](../app-service/app-service-key-vault-references#grant-your-app-access-to-a-key-vault) to your key vault:\\r\\n3. Use the following [`az keyvault secret set`](/en-us/cli/azure/keyvault/secret#az-keyvault-secret-set) command to add your external URL as a secret in your key vault:\\r\\n\\r\\n    ```azurecli\\r\\n    az keyvault secret set --vault-name \"Contoso-Vault\" --name \"external-url\" --value \"<SAS-URL>\"    \\r\\n    ```\\r\\n4. Use the following [`az webapp config appsettings set`](/en-us/cli/azure/webapp/config/appsettings#az-webapp-config-appsettings-set) command to create the `WEBSITE_RUN_FROM_PACKAGE` application setting with the value as a Key Vault reference to the external URL:\\r\\n\\r\\n    ```azurecli\\r\\n    az webapp config appsettings set --settings WEBSITE_RUN_FROM_PACKAGE=\"@Microsoft.KeyVault(SecretUri=https://Contoso-Vault.vault.azure.net/secrets/external-url/<secret-version>\"    \\r\\n    ```\\r\\n\\r\\n    The `<secret-version>` will be in the output of the previous `az keyvault secret set` command.\\r\\n\\r\\nUpdating this application setting causes your function app to restart. After the app has restarted, browse to it make sure it has started correctly using the Key Vault reference.\\r\\n\\r\\n## How to rotate the access token\\r\\n\\r\\nIt is best practice to periodically rotate the SAS key of your storage account. To ensure the function app does not inadvertently lose access, you must also update the SAS URL in Key Vault.\\r\\n\\r\\n1. Rotate the SAS key by navigating to your storage account in the Azure portal. Under **Settings** &gt; **Access keys**, select the icon to rotate the SAS key.\\r\\n2. Copy the new SAS URL, and use the following command to set the updated SAS URL in your key vault:\\r\\n\\r\\n    ```azurecli\\r\\n    az keyvault secret set --vault-name \"Contoso-Vault\" --name \"external-url\" --value \"<SAS-URL>\"    \\r\\n    ```\\r\\n3. Update the key vault reference in your application setting to the new secret version:\\r\\n\\r\\n    ```azurecli\\r\\n    az webapp config appsettings set --settings WEBSITE_RUN_FROM_PACKAGE=\"@Microsoft.KeyVault(SecretUri=https://Contoso-Vault.vault.azure.net/secrets/external-url/<secret-version>\"    \\r\\n    ```\\r\\n\\r\\n    The `<secret-version>` will be in the output of the previous `az keyvault secret set` command.\\r\\n\\r\\n## How to revoke the function app\\'s data access\\r\\n\\r\\nThere are two methods to revoke the function app\\'s access to the storage account.\\r\\n\\r\\n### Rotate the SAS key for the Azure Storage account\\r\\n\\r\\nIf the SAS key for the storage account is rotated, the function app will no longer have access to the storage account, but it will continue to run with the last downloaded version of the package file. Restart the function app to clear the last downloaded version.\\r\\n\\r\\n### Remove the function app\\'s access to Key Vault\\r\\n\\r\\nYou can revoke the function app\\'s access to the site data by disabling the function app\\'s access to Key Vault. To do this, remove the access policy for the function app\\'s identity. This is the same identity you created earlier while configuring key vault references.\\r\\n\\r\\n## Summary\\r\\n\\r\\nYour application files are now encrypted at rest in your storage account. When your function app starts, it retrieves the SAS URL from your key vault. Finally, the function app loads the application files from the storage account.\\r\\n\\r\\nIf you need to revoke the function app\\'s access to your storage account, you can either revoke access to the key vault or rotate the storage account keys, both of which invalidate the SAS URL.\\r\\n\\r\\n## Frequently Asked Questions\\r\\n\\r\\n### Is there any additional charge for running my function app from the deployment package?\\r\\n\\r\\nOnly the cost associated with the Azure Storage Account and any applicable egress charges.\\r\\n\\r\\n### How does running from the deployment package affect my function app?\\r\\n\\r\\n- Running your app from the deployment package makes `wwwroot/` read-only. Your app receives an error when it attempts to write to this directory.\\r\\n- TAR and GZIP formats are not supported.\\r\\n- This feature is not compatible with local cache.',\n",
       "  'metadata': {}},\n",
       " {'_id': 'bcf023ad-012c-e840-ef25-dbf13657d751',\n",
       "  'title': 'Configure monitoring for Azure Functions',\n",
       "  'text': '# How to configure monitoring for Azure Functions\\r\\n\\r\\nAzure Functions integrates with Application Insights to better enable you to monitor your function apps. Application Insights, a feature of Azure Monitor, is an extensible Application Performance Management (APM) service that collects data generated by your function app, including information your app writes to logs. Application Insights integration is typically enabled when your function app is created. If your app doesn\\'t have the instrumentation key set, you must first enable Application Insights integration.\\r\\n\\r\\nYou can use Application Insights without any custom configuration. However, the default configuration can result in high volumes of data. If you\\'re using a Visual Studio Azure subscription, you might hit your data cap for Application Insights. For information about Application Insights costs, see [Application Insights billing](/en-us/azure/azure-monitor/logs/cost-logs#application-insights-billing). For more information, see Solutions with high-volume of telemetry.\\r\\n\\r\\nIn this article, you learn how to configure and customize the data that your functions send to Application Insights. You can set common logging configurations in the *[host.json](functions-host-json)* file. By default, these settings also govern custom logs emitted by your code. However, in some cases this behavior can be disabled in favor of options that give you more control over logging. For more information, see Custom application logs.\\r\\n\\r\\nNote\\r\\n\\r\\nYou can use specially configured application settings to represent specific settings in a *host.json* file for a particular environment. Doing so lets you effectively change *host.json* settings without needing to republish the *host.json* file in your project. For more information, see [Override host.json values](functions-host-json#override-hostjson-values).\\r\\n\\r\\n## Custom application logs\\r\\n\\r\\nBy default, custom application logs you write are sent to the Functions host, which then sends them to Application Insights under the Worker category. Some language stacks allow you to instead send the logs directly to Application Insights, which gives you full control over how logs you write are emitted. In this case, the logging pipeline changes from `worker -> Functions host -> Application Insights` to `worker -> Application Insights`.\\r\\n\\r\\nThe following table summarizes the configuration options available for each stack:\\r\\n\\r\\n| Language stack | Where to configure custom logs |\\r\\n| --- | --- |\\r\\n| .NET (in-process model) | `host.json` |\\r\\n| .NET (isolated model) | Default (send custom logs to the Functions host): `host.json`To send logs directly to Application Insights, see: [Configure Application Insights in the HostBuilder](dotnet-isolated-process-guide#application-insights) |\\r\\n| Node.js | `host.json` |\\r\\n| Python | `host.json` |\\r\\n| Java | Default (send custom logs to the Functions host): `host.json`To send logs directly to Application Insights, see: [Configure the Application Insights Java agent](/en-us/azure/azure-monitor/app/monitor-functions#distributed-tracing-for-java-applications) |\\r\\n| PowerShell | `host.json` |\\r\\n\\r\\nWhen you configure custom application logs to be sent directly, the host no longer emits them, and `host.json` no longer controls their behavior. Similarly, the options exposed by each stack apply only to custom logs, and they don\\'t change the behavior of the other runtime logs described in this article. In this case, to control the behavior of all logs, you might need to make changes in both configurations.\\r\\n\\r\\n## Configure categories\\r\\n\\r\\nThe Azure Functions logger includes a *category* for every log. The category indicates which part of the runtime code or your function code wrote the log. Categories differ between version 1.x and later versions.\\r\\n\\r\\nCategory names are assigned differently in Functions compared to other .NET frameworks. For example, when you use `ILogger<T>` in ASP.NET, the category is the name of the generic type. C# functions also use `ILogger<T>`, but instead of setting the generic type name as a category, the runtime assigns categories based on the source. For example:\\r\\n\\r\\n- Entries related to running a function are assigned a category of `Function.<FUNCTION_NAME>`.\\r\\n- Entries created by user code inside the function, such as when calling `logger.LogInformation()`, are assigned a category of `Function.<FUNCTION_NAME>.User`.\\r\\n\\r\\nThe following table describes the main categories of logs that the runtime creates:\\r\\n\\r\\n**v2.x+**\\r\\n\\r\\n| Category | Table | Description |\\r\\n| --- | --- | --- |\\r\\n| **`Function`** | **traces** | Includes function started and completed logs for all function runs. For successful runs, these logs are at the `Information` level. Exceptions are logged at the `Error` level. The runtime also creates `Warning` level logs, such as when queue messages are sent to the [poison queue](functions-bindings-storage-queue-trigger#poison-messages). |\\r\\n| **`Function.<YOUR_FUNCTION_NAME>`** | **dependencies** | Dependency data is automatically collected for some services. For successful runs, these logs are at the `Information` level. For more information, see [Dependencies](functions-monitoring#dependencies). Exceptions are logged at the `Error` level. The runtime also creates `Warning` level logs, such as when queue messages are sent to the [poison queue](functions-bindings-storage-queue-trigger#poison-messages). |\\r\\n| **`Function.<YOUR_FUNCTION_NAME>`** | **customMetrics** **customEvents** | C# and JavaScript SDKs lets you collect custom metrics and log custom events. For more information, see [Custom telemetry data](functions-monitoring#custom-telemetry-data). |\\r\\n| **`Function.<YOUR_FUNCTION_NAME>`** | **traces** | Includes function started and completed logs for specific function runs. For successful runs, these logs are at the `Information` level. Exceptions are logged at the `Error` level. The runtime also creates `Warning` level logs, such as when queue messages are sent to the [poison queue](functions-bindings-storage-queue-trigger#poison-messages). |\\r\\n| **`Function.<YOUR_FUNCTION_NAME>.User`** | **traces** | User-generated logs, which can be any log level. For more information about writing to logs from your functions, see [Writing to logs](functions-monitoring#writing-to-logs). |\\r\\n| **`Host.Aggregator`** | **customMetrics** | These runtime-generated logs provide counts and averages of function invocations over a configurable period of time. The default period is 30 seconds or 1,000 results, whichever comes first. Examples are the number of runs, success rate, and duration. All of these logs are written at the `Information` level. If you filter at `Warning` or higher, you don\\'t see any of this data. |\\r\\n| **`Host.Results`** | **requests** | These runtime-generated logs indicate success or failure of a function. All of these logs are written at the `Information` level. If you filter at `Warning` or higher, you don\\'t see any of this data. |\\r\\n| **`Microsoft`** | **traces** | Fully qualified log category that reflects a .NET runtime component invoked by the host. |\\r\\n| **`Worker`** | **traces** | Logs generated by the language worker process for non-.NET languages. Language worker logs might also be logged in a `Microsoft.*` category, such as `Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcFunctionInvocationDispatcher`. These logs are written at the `Information` level. |\\r\\n\\r\\nNote\\r\\n\\r\\nFor .NET class library functions, these categories assume you\\'re using `ILogger` and not `ILogger<T>`. For more information, see the [Functions ILogger documentation](functions-dotnet-class-library#ilogger).\\r\\n\\r\\n**v1.x**\\r\\n\\r\\n| Category | Table | Description |\\r\\n| --- | --- | --- |\\r\\n| **`Function`** | **traces** | User-generated logs, which can be any log level. For more information about writing to logs from your functions, see [Writing to logs](functions-monitoring#writing-to-logs). |\\r\\n| **`Host.Aggregator`** | **customMetrics** | These runtime-generated logs provide counts and averages of function invocations over a configurable period of time. The default period is 30 seconds or 1,000 results, whichever comes first. Examples are the number of runs, success rate, and duration. All of these logs are written at `Information` level. If you filter at `Warning` or higher, you don\\'t see any of this data. |\\r\\n| **`Host.Executor`** | **traces** | Includes **Function started** and **Function completed** logs for specific function runs. For successful runs, these logs are at the `Information` level. Exceptions are logged at the `Error` level. The runtime also creates logs at the `Warning`level, such as when queue messages are sent to the [poison queue](functions-bindings-storage-queue-trigger#poison-messages). |\\r\\n| **`Host.Results`** | **requests** | These runtime-generated logs indicate success or failure of a function. All of these logs are written at the `Information` level. If you filter at `Warning` or higher, you don\\'t see any of this data. |\\r\\n\\r\\nThe **Table** column indicates to which table in Application Insights the log is written.\\r\\n\\r\\n## Configure log levels\\r\\n\\r\\nA *log level* is assigned to every log. The value is an integer that indicates relative importance:\\r\\n\\r\\n| LogLevel | Code | Description |\\r\\n| --- | --- | --- |\\r\\n| Trace | 0 | Logs that contain the most detailed messages. These messages might contain sensitive application data. These messages are disabled by default and should never be enabled in a production environment. |\\r\\n| Debug | 1 | Logs that are used for interactive investigation during development. These logs should primarily contain information useful for debugging and have no long-term value. |\\r\\n| Information | 2 | Logs that track the general flow of the application. These logs should have long-term value. |\\r\\n| Warning | 3 | Logs that highlight an abnormal or unexpected event in the application flow, but don\\'t otherwise cause the application execution to stop. |\\r\\n| Error | 4 | Logs that highlight when the current flow of execution is stopped because of a failure. These errors should indicate a failure in the current activity, not an application-wide failure. |\\r\\n| Critical | 5 | Logs that describe an unrecoverable application or system crash, or a catastrophic failure that requires immediate attention. |\\r\\n| None | 6 | Disables logging for the specified category. |\\r\\n\\r\\nThe [*host.json* file](functions-host-json) configuration determines how much logging a functions app sends to Application Insights.\\r\\n\\r\\nFor each category, you indicate the minimum log level to send. The *host.json* settings vary depending on the [Functions runtime version](functions-versions).\\r\\n\\r\\nThe following examples define logging based on the following rules:\\r\\n\\r\\n- The default logging level is set to `Warning` to prevent excessive logging for unanticipated categories.\\r\\n- `Host.Aggregator` and `Host.Results` are set to lower levels. Setting logging levels too high (especially higher than `Information`) can result in loss of metrics and performance data.\\r\\n- Logging for function runs is set to `Information`. If necessary, you can [override](functions-host-json#override-hostjson-values) this setting in local development to `Debug` or `Trace`.\\r\\n\\r\\n**v2.x+**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"logging\": {\\r\\n    \"fileLoggingMode\": \"debugOnly\",\\r\\n    \"logLevel\": {\\r\\n      \"default\": \"Warning\",\\r\\n      \"Host.Aggregator\": \"Trace\",\\r\\n      \"Host.Results\": \"Information\",\\r\\n      \"Function\": \"Information\"\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n**v1.x**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"logger\": {\\r\\n    \"categoryFilter\": {\\r\\n      \"defaultLevel\": \"Warning\",\\r\\n      \"categoryLevels\": {\\r\\n        \"Host.Results\": \"Information\",\\r\\n        \"Host.Aggregator\": \"Trace\",\\r\\n        \"Function\": \"Information\"\\r\\n      }\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nIf *[host.json](functions-host-json)* includes multiple logs that start with the same string, the more defined logs ones are matched first. Consider the following example that logs everything in the runtime, except `Host.Aggregator`, at the `Error` level:\\r\\n\\r\\n**v2.x+**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"logging\": {\\r\\n    \"fileLoggingMode\": \"debugOnly\",\\r\\n    \"logLevel\": {\\r\\n      \"default\": \"Information\",\\r\\n      \"Host\": \"Error\",\\r\\n      \"Function\": \"Error\",\\r\\n      \"Host.Aggregator\": \"Information\"\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n**v1.x**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"logger\": {\\r\\n    \"categoryFilter\": {\\r\\n      \"defaultLevel\": \"Information\",\\r\\n      \"categoryLevels\": {\\r\\n        \"Host\": \"Error\",\\r\\n        \"Function\": \"Error\",\\r\\n        \"Host.Aggregator\": \"Information\"\\r\\n      }\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nYou can use a log level setting of `None` to prevent any logs from being written for a category.\\r\\n\\r\\nCaution\\r\\n\\r\\nAzure Functions integrates with Application Insights by storing telemetry events in Application Insights tables. If you set a category log level to any value different from `Information`, it prevents the telemetry from flowing to those tables, and you won\\'t be able to see related data in the **Application Insights** and **Function Monitor** tabs.\\r\\n\\r\\nFor example, for the previous samples:\\r\\n\\r\\n- If you set the `Host.Results` category to the `Error` log level, Azure gathers only host execution telemetry events in the `requests` table for failed function executions, preventing the display of host execution details of successful executions in both the **Application Insights** and **Function Monitor** tabs.\\r\\n- If you set the `Function` category to the `Error` log level, it stops gathering function telemetry data related to `dependencies`, `customMetrics`, and `customEvents` for all the functions, preventing you from viewing any of this data in Application Insights. Azure gathers only `traces` logged at the `Error` level.\\r\\n\\r\\nIn both cases, Azure continues to collect errors and exceptions data in the **Application Insights** and **Function Monitor** tabs. For more information, see Solutions with high-volume of telemetry.\\r\\n\\r\\n## Configure the aggregator\\r\\n\\r\\nAs noted in the previous section, the runtime aggregates data about function executions over a period of time. The default period is 30 seconds or 1,000 runs, whichever comes first. You can configure this setting in the *[host.json](functions-host-json)* file. For example:\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"aggregator\": {\\r\\n      \"batchSize\": 1000,\\r\\n      \"flushTimeout\": \"00:00:30\"\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n## Configure sampling\\r\\n\\r\\nApplication Insights has a [sampling](/en-us/azure/azure-monitor/app/sampling) feature that can protect you from producing too much telemetry data on completed executions at times of peak load. When the rate of incoming executions exceeds a specified threshold, Application Insights starts to randomly ignore some of the incoming executions. The default setting for maximum number of executions per second is 20 (five in version 1.x). You can configure sampling in [*host.json*](functions-host-json#applicationinsights). Here\\'s an example:\\r\\n\\r\\n**v2.x+**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"logging\": {\\r\\n    \"applicationInsights\": {\\r\\n      \"samplingSettings\": {\\r\\n        \"isEnabled\": true,\\r\\n        \"maxTelemetryItemsPerSecond\" : 20,\\r\\n        \"excludedTypes\": \"Request;Exception\"\\r\\n      }\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nYou can exclude certain types of telemetry from sampling. In this example, data of type `Request` and `Exception` is excluded from sampling. It ensures that *all* function executions (requests) and exceptions are logged while other types of telemetry remain subject to sampling.\\r\\n\\r\\n**v1.x**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"applicationInsights\": {\\r\\n    \"sampling\": {\\r\\n      \"isEnabled\": true,\\r\\n      \"maxTelemetryItemsPerSecond\" : 5\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nIf your project uses a dependency on the Application Insights SDK to do manual telemetry tracking, you might experience unusual behavior if your sampling configuration differs from the sampling configuration in your function app. In such cases, use the same sampling configuration as the function app. For more information, see [Sampling in Application Insights](/en-us/azure/azure-monitor/app/sampling).\\r\\n\\r\\n## Enable SQL query collection\\r\\n\\r\\nApplication Insights automatically collects data on dependencies for HTTP requests, database calls, and for several bindings. For more information, see [Dependencies](functions-monitoring#dependencies). For SQL calls, the name of the server and database is always collected and stored, but SQL query text isn\\'t collected by default. You can use `dependencyTrackingOptions.enableSqlCommandTextInstrumentation` to enable SQL query text logging by using the following settings (at a minimum) in your [host.json file](functions-host-json#applicationinsightsdependencytrackingoptions):\\r\\n\\r\\n```json\\r\\n\"logging\": {\\r\\n    \"applicationInsights\": {\\r\\n        \"enableDependencyTracking\": true,    \\r\\n        \"dependencyTrackingOptions\": {\\r\\n            \"enableSqlCommandTextInstrumentation\": true\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nFor more information, see [Advanced SQL tracking to get full SQL query](/en-us/azure/azure-monitor/app/asp-net-dependencies#advanced-sql-tracking-to-get-full-sql-query).\\r\\n\\r\\n## Configure scale controller logs\\r\\n\\r\\n*This feature is in preview.*\\r\\n\\r\\nYou can have the [Azure Functions scale controller](event-driven-scaling#runtime-scaling) emit logs to either Application Insights or to Blob storage to better understand the decisions the scale controller is making for your function app.\\r\\n\\r\\nTo enable this feature, add an application setting named `SCALE_CONTROLLER_LOGGING_ENABLED` to your function app settings. The following value of the setting must be in the format `<DESTINATION>:<VERBOSITY>`. For more information, see the following table:\\r\\n\\r\\n| Property | Description |\\r\\n| --- | --- |\\r\\n| **`<DESTINATION>`** | The destination to which logs are sent. Valid values are `AppInsights` and `Blob`.When you use `AppInsights`, ensure that the [Application Insights is enabled in your function app](configure-monitoring#enable-application-insights-integration).When you set the destination to `Blob`, logs are created in a blob container named `azure-functions-scale-controller` in the default storage account set in the `AzureWebJobsStorage` application setting. |\\r\\n| **`<VERBOSITY>`** | Specifies the level of logging. Supported values are `None`, `Warning`, and `Verbose`.When set to `Verbose`, the scale controller logs a reason for every change in the worker count, and information about the triggers that factor into those decisions. Verbose logs include trigger warnings and the hashes used by the triggers before and after the scale controller runs. |\\r\\n\\r\\nTip\\r\\n\\r\\nKeep in mind that while you leave scale controller logging enabled, it impacts the [potential costs of monitoring your function app](functions-monitoring#application-insights-pricing-and-limits). Consider enabling logging until you have collected enough data to understand how the scale controller is behaving, and then disabling it.\\r\\n\\r\\nFor example, the following Azure CLI command turns on verbose logging from the scale controller to Application Insights:\\r\\n\\r\\n```azurecli\\r\\naz functionapp config appsettings set --name <FUNCTION_APP_NAME> \\\\\\r\\n--resource-group <RESOURCE_GROUP_NAME> \\\\\\r\\n--settings SCALE_CONTROLLER_LOGGING_ENABLED=AppInsights:Verbose\\r\\n```\\r\\n\\r\\nIn this example, replace `<FUNCTION_APP_NAME>` and `<RESOURCE_GROUP_NAME>` with the name of your function app and the resource group name, respectively.\\r\\n\\r\\nThe following Azure CLI command disables logging by setting the verbosity to `None`:\\r\\n\\r\\n```azurecli\\r\\naz functionapp config appsettings set --name <FUNCTION_APP_NAME> \\\\\\r\\n--resource-group <RESOURCE_GROUP_NAME> \\\\\\r\\n--settings SCALE_CONTROLLER_LOGGING_ENABLED=AppInsights:None\\r\\n```\\r\\n\\r\\nYou can also disable logging by removing the `SCALE_CONTROLLER_LOGGING_ENABLED` setting using the following Azure CLI command:\\r\\n\\r\\n```azurecli\\r\\naz functionapp config appsettings delete --name <FUNCTION_APP_NAME> \\\\\\r\\n--resource-group <RESOURCE_GROUP_NAME> \\\\\\r\\n--setting-names SCALE_CONTROLLER_LOGGING_ENABLED\\r\\n```\\r\\n\\r\\nWith scale controller logging enabled, you\\'re now able to [query your scale controller logs](analyze-telemetry-data#query-scale-controller-logs).\\r\\n\\r\\n## Enable Application Insights integration\\r\\n\\r\\nFor a function app to send data to Application Insights, it needs to connect to the Application Insights resource using **only one** of these application settings:\\r\\n\\r\\n| Setting name | Description |\\r\\n| --- | --- |\\r\\n| **[`APPLICATIONINSIGHTS_CONNECTION_STRING`](functions-app-settings#applicationinsights_connection_string)** | This setting is recommended and is required when your Application Insights instance runs in a sovereign cloud. The connection string supports other [new capabilities](/en-us/azure/azure-monitor/app/migrate-from-instrumentation-keys-to-connection-strings#new-capabilities). |\\r\\n| **[`APPINSIGHTS_INSTRUMENTATIONKEY`](functions-app-settings#appinsights_instrumentationkey)** | Legacy setting, which Application Insights has deprecated in favor of the connection string setting. |\\r\\n\\r\\nWhen you create your function app in the [Azure portal](functions-get-started) from the command line by using [Azure Functions Core Tools](create-first-function-cli-csharp) or [Visual Studio Code](create-first-function-vs-code-csharp), Application Insights integration is enabled by default. The Application Insights resource has the same name as your function app, and is created either in the same region or in the nearest region.\\r\\n\\r\\n### Require Microsoft Entra authentication\\r\\n\\r\\nYou can use the [`APPLICATIONINSIGHTS_AUTHENTICATION_STRING`](functions-app-settings#applicationinsights_authentication_string) setting to enable connections to Application Insights using Microsoft Entra authentication. This creates a consistent authentication experience across all Application Insights pipelines, including Profiler and Snapshot Debugger, as well as from the Functions host and language-specific agents.\\r\\n\\r\\nNote\\r\\n\\r\\nThere\\'s no Entra authentication support for local development.\\r\\n\\r\\nThe value contains either `Authorization=AAD` for a system-assigned managed identity or `ClientId=<YOUR_CLIENT_ID>;Authorization=AAD` for a user-assigned managed identity. The managed identity must already be available to the function app, with an assigned role equivalent to [Monitoring Metrics Publisher](/en-us/azure/role-based-access-control/built-in-roles/monitor#monitoring-metrics-publisher). For more information, see [Microsoft Entra authentication for Application Insights](/en-us/azure/azure-monitor/app/azure-ad-authentication).\\r\\n\\r\\nThe [`APPLICATIONINSIGHTS_CONNECTION_STRING`](functions-app-settings#applicationinsights_connection_string) setting is still required.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen using `APPLICATIONINSIGHTS_AUTHENTICATION_STRING` to connect to Application Insights using Microsoft Entra authentication, you should also [Disable local authentication for Application Insights](/en-us/azure/azure-monitor/app/azure-ad-authentication#disable-local-authentication). This configuration requires Microsoft Entra authentication in order for telemetry to be ingested into your workspace.\\r\\n\\r\\n### New function app in the portal\\r\\n\\r\\nTo review the Application Insights resource being created, select it to expand the **Application Insights** window. You can change the **New resource name** or select a different **Location** in an [Azure geography](https://azure.microsoft.com/global-infrastructure/geographies/) where you want to store your data.\\r\\n\\r\\nWhen you select **Create**, an Application Insights resource is created with your function app, which has the `APPLICATIONINSIGHTS_CONNECTION_STRING` set in application settings. Everything is ready to go.\\r\\n\\r\\n### Add to an existing function app\\r\\n\\r\\nIf an Application Insights resource wasn\\'t created with your function app, use the following steps to create the resource. You can then add the connection string from that resource as an [application setting](functions-how-to-use-azure-function-app-settings#settings) in your function app.\\r\\n\\r\\n1. In the [Azure portal](https://portal.azure.com), search for and select **function app**, and then select your function app.\\r\\n2. Select the **Application Insights is not configured** banner at the top of the window. If you don\\'t see this banner, then your app might already have Application Insights enabled.\\r\\n3. Expand **Change your resource** and create an Application Insights resource by using the settings specified in the following table:\\r\\n\\r\\n    | Setting | Suggested value | Description |\\r\\n    | --- | --- | --- |\\r\\n    | **New resource name** | Unique app name | It\\'s easiest to use the same name as your function app, which must be unique in your subscription. |\\r\\n    | **Location** | West Europe | If possible, use the same [region](https://azure.microsoft.com/regions/) as your function app, or the one that\\'s close to that region. |\\r\\n4. Select **Apply**.\\r\\n\\r\\n    The Application Insights resource is created in the same resource group and subscription as your function app. After the resource is created, close the **Application Insights** window.\\r\\n5. In your function app, expand **Settings**, and then select **Environment variables**. In the **App settings** tab, if you see an app setting named `APPLICATIONINSIGHTS_CONNECTION_STRING`, Application Insights integration is enabled for your function app running in Azure. If this setting doesn\\'t exist, add it by using your Application Insights connection string as the value.\\r\\n\\r\\nNote\\r\\n\\r\\nOlder function apps might use `APPINSIGHTS_INSTRUMENTATIONKEY` instead of `APPLICATIONINSIGHTS_CONNECTION_STRING`. When possible, update your app to use the connection string instead of the instrumentation key.\\r\\n\\r\\n## Disable built-in logging\\r\\n\\r\\nEarly versions of Functions used built-in monitoring, which is no longer recommended. When you enable Application Insights, disable the built-in logging that uses Azure Storage. The built-in logging is useful for testing with light workloads, but isn\\'t intended for high-load production use. For production monitoring, we recommend Application Insights. If you use built-in logging in production, the logging record might be incomplete because of throttling on Azure Storage.\\r\\n\\r\\nTo disable built-in logging, delete the `AzureWebJobsDashboard` app setting. For more information about how to delete app settings in the Azure portal, see the **Application settings** section of [How to manage a function app](functions-how-to-use-azure-function-app-settings#settings). Before you delete the app setting, ensure that no existing functions in the same function app use the setting for Azure Storage triggers or bindings.\\r\\n\\r\\n## Solutions with high volume of telemetry\\r\\n\\r\\nFunction apps are an essential part of solutions that can cause high volumes of telemetry, such as IoT solutions, rapid event driven solutions, high load financial systems, and integration systems. In this case, you should consider extra configuration to reduce costs while maintaining observability.\\r\\n\\r\\nThe generated telemetry can be consumed in real-time dashboards, alerting, detailed diagnostics, and so on. Depending on how the generated telemetry is consumed, you need to define a strategy to reduce the volume of data generated. This strategy allows you to properly monitor, operate, and diagnose your function apps in production. Consider the following options:\\r\\n\\r\\n- **Use sampling**: As mentioned previously, sampling helps to dramatically reduce the volume of telemetry events ingested while maintaining a statistically correct analysis. It could happen that even using sampling you still get a high volume of telemetry. Inspect the options that [adaptive sampling](/en-us/azure/azure-monitor/app/sampling#configuring-adaptive-sampling-for-aspnet-applications) provides to you. For example, set the `maxTelemetryItemsPerSecond` to a value that balances the volume generated with your monitoring needs. Keep in mind that the telemetry sampling is applied per host executing your function app.\\r\\n- **Default log level**: Use `Warning` or `Error` as the default value for all telemetry categories. Later, you can decide which categories you want to set at the `Information` level, so that you can monitor and diagnose your functions properly.\\r\\n- **Tune your functions telemetry**: With the default log level set to `Error` or `Warning`, no detailed information from each function is gathered (dependencies, custom metrics, custom events, and traces). For those functions that are key for production monitoring, define an explicit entry for the `Function.<YOUR_FUNCTION_NAME>` category and set it to `Information`, so that you can gather detailed information. To avoid gathering [user-generated logs](functions-monitoring#writing-to-logs) at the `Information` level, set the `Function.<YOUR_FUNCTION_NAME>.User` category to the `Error` or `Warning` log level.\\r\\n- **Host.Aggregator category**: As described in configure categories, this category provides aggregated information of function invocations. The information from this category is gathered in the Application Insights `customMetrics` table, and is shown in the function **Overview** tab in the Azure portal. Depending on how you configure the aggregator, consider that there can be a delay, determined by the `flushTimeout` setting, in the telemetry gathered. If you set this category to a value different from `Information`, you stop gathering the data in the `customMetrics` table and don\\'t display metrics in the function **Overview** tab.\\r\\n\\r\\n    The following screenshot shows `Host.Aggregator` telemetry data displayed in the function **Overview** tab:\\r\\n\\r\\n    The following screenshot shows `Host.Aggregator` telemetry data in Application Insights `customMetrics` table:\\r\\n- **Host.Results category**: As described in configure categories, this category provides the runtime-generated logs indicating the success or failure of a function invocation. The information from this category is gathered in the Application Insights `requests` table, and is shown in the function **Monitor** tab and in different Application Insights dashboards (Performance, Failures, and so on). If you set this category to a value different than `Information`, you gather only telemetry generated at the log level defined (or higher). For example, setting it to `error` results in tracking requests data only for failed executions.\\r\\n\\r\\n    The following screenshot shows the `Host.Results` telemetry data displayed in the function **Monitor** tab:\\r\\n\\r\\n    The following screenshot shows `Host.Results` telemetry data displayed in Application Insights Performance dashboard:\\r\\n- **Host.Aggregator vs Host.Results**: Both categories provide good insights about function executions. If needed, you can remove the detailed information from one of these categories, so that you can use the other for monitoring and alerting. Here\\'s a sample:\\r\\n\\r\\n**v2.x+**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"version\": \"2.0\",  \\r\\n  \"logging\": {\\r\\n    \"logLevel\": {\\r\\n      \"default\": \"Warning\",\\r\\n      \"Function\": \"Error\",\\r\\n      \"Host.Aggregator\": \"Error\",\\r\\n      \"Host.Results\": \"Information\", \\r\\n      \"Function.Function1\": \"Information\",\\r\\n      \"Function.Function1.User\": \"Error\"\\r\\n    },\\r\\n    \"applicationInsights\": {\\r\\n      \"samplingSettings\": {\\r\\n        \"isEnabled\": true,\\r\\n        \"maxTelemetryItemsPerSecond\": 1,\\r\\n        \"excludedTypes\": \"Exception\"\\r\\n      }\\r\\n    }\\r\\n  }\\r\\n} \\r\\n```\\r\\n\\r\\n**v1.x**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"logger\": {\\r\\n    \"categoryFilter\": {\\r\\n      \"defaultLevel\": \"Warning\",\\r\\n      \"categoryLevels\": {\\r\\n        \"Function\": \"Error\",\\r\\n        \"Host.Aggregator\": \"Error\",\\r\\n        \"Host.Results\": \"Information\",\\r\\n        \"Host.Executor\": \"Warning\"\\r\\n      }\\r\\n    }\\r\\n  },\\r\\n  \"applicationInsights\": {\\r\\n    \"sampling\": {\\r\\n      \"isEnabled\": true,\\r\\n      \"maxTelemetryItemsPerSecond\" : 5\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nWith this configuration:\\r\\n\\r\\n- The default value for all functions and telemetry categories is set to `Warning` (including Microsoft and Worker categories). So, by default, all errors and warnings generated by runtime and custom logging are gathered.\\r\\n- The `Function` category log level is set to `Error`, so for all functions, by default, only exceptions and error logs are gathered. Dependencies, user-generated metrics, and user-generated events are skipped.\\r\\n- For the `Host.Aggregator` category, as it\\'s set to the `Error` log level, aggregated information from function invocations isn\\'t gathered in the `customMetrics` Application Insights table, and information about executions counts (total, successful, and failed) aren\\'t shown in the function overview dashboard.\\r\\n- For the `Host.Results` category, all the host execution information is gathered in the `requests` Application Insights table. All the invocations results are shown in the function Monitor dashboard and in Application Insights dashboards.\\r\\n- For the function called `Function1`, we set the log level to `Information`. So, for this concrete function, all the telemetry is gathered (dependency, custom metrics, and custom events). For the same function, we set the `Function1.User` category (user-generated traces) to `Error`, so only custom error logging is gathered.\\r\\n\\r\\n    Note\\r\\n\\r\\n    Configuration per function isn\\'t supported in v1.x of the Functions runtime.\\r\\n- Sampling is configured to send one telemetry item per second per type, excluding the exceptions. This sampling happens for each server host running our function app. So, if we have four instances, this configuration emits four telemetry items per second per type and all the exceptions that might occur.\\r\\n\\r\\n    Note\\r\\n\\r\\n    Metric counts such as request rate and exception rate are adjusted to compensate for the sampling rate, so that they show approximately correct values in Metric Explorer.\\r\\n\\r\\nTip\\r\\n\\r\\nExperiment with different configurations to ensure that you cover your requirements for logging, monitoring, and alerting. Also, ensure that you have detailed diagnostics in case of unexpected errors or malfunctioning.\\r\\n\\r\\n## Overriding monitoring configuration at runtime\\r\\n\\r\\nFinally, there could be situations where you need to quickly change the logging behavior of a certain category in production, and you don\\'t want to make a whole deployment just for a change in the *host.json* file. For such cases, you can override the [host.json values](functions-host-json#override-hostjson-values).\\r\\n\\r\\nTo configure these values at App settings level (and avoid redeployment on just *host.json* changes), you should override specific `host.json` values by creating an equivalent value as an application setting. When the runtime finds an application setting in the format `AzureFunctionsJobHost__path__to__setting`, it overrides the equivalent `host.json` setting located at `path.to.setting` in the JSON. When expressed as an application setting, a double underscore (`__`) replaces the dot (`.`) used to indicate JSON hierarchy. For example, you can use the following app settings to configure individual function log levels in `host.json`.\\r\\n\\r\\n| Host.json path | App setting |\\r\\n| --- | --- |\\r\\n| logging.logLevel.default | AzureFunctionsJobHost\\\\_\\\\_logging\\\\_\\\\_logLevel\\\\_\\\\_default |\\r\\n| logging.logLevel.Host.Aggregator | AzureFunctionsJobHost\\\\_\\\\_logging\\\\_\\\\_logLevel\\\\_\\\\_Host.Aggregator |\\r\\n| logging.logLevel.Function | AzureFunctionsJobHost\\\\_\\\\_logging\\\\_\\\\_logLevel\\\\_\\\\_Function |\\r\\n| logging.logLevel.Function.Function1 | AzureFunctionsJobHost\\\\_\\\\_logging\\\\_\\\\_logLevel\\\\_\\\\_Function.Function1 |\\r\\n| logging.logLevel.Function.Function1.User | AzureFunctionsJobHost\\\\_\\\\_logging\\\\_\\\\_logLevel\\\\_\\\\_Function.Function1.User |\\r\\n\\r\\nYou can override the settings directly at the Azure portal Function App Configuration pane or by using an Azure CLI or PowerShell script.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz functionapp config appsettings set --name MyFunctionApp --resource-group MyResourceGroup --settings \"AzureFunctionsJobHost__logging__logLevel__Host.Aggregator=Information\"\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n```powershell\\r\\nUpdate-AzFunctionAppSetting -Name MyAppName -ResourceGroupName MyResourceGroupName -AppSetting @{\"AzureFunctionsJobHost__logging__logLevel__Host.Aggregator\" = \"Information\"}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nOverriding the `host.json` through changing app settings will restart your function app. App settings that contain a period aren\\'t supported when running on Linux in an Elastic Premium plan or a Dedicated (App Service) plan. In these hosting environments, you should continue to use the *host.json* file.\\r\\n\\r\\n## Monitor function apps using Health check\\r\\n\\r\\nYou can use the Health Check feature to monitor function apps on the Premium (Elastic Premium) and Dedicated (App Service) plans. Health check isn\\'t an option for the Consumption plan. To learn how to configure it, see [Monitor App Service instances using Health check](../app-service/',\n",
       "  'metadata': {}},\n",
       " {'_id': '6152970d-01ac-18a7-1a87-b5e89168f86a',\n",
       "  'title': 'How to use a secured storage account with Azure Functions',\n",
       "  'text': \"# How to use a secured storage account with Azure Functions\\r\\n\\r\\nThis article shows you how to connect your function app to a secured storage account. For an in-depth tutorial on how to create your function app with inbound and outbound access restrictions, see the [Integrate with a virtual network](functions-create-vnet) tutorial. To learn more about Azure Functions and networking, see [Azure Functions networking options](functions-networking-options).\\r\\n\\r\\n## Restrict your storage account to a virtual network\\r\\n\\r\\nWhen you create a function app, you either create a new storage account or link to an existing one. Currently, only the Azure portal, [ARM template deployments](functions-infrastructure-as-code?tabs=json&amp;pivots=premium-plan#secured-deployments), and [Bicep deployments](functions-infrastructure-as-code?tabs=bicep&amp;pivots=premium-plan#secured-deployments) support function app creation with an existing secured storage account.\\r\\n\\r\\nNote\\r\\n\\r\\nSecured storage accounts are supported for all tiers of the [Dedicated (App Service) plan](dedicated-plan) and the [Elastic Premium plan](functions-premium-plan). They're also supported by the [Flex Consumption plan](flex-consumption-plan). The [Consumption plan](consumption-plan) doesn't support virtual networks.\\r\\n\\r\\nFor a list of all restrictions on storage accounts, see [Storage account requirements](storage-considerations#storage-account-requirements).\\r\\n\\r\\n## Secure storage during function app creation\\r\\n\\r\\nYou can create a function app, along with a new storage account that is secured behind a virtual network. The following sections show you how to create these resources by using either the Azure portal or by using deployment templates.\\r\\n\\r\\n**Azure portal**\\r\\nComplete the steps in [Create a function app in a Premium plan](functions-create-vnet#create-a-function-app-in-a-premium-plan). This section of the virtual networking tutorial shows you how to create a function app that connects to storage over private endpoints.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen you create your function app in the Azure portal, you can also choose an existing secured storage account in the **Storage** tab. However, you must configure the appropriate networking on the function app so that it can connect through the virtual network used to secure the storage account. If you don't have permissions to configure networking or you haven't fully prepared your network, select **Configure networking after creation** in the **Networking** tab. You can configure networking for your new function app in the portal under **Settings** &gt; **Networking**.\\r\\n\\r\\n**Deployment templates**\\r\\nUse Bicep files or Azure Resource Manager (ARM) templates to create a secured function app and storage account resources. When you create a secured storage account in an automated deployment, you must set the `vnetContentShareEnabled` site property, create the file share as part of your deployment, and set the `WEBSITE_CONTENTSHARE` app setting to the name of the file share. For more information, including links to example deployments, see [Secured deployments](functions-infrastructure-as-code?pivots=premium-plan#secured-deployments).\\r\\n\\r\\n## Secure storage for an existing function app\\r\\n\\r\\nWhen you have an existing function app, you can directly configure networking on the storage account being used by the app. However, this process results in your function app being down while you configure networking and while your function app restarts.\\r\\n\\r\\nTo minimize downtime, you can instead swap-out an existing storage account for a new, secured storage account.\\r\\n\\r\\n### 1. Enable virtual network integration\\r\\n\\r\\nAs a prerequisite, you need to enable virtual network integration for your function app:\\r\\n\\r\\n1. Choose a function app with a storage account that doesn't have service endpoints or private endpoints enabled.\\r\\n2. [Enable virtual network integration](functions-networking-options#enable-virtual-network-integration) for your function app.\\r\\n\\r\\n### 2. Create a secured storage account\\r\\n\\r\\nSet up a secured storage account for your function app:\\r\\n\\r\\n1. [Create a second storage account](../storage/common/storage-account-create). This storage account is the secured storage account for your function app to use instead of its original unsecured storage account. You can also use an existing storage account not already being used by Functions.\\r\\n2. Save the connection string for this storage account to use later.\\r\\n3. [Create a file share](../storage/files/storage-how-to-create-file-share#create-a-file-share) in the new storage account. For your convenience, you can use the same file share name from your original storage account. Otherwise, if you use a new file share name, you must update your app setting.\\r\\n4. Secure the new storage account in one of the following ways:\\r\\n\\r\\n    - [Create a private endpoint](../storage/common/storage-private-endpoints#creating-a-private-endpoint). As you set up your private endpoint connection, create private endpoints for the `file` and `blob` subresources. For Durable Functions, you must also make `queue` and `table` subresources accessible through private endpoints. If you're using a custom or on-premises Domain Name System (DNS) server, [configure your DNS server](../storage/common/storage-private-endpoints#dns-changes-for-private-endpoints) to resolve to the new private endpoints.\\r\\n    - [Restrict traffic to specific subnets](../storage/common/storage-network-security#grant-access-from-a-virtual-network). Ensure your function app is network integrated with an allowed subnet and that the subnet has a service endpoint to `Microsoft.Storage`.\\r\\n5. Copy the file and blob content from the current storage account used by the function app to the newly secured storage account and file share. [AzCopy](../storage/common/storage-use-azcopy-blobs-copy) and [Azure Storage Explorer](https://techcommunity.microsoft.com/t5/azure-developer-community-blog/azure-tips-and-tricks-how-to-move-azure-storage-blobs-between/ba-p/3545304) are common methods. If you use Azure Storage Explorer, you might need to allow your client IP address access to your storage account's firewall.\\r\\n\\r\\nNow you're ready to configure your function app to communicate with the newly secured storage account.\\r\\n\\r\\n### 3. Enable application and configuration routing\\r\\n\\r\\nNote\\r\\n\\r\\nThese configuration steps are required only for the [Elastic Premium](functions-premium-plan) and [Dedicated (App Service)](dedicated-plan) hosting plans. The [Flex Consumption plan](flex-consumption-plan) doesn't require site settings to configure networking.\\r\\n\\r\\nYou're now ready to route your function app's traffic to go through the virtual network:\\r\\n\\r\\n1. Enable [application routing](../app-service/overview-vnet-integration#application-routing) to route your app's traffic to the virtual network:\\r\\n\\r\\n    1. In your function app, expand **Settings**, and then select **Networking**. In the **Networking** page, under **Outbound traffic configuration**, select the subnet associated with your virtual network integration.\\r\\n    2. In the new page, under **Application routing**, select **Outbound internet traffic**.\\r\\n2. Enable [content share routing](../app-service/overview-vnet-integration#content-share) to enable your function app to communicate with your new storage account through its virtual network. In the same page as the previous step, under **Configuration routing**, select **Content storage**.\\r\\n\\r\\nNote\\r\\n\\r\\nYou must take special care when routing to the content share in a storage account shared by multiple function apps in the same plan. For more information, see [Consistent routing through virtual networks](storage-considerations#consistent-routing-through-virtual-networks) in the Storage considerations article.\\r\\n\\r\\n### 4. Update application settings\\r\\n\\r\\nFinally, you need to update your application settings to point to the new secure storage account:\\r\\n\\r\\n1. In your function app, expand **Settings**, and then select **Environment variables**.\\r\\n2. In the **App settings** tab, update the following settings by selecting each setting, editing it, and then selecting **Apply**:\\r\\n\\r\\n    | Setting name | Value | Comment |\\r\\n    | --- | --- | --- |\\r\\n    | [`AzureWebJobsStorage`](functions-app-settings#azurewebjobsstorage) | Storage connection string | Use the connection string for your new secured storage account, which you saved earlier. |\\r\\n    | [`WEBSITE_CONTENTAZUREFILECONNECTIONSTRING`](functions-app-settings#website_contentazurefileconnectionstring) | Storage connection string | Use the connection string for your new secured storage account, which you saved earlier. |\\r\\n    | [`WEBSITE_CONTENTSHARE`](functions-app-settings#website_contentshare) | File share | Use the name of the file share created in the secured storage account where the project deployment files reside. |\\r\\n3. Select **Apply**, and then **Confirm** to save the new application settings in the function app.\\r\\n\\r\\n    The function app restarts.\\r\\n\\r\\nAfter the function app finishes restarting, it connects to the secured storage account.\",\n",
       "  'metadata': {}},\n",
       " {'_id': '91012c20-a7d6-4442-8c01-d0529994ade0',\n",
       "  'title': 'Azure Functions Consumption plan hosting',\n",
       "  'text': \"# Azure Functions Consumption plan hosting\\r\\n\\r\\nWhen you're using the Consumption plan, instances of the Azure Functions host are dynamically added and removed based on the number of incoming events. The Consumption plan, along with the [Flex Consumption plan](flex-consumption-plan), is a fully *serverless* hosting option for Azure Functions.\\r\\n\\r\\n## Benefits\\r\\n\\r\\nThe Consumption plan scales automatically, even during periods of high load. When running functions in a Consumption plan, you're charged for compute resources only when your functions are running. On a Consumption plan, a function execution times out after a configurable period of time.\\r\\n\\r\\nFor a comparison of the Consumption plan against the other plan and hosting types, see [function scale and hosting options](functions-scale).\\r\\n\\r\\nTip\\r\\n\\r\\nIf you want the benefits of dynamic scale and execution-only billing, but also need to integrate your app with virtual networks, you should instead consider hosting your app in the [Flex Consumption plan](flex-consumption-plan).\\r\\n\\r\\n## Billing\\r\\n\\r\\nBilling is based on number of executions, execution time, and memory used. Usage is aggregated across all functions within a function app. For more information, see the [Azure Functions pricing page](https://azure.microsoft.com/pricing/details/functions/).\\r\\n\\r\\nTo learn more about how to estimate costs when running in a Consumption plan, see [Understanding Consumption plan costs](functions-consumption-costs).\\r\\n\\r\\n## Create a Consumption plan function app\\r\\n\\r\\nWhen you create a function app in the Azure portal, the Consumption plan is the default. When using APIs to create your function app, you don't have to first create an App Service plan as you do with Premium and Dedicated plans.\\r\\n\\r\\nIn Consumption plan hosting, each function app typically runs in its own plan. In the Azure portal or in code, you may also see the Consumption plan referred to as `Dynamic` or `Y1`.\\r\\n\\r\\nUse the following links to learn how to create a serverless function app in a Consumption plan, either programmatically or in the Azure portal:\\r\\n\\r\\n- [Azure CLI](scripts/functions-cli-create-serverless)\\r\\n- [Azure portal](functions-get-started)\\r\\n- [Azure Resource Manager template](functions-create-first-function-resource-manager)\\r\\n\\r\\nYou can also create function apps in a Consumption plan when you publish a Functions project from [Visual Studio Code](create-first-function-vs-code-csharp#publish-the-project-to-azure) or [Visual Studio](functions-create-your-first-function-visual-studio#publish-the-project-to-azure).\\r\\n\\r\\n## Multiple apps in the same plan\\r\\n\\r\\nThe general recommendation is for each function app to have its own Consumption plan. However, if needed, function apps in the same region can be assigned to the same Consumption plan. Keep in mind that there is a [limit to the number of function apps that can run in a Consumption plan](functions-scale#service-limits). Function apps in the same plan still scale independently of each other.\",\n",
       "  'metadata': {}},\n",
       " {'_id': '5cae03dd-1d3c-0562-0f98-96e31f469f45',\n",
       "  'title': 'Linux container support in Azure Functions',\n",
       "  'text': '# Linux container support in Azure Functions\\r\\n\\r\\nWhen you plan and develop your individual functions to run in Azure Functions, you\\'re typically focused on the code itself. Azure Functions makes it easy to deploy just your code project to a function app in Azure. When you deploy your code project to a function app that runs on Linux, the project runs in a container that is created for you automatically. This container is managed by Functions.\\r\\n\\r\\nFunctions also supports containerized function app deployments. In a containerized deployment, you create your own function app instance in a local Docker container from a supported based image. You can then deploy this *containerized* function app to a hosting environment in Azure. Creating your own function app container lets you customize or otherwise control the immediate runtime environment of your function code.\\r\\n\\r\\nImportant\\r\\n\\r\\nWhen creating your own containers, you are required to keep the base image of your container updated to the latest supported base image. Supported base images for Azure Functions are language-specific and are found in the [Azure Functions base image repos](https://mcr.microsoft.com/catalog?search=functions).\\r\\n\\r\\nThe Functions team is committed to publishing monthly updates for these base images. Regular updates include the latest minor version updates and security fixes for both the Functions runtime and languages. You should regularly update your container from the latest base image and redeploy the updated version of your container.\\r\\n\\r\\n## Container hosting options\\r\\n\\r\\nThere are several options for hosting your containerized function apps in Azure:\\r\\n\\r\\n| Hosting option | Benefits |\\r\\n| --- | --- |\\r\\n| **[Azure Container Apps](functions-container-apps-hosting)** | Azure Functions provides integrated support for developing, deploying, and managing containerized function apps on [Azure Container Apps](../container-apps/overview). This enables you to manage your apps using the same Functions tools and pages in the Azure portal. Use Azure Container Apps to host your function app containers when you need to run your event-driven functions in Azure in the same environment as other microservices, APIs, websites, workflows, or any container hosted programs. Container Apps hosting lets you run your functions in a managed Kubernetes-based environment with built-in support for open-source monitoring, mTLS, Dapr, and KEDA. Supports scale-to-zero and provides a serverless pay-for-what-you-use hosting model. You can also request dedicated hardware, even GPUs, by using workload profiles. *Recommended hosting option for running containerized function apps on Azure.* |\\r\\n| **Azure Arc-enabled Kubernetes clusters (preview)** | You can host your function apps on Azure Arc-enabled Kubernetes clusters as either a [code-only deployment](create-first-function-arc-cli) or in a [custom Linux container](create-first-function-arc-custom-container). Azure Arc lets you attach Kubernetes clusters so that you can manage and configure them in Azure. *Hosting Azure Functions containers on Azure Arc-enabled Kubernetes clusters is currently in preview.* |\\r\\n| **[Azure Functions](functions-how-to-custom-container?pivots=azure-functions#azure-portal-create-using-containers)** | You can host your containerized function apps in Azure Functions by running the container in either an [Elastic Premium plan](functions-premium-plan) or a [Dedicated plan](dedicated-plan). Premium plan hosting provides you with the benefits of dynamic scaling. You might want to use Dedicated plan hosting to take advantage of existing unused App Service plan resources. |\\r\\n| **[Kubernetes](functions-kubernetes-keda)** | Because the Azure Functions runtime provides flexibility in hosting where and how you want, you can host and manage your function app containers directly in Kubernetes clusters. [KEDA](https://keda.sh) (Kubernetes-based Event Driven Autoscaling) pairs seamlessly with the Azure Functions runtime and tooling to provide event driven scale in Kubernetes. Just keep in mind that running your containerized function apps on Kubernetes, either by using KEDA or by direct deployment, is an open-source effort that you can use free of cost, with best-effort support provided by contributors and from the community. You\\'re responsible for maintaining your own function app containers in a cluster, even when deploying to Azure Kubernetes Service (AKS). |\\r\\n\\r\\n## Feature support comparison\\r\\n\\r\\nThe degree to which various features and behaviors of Azure Functions are supported when running your function app in a container depends on the container hosting option you choose.\\r\\n\\r\\n| Feature/behavior | [Container Apps (integrated)](functions-container-apps-hosting) | [Container Apps (direct)](../container-apps/overview) | [Premium plan](functions-premium-plan) | [Dedicated plan](dedicated-plan) | [Kubernetes](functions-kubernetes-keda) |\\r\\n| --- | --- | --- | --- | --- | --- |\\r\\n| Product support | Yes | No | Yes | Yes | No |\\r\\n| Functions portal integration | Yes | No | Yes | Yes | No |\\r\\n| [Event-driven scaling](event-driven-scaling) | Yes^5^ | Yes ([scale rules](../container-apps/scale-app#scale-rules)) | Yes | No | No |\\r\\n| Maximum scale (instances) | 1000^1^ | 1000^1^ | 100^2^ | 10-30^3^ | Varies by cluster |\\r\\n| [Scale-to-zero instances](event-driven-scaling#scale-in-behaviors) | Yes | Yes | No | No | KEDA |\\r\\n| Execution time limit | Unbounded^6^ | Unbounded^6^ | Unbounded^7^ | Unbounded^8^ | None |\\r\\n| [Core Tools deployment](functions-run-local#deploy-containers) | [`func azurecontainerapps`](functions-core-tools-reference#func-azurecontainerapps-deploy) | No | No | No | [`func kubernetes`](functions-core-tools-reference#func-kubernetes-deploy) |\\r\\n| [Revisions](../container-apps/revisions) | No | Yes | No | No | No |\\r\\n| [Deployment slots](functions-deployment-slots) | No | No | Yes | Yes | No |\\r\\n| [Streaming logs](streaming-logs) | Yes | [Yes](../container-apps/log-streaming) | Yes | Yes | No |\\r\\n| [Console access](../container-apps/container-console) | Not currently available^4^ | Yes | Yes (using [Kudu](functions-how-to-custom-container#enable-ssh-connections)) | Yes (using [Kudu](functions-how-to-custom-container#enable-ssh-connections)) | Yes (in pods [using `kubectl`](https://kubernetes.io/docs/reference/kubectl/)) |\\r\\n| Cold start mitigation | Minimum replicas | [Scale rules](../container-apps/scale-app#scale-rules) | [Always-ready/pre-warmed instances](functions-premium-plan#eliminate-cold-starts) | n/a | n/a |\\r\\n| [App Service authentication](../app-service/overview-authentication-authorization) | Not currently available^4^ | Yes | Yes | Yes | No |\\r\\n| [Custom domain names](../app-service/app-service-web-tutorial-custom-domain) | Not currently available^4^ | Yes | Yes | Yes | No |\\r\\n| [Private key certificates](../app-service/overview-tls) | Not currently available^4^ | Yes | Yes | Yes | No |\\r\\n| Virtual networks | Yes | Yes | Yes | Yes | Yes |\\r\\n| Availability zones | Yes | Yes | Yes | Yes | Yes |\\r\\n| Diagnostics | Not currently available^4^ | [Yes](../container-apps/troubleshooting#use-the-diagnose-and-solve-problems-tool) | [Yes](functions-diagnostics) | [Yes](functions-diagnostics) | No |\\r\\n| Dedicated hardware | Yes ([workload profiles](../container-apps/workload-profiles-overview)) | Yes ([workload profiles](../container-apps/workload-profiles-overview)) | No | Yes | Yes |\\r\\n| Dedicated GPUs | Yes ([workload profiles](../container-apps/workload-profiles-overview)) | Yes ([workload profiles](../container-apps/workload-profiles-overview)) | No | No | Yes |\\r\\n| [Configurable memory/CPU count](../container-apps/workload-profiles-overview) | Yes | Yes | No | No | Yes |\\r\\n| \"Free grant\" option | [Yes](../container-apps/billing#consumption-plan) | [Yes](../container-apps/billing#consumption-plan) | No | No | No |\\r\\n| Pricing details | [Container Apps billing](../container-apps/billing) | [Container Apps billing](../container-apps/billing) | [Premium plan billing](functions-premium-plan#billing) | [Dedicated plan billing](dedicated-plan#billing) | [AKS pricing](/en-us/azure/aks/free-standard-pricing-tiers) |\\r\\n| Service name requirements | 2-32 characters: limited to lowercase letters, numbers, and hyphens. Must start with a letter and end with an alphanumeric character. | 2-32 characters: limited to lowercase letters, numbers, and hyphens. Must start with a letter and end with an alphanumeric character. | Less than 64 characters: limited to alphanumeric characters and hyphens. Can\\'t start with or end in a hyphen. | Less than 64 characters: limited to alphanumeric characters and hyphens. Can\\'t start with or end in a hyphen. | Less than 253 characters: limited to alphanumeric characters and hyphens. Must start and end with an alphanumeric character. |\\r\\n\\r\\n1. On Container Apps, the default is 10 instances, but you can set the [maximum number of replicas](../container-apps/scale-app#scale-definition), which has an overall maximum of 1000. This setting is honored as long as there\\'s enough cores quota available. When you create your function app from the Azure portal, you\\'re limited to 300 instances.\\r\\n2. In some regions, Linux apps on a Premium plan can scale to 100 instances. For more information, see the [Premium plan article](functions-premium-plan#region-max-scale-out).\\r\\n3. For specific limits for the various App Service plan options, see the [App Service plan limits](../azure-resource-manager/management/azure-subscription-service-limits#app-service-limits).\\r\\n4. Feature parity is a goal of integrated hosting on Azure Container Apps.\\r\\n5. Requires [KEDA](functions-kubernetes-keda); supported by most triggers. To learn which triggers support event-driven scaling, see [Considerations for Container Apps hosting](functions-container-apps-hosting#considerations-for-container-apps-hosting).\\r\\n6. When the [minimum number of replicas](../container-apps/scale-app#scale-definition) is set to zero, the default timeout depends on the specific triggers used in the app.\\r\\n7. There\\'s no maximum execution timeout duration enforced. However, the grace period given to a function execution is 60 minutes [during scale in](event-driven-scaling#scale-in-behaviors), and a grace period of 10 minutes is given during platform updates.\\r\\n8. Requires the App Service plan be set to [Always On](dedicated-plan#always-on). A grace period of 10 minutes is given during platform updates.\\r\\n\\r\\n## Getting started\\r\\n\\r\\nUse these links to get started working with Azure Functions in Linux containers:\\r\\n\\r\\n| I want to... | See article: |\\r\\n| --- | --- |\\r\\n| Create my first containerized functions | [Create a function app in a local Linux container](functions-create-container-registry) |\\r\\n| Create and deploy functions to Azure Container Apps | [Create your first containerized functions on Azure Container Apps](functions-deploy-container-apps) |\\r\\n| Create and deploy containerized functions to Azure Functions | [Create your first containerized Azure Functions](functions-deploy-container) |\\r\\n| Create and deploy functions to Azure Arc-enabled Kubernetes | [Create your first containerized Azure Functions on Azure Arc (preview)](create-first-function-arc-custom-container) |',\n",
       "  'metadata': {}},\n",
       " {'_id': '9d3bcdbe-e7b3-6787-cbcd-103c1dd09686',\n",
       "  'title': 'Quickstart: Create a function app on Azure Arc',\n",
       "  'text': '# Create your first function on Azure Arc (preview)\\r\\n\\r\\nIn this quickstart, you create an Azure Functions project and deploy it to a function app running on an [Azure Arc-enabled Kubernetes cluster](/en-us/azure/azure-arc/kubernetes/overview). To learn more, see [App Service, Functions, and Logic Apps on Azure Arc](../app-service/overview-arc-integration). This scenario only supports function apps running on Linux.\\r\\n\\r\\nNote\\r\\n\\r\\nSupport for running functions on an Azure Arc-enabled Kubernetes cluster is currently in preview.\\r\\n\\r\\nPublishing PowerShell function projects to Azure Arc-enabled Kubernetes clusters isn\\'t currently supported. If you need to deploy PowerShell functions to Azure Arc-enabled Kubernetes clusters, [create your function app in a container](create-first-function-arc-custom-container).\\r\\n\\r\\nIf you need to customize the container in which your function app runs, instead see [Create your first containerized functions on Azure Arc (preview)](create-first-function-arc-custom-container).\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nOn your local computer:\\r\\n\\r\\n**C#**\\r\\n- [.NET SDK](https://dotnet.microsoft.com/download)\\r\\n- [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later\\r\\n\\r\\n**JavaScript**\\r\\n- [Node.js](https://nodejs.org/) version 18. Node.js version 14 is also supported.\\r\\n- [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later\\r\\n\\r\\n**Python**\\r\\n- [Python versions that are supported by Azure Functions](supported-languages#languages-by-runtime-version)\\r\\n- [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later\\r\\n\\r\\n**PowerShell**\\r\\n- [PowerShell 7](/en-us/powershell/scripting/install/installing-powershell-core-on-windows)\\r\\n- [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later\\r\\n- PowerShell 7 requires version 1.2.5 of the connectedk8s Azure CLI extension, or a later version. It also requires version 0.1.3 of the appservice-kube Azure CLI extension, or a later version. Make sure you install the correct version of both of these extensions as you complete this quickstart article.\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\n## Create an App Service Kubernetes environment\\r\\n\\r\\nBefore you begin, you must [create an App Service Kubernetes environment](../app-service/manage-create-arc-environment) for an Azure Arc-enabled Kubernetes cluster.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen you create the environment, make sure to make note of both the custom location name and the name of the resource group that contains the custom location. You can use these to find the custom location ID, which you\\'ll need when creating your function app in the environment.\\r\\n\\r\\nIf you didn\\'t create the environment, check with your cluster administrator.\\r\\n\\r\\n## Add Azure CLI extensions\\r\\n\\r\\nLaunch the Bash environment in [Azure Cloud Shell](/en-us/azure/cloud-shell/get-started).\\r\\n\\r\\nBecause these CLI commands are not yet part of the core CLI set, add them with the following commands:\\r\\n\\r\\n```azurecli\\r\\naz extension add --upgrade --yes --name customlocation\\r\\naz extension remove --name appservice-kube\\r\\naz extension add --upgrade --yes --name appservice-kube\\r\\n```\\r\\n\\r\\n## Create the local function project\\r\\n\\r\\nIn Azure Functions, a function project is the unit of deployment and execution for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations. In this section, you create a function project that contains a single function.\\r\\n\\r\\n1. Run the `func init` command, as follows, to create a functions project in a folder named *LocalFunctionProj* with the specified runtime:\\r\\n\\r\\n**C#**\\r\\n\\r\\n    ```console\\r\\n    func init LocalFunctionProj --dotnet\\r\\n    ```\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n    ```console\\r\\n    func init LocalFunctionProj --javascript\\r\\n    ```\\r\\n\\r\\n**Python**\\r\\nPython requires a virtual environment, the commands for which differ between bash and a Windows command line.\\r\\n\\r\\n    - bash:\\r\\n\\r\\n        ```bash\\r\\n        python -m venv .venv\\r\\n        source .venv/bin/activate\\r\\n        ```\\r\\n    - command line:\\r\\n\\r\\n        ```cmd\\r\\n        py -m venv .venv\\r\\n        .venv\\\\scripts\\\\activate\\r\\n        ```\\r\\n\\r\\nNow, you create the project inside the virtual environment.\\r\\n\\r\\n    ```console\\r\\n    func init LocalFunctionProj --python\\r\\n    ```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n    ```console\\r\\n    func init LocalFunctionProj --powershell\\r\\n    ```\\r\\n2. Navigate into the project folder:\\r\\n\\r\\n    ```console\\r\\n    cd LocalFunctionProj\\r\\n    ```\\r\\n\\r\\n    This folder contains various files for the project, including configurations files named [local.settings.json](functions-develop-local#local-settings-file) and [host.json](functions-host-json). By default, the *local.settings.json* file is excluded from source control in the *.gitignore* file. This exclusion is because the file can contain secrets that are downloaded from Azure.\\r\\n3. Add a function to your project by using the following command, where the `--name` argument is the unique name of your function (HttpExample) and the `--template` argument specifies the function\\'s trigger (HTTP).\\r\\n\\r\\n    ```console\\r\\n    func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\"\\r\\n    ```\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder.\\r\\n\\r\\n    ```console\\r\\n    func start\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following lines must appear:\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown above, you likely started the host from outside the root folder of the project. In that case, use **Ctrl**+**C** to stop the host, go to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your HTTP function from this output to a browser and append the query string `?name=<YOUR_NAME>`, making the full URL like `http://localhost:7071/api/HttpExample?name=Functions`. The browser should display a response message that echoes back your query string value. The terminal in which you started your project also shows log output as you make requests.\\r\\n3. When you\\'re done, press Ctrl + C and type `y` to stop the functions host.\\r\\n\\r\\n## Get the custom location\\r\\n\\r\\nTo be able to create a function app in a custom location, you\\'ll need to get information about the environment.\\r\\n\\r\\nGet the following information about the custom location from your cluster administrator (see [Create a custom location](/en-us/azure/app-service/manage-create-arc-environment#create-a-custom-location)).\\r\\n\\r\\n```azurecli\\r\\ncustomLocationGroup=\"<resource-group-containing-custom-location>\"\\r\\ncustomLocationName=\"<name-of-custom-location>\"\\r\\n```\\r\\n\\r\\nGet the custom location ID for the next step.\\r\\n\\r\\n```azurecli\\r\\ncustomLocationId=$(az customlocation show \\\\\\r\\n    --resource-group $customLocationGroup \\\\\\r\\n    --name $customLocationName \\\\\\r\\n    --query id \\\\\\r\\n    --output tsv)\\r\\n```\\r\\n\\r\\n## Create Azure resources\\r\\n\\r\\nBefore you can deploy your function code to your new App Service Kubernetes environment, you need to create two more resources:\\r\\n\\r\\n- A [Storage account](../storage/common/storage-account-create). While this article creates a storage account, in some cases a storage account may not be required. For more information, see [Azure Arc-enabled clusters](storage-considerations#azure-arc-enabled-clusters) in the storage considerations article.\\r\\n- A function app, which provides the context for executing your function code. The function app runs in the App Service Kubernetes environment and maps to your local function project. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nNote\\r\\n\\r\\nFunction apps run in an App Service Kubernetes environment on a Dedicated (App Service) plan. When you create your function app without an existing plan, the correct plan is created for you.\\r\\n\\r\\n### Create Storage account\\r\\n\\r\\nUse the [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command to create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n```azurecli\\r\\naz storage account create --name <STORAGE_NAME> --location westeurope --resource-group myResourceGroup --sku Standard_LRS\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nIn some cases, a storage account may not be required. For more information, see [Azure Arc-enabled clusters](storage-considerations#azure-arc-enabled-clusters) in the storage considerations article.\\r\\n\\r\\nIn the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements). The `--location` value is a standard Azure region.\\r\\n\\r\\n### Create the function app\\r\\n\\r\\nRun the [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command to create a new function app in the environment.\\r\\n\\r\\n**C#**\\r\\n\\r\\n```azurecli\\r\\naz functionapp create --resource-group MyResourceGroup --name <APP_NAME> --custom-location <CUSTOM_LOCATION_ID> --storage-account <STORAGE_NAME> --functions-version 4 --runtime dotnet \\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```azurecli\\r\\naz functionapp create --resource-group MyResourceGroup --name <APP_NAME> --custom-location <CUSTOM_LOCATION_ID> --storage-account <STORAGE_NAME> --functions-version 4 --runtime node --runtime-version 18\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```azurecli\\r\\naz functionapp create --resource-group MyResourceGroup --name <APP_NAME> --custom-location <CUSTOM_LOCATION_ID> --storage-account <STORAGE_NAME> --functions-version 4 --runtime python --runtime-version 3.8\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n```azurecli\\r\\naz functionapp create --resource-group myResourceGroup --name <APP_NAME> --custom-location <CUSTOM_LOCATION_ID> --storage-account <STORAGE_NAME> --functions-version 4 --runtime powershell --runtime-version 7.0\\r\\n```\\r\\n\\r\\nIn this example, replace `<CUSTOM_LOCATION_ID>` with the ID of the custom location you determined for the App Service Kubernetes environment. Also, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, and replace `<APP_NAME>` with a globally unique name appropriate to you.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nAfter you\\'ve successfully created your function app in Azure, you\\'re now ready to deploy your local functions project by using the [`func azure functionapp publish`](functions-run-local#project-file-deployment) command.\\r\\n\\r\\nIn your root project folder, run this [`func azure functionapp publish`](functions-core-tools-reference#func-azure-functionapp-publish) command:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp publish <APP_NAME>\\r\\n```\\r\\n\\r\\nIn this example, replace `<APP_NAME>` with the name of your app. A successful deployment shows results similar to the following output (truncated for simplicity):\\r\\n\\r\\n```\\r\\n...\\r\\n\\r\\nGetting site publishing info...\\r\\nCreating archive for current directory...\\r\\nPerforming remote build for functions project.\\r\\n\\r\\n...\\r\\n\\r\\nDeployment successful.\\r\\nRemote build succeeded!\\r\\nSyncing triggers...\\r\\nFunctions in msdocs-azurefunctions-qs:\\r\\n    HttpExample - [httpTrigger]\\r\\n        Invoke url: https://msdocs-azurefunctions-qs.azurewebsites.net/api/httpexample\\r\\n```\\r\\n\\r\\nBecause it can take some time for a full deployment to complete on an Azure Arc-enabled Kubernetes cluster, you may want to rerun the following command to verify your published functions:\\r\\n\\r\\n```command\\r\\nfunc azure functionapp list-functions\\r\\n```\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"',\n",
       "  'metadata': {}},\n",
       " {'_id': '9b90f59f-34f0-fc9a-bdb1-f37cbd425331',\n",
       "  'title': 'Create your first containerized Azure Functions on Azure Arc',\n",
       "  'text': '# Create your first containerized Azure Functions on Azure Arc (preview) (programming-language-csharp)\\r\\n\\r\\nIn this article, you create a function app running in a Linux container and deploy it to an [Azure Arc-enabled Kubernetes cluster](/en-us/azure/azure-arc/kubernetes/overview) from a container registry. When you create your own container, you can customize the execution environment for your function app. To learn more, see [App Service, Functions, and Logic Apps on Azure Arc](../app-service/overview-arc-integration).\\r\\n\\r\\nNote\\r\\n\\r\\nSupport for deploying a custom container to an Azure Arc-enabled Kubernetes cluster is currently in preview.\\r\\n\\r\\nYou can also publish your functions to an Azure Arc-enabled Kubernetes cluster without first creating a container. To learn more, see [Create your first function on Azure Arc (preview)](create-first-function-arc-cli)\\r\\n\\r\\n## Choose your development language\\r\\n\\r\\nFirst, you use Azure Functions tools to create your project code as a function app in a Docker container using a language-specific Linux base image. Make sure to select your language of choice at the top of the article.\\r\\n\\r\\nCore Tools automatically generates a Dockerfile for your project that uses the most up-to-date version of the correct base image for your functions language. You should regularly update your container from the latest base image and redeploy from the updated version of your container. For more information, see [Creating containerized function apps](functions-how-to-custom-container#creating-containerized-function-apps).\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nBefore you begin, you must have the following requirements in place:\\r\\n\\r\\n- Install the [.NET 8.0 SDK](https://dotnet.microsoft.com/download).\\r\\n- Install [Azure Functions Core Tools](functions-run-local#v2) version 4.0.5198, or a later version.\\r\\n\\r\\n- [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or a later version.\\r\\n\\r\\nIf you don\\'t have an [Azure subscription](../guides/developer/azure-developer-guide#understanding-accounts-subscriptions-and-billing), create an [Azure free account](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio) before you begin.\\r\\n\\r\\nTo publish the containerized function app image you create to a container registry, you need a Docker ID and [Docker](https://docs.docker.com/install/) running on your local computer. If you don\\'t have a Docker ID, you can [create a Docker account](https://hub.docker.com/signup).\\r\\n\\r\\n**Azure Container Registry**\\r\\nYou also need to complete the [Create a container registry](/en-us/azure/container-registry/container-registry-get-started-portal#create-a-container-registry) section of the Container Registry quickstart to create a registry instance. Make a note of your fully qualified login server name.\\r\\n\\r\\n**Docker Hub**\\r\\nYou should be all set.\\r\\n\\r\\n## Create and test the local functions project\\r\\n\\r\\nIn a terminal or command prompt, run the following command for your chosen language to create a function app project in the current folder:\\r\\n\\r\\n```console\\r\\nfunc init --worker-runtime dotnet-isolated --docker\\r\\n```\\r\\n\\r\\nThe `--docker` option generates a *Dockerfile* for the project, which defines a suitable container for use with Azure Functions and the selected runtime.\\r\\n\\r\\nUse the following command to add a function to your project, where the `--name` argument is the unique name of your function and the `--template` argument specifies the function\\'s trigger. `func new` creates a C# code file in your project.\\r\\n\\r\\n```console\\r\\nfunc new --name HttpExample --template \"HTTP trigger\"\\r\\n```\\r\\n\\r\\nTo test the function locally, start the local Azure Functions runtime host in the root of the project folder.\\r\\n\\r\\n```console\\r\\nfunc start  \\r\\n```\\r\\n\\r\\nAfter you see the `HttpExample` endpoint written to the output, navigate to that endpoint. You should see a welcome message in the response output.\\r\\n\\r\\nPress **Ctrl**+**C** (**Command**+**C** on macOS) to stop the host.\\r\\n\\r\\n## Build the container image and verify locally\\r\\n\\r\\n(Optional) Examine the *Dockerfile* in the root of the project folder. The *Dockerfile* describes the required environment to run the function app on Linux. The complete list of supported base images for Azure Functions can be found in the [Azure Functions base image page](https://hub.docker.com/_/microsoft-azure-functions-base).\\r\\n\\r\\nIn the root project folder, run the [docker build](https://docs.docker.com/engine/reference/commandline/build/) command, provide a name as `azurefunctionsimage`, and tag as `v1.0.0`. Replace `<DOCKER_ID>` with your Docker Hub account ID. This command builds the Docker image for the container.\\r\\n\\r\\n```console\\r\\ndocker build --tag <DOCKER_ID>/azurefunctionsimage:v1.0.0 .\\r\\n```\\r\\n\\r\\nWhen the command completes, you can run the new container locally.\\r\\n\\r\\nTo verify the build, run the image in a local container using the [docker run](https://docs.docker.com/engine/reference/commandline/run/) command, replace `<DOCKER_ID>` again with your Docker Hub account ID, and add the ports argument as `-p 8080:80`:\\r\\n\\r\\n```console\\r\\ndocker run -p 8080:80 -it <DOCKER_ID>/azurefunctionsimage:v1.0.0\\r\\n```\\r\\n\\r\\nAfter the image starts in the local container, browse to `http://localhost:8080/api/HttpExample`, which must display the same greeting message as before. Because the HTTP triggered function you created uses anonymous authorization, you can call the function running in the container without having to obtain an access key. For more information, see [authorization keys](functions-bindings-http-webhook-trigger#authorization-keys).\\r\\n\\r\\nAfter verifying the function app in the container, press **Ctrl**+**C** (**Command**+**C** on macOS) to stop execution.\\r\\n\\r\\n## Publish the container image to a registry\\r\\n\\r\\nTo make your container image available for deployment to a hosting environment, you must push it to a container registry.\\r\\n\\r\\n**Azure Container Registry**\\r\\nAzure Container Registry is a private registry service for building, storing, and managing container images and related artifacts. You should use a private registry service for publishing your containers to Azure services.\\r\\n\\r\\n1. Use this command to sign in to your registry instance using your current Azure credentials:\\r\\n\\r\\n    ```azurecli\\r\\n    az acr login --name <REGISTRY_NAME>\\r\\n    ```\\r\\n\\r\\n    In the previous command, replace `<REGISTRY_NAME>` with the name of your Container Registry instance.\\r\\n2. Use this command to tag your image with the fully qualified name of your registry login server:\\r\\n\\r\\n    ```docker\\r\\n    docker tag <DOCKER_ID>/azurefunctionsimage:v1.0.0 <LOGIN_SERVER>/azurefunctionsimage:v1.0.0 \\r\\n    ```\\r\\n\\r\\n    Replace `<LOGIN_SERVER>` with the fully qualified name of your registry login server and `<DOCKER_ID>` with your Docker ID.\\r\\n3. Use this command to push the container to your registry instance:\\r\\n\\r\\n    ```docker\\r\\n    docker push <LOGIN_SERVER>/azurefunctionsimage:v1.0.0\\r\\n    ```\\r\\n\\r\\n**Docker Hub**\\r\\nDocker Hub is a container registry that hosts images and provides image and container services.\\r\\n\\r\\n1. If you haven\\'t already signed in to Docker, do so with the [`docker login`](https://docs.docker.com/engine/reference/commandline/login/) command, replacing `<docker_id>` with your Docker Hub account ID. This command prompts you for your username and password. A \"sign in Succeeded\" message confirms that you\\'re signed in.\\r\\n\\r\\n    ```console\\r\\n    docker login\\r\\n    ```\\r\\n2. After you\\'ve signed in, push the image to Docker Hub by using the [`docker push`](https://docs.docker.com/engine/reference/commandline/push/) command, again replace the `<docker_id>` with your Docker Hub account ID.\\r\\n\\r\\n    ```console\\r\\n    docker push <DOCKER_ID>/azurefunctionsimage:v1.0.0\\r\\n    ```\\r\\n\\r\\n    Depending on your network speed, pushing the image for the first time might take a few minutes. Subsequent changes are pushed faster.\\r\\n\\r\\n## Create an App Service Kubernetes environment\\r\\n\\r\\nBefore you begin, you must [create an App Service Kubernetes environment](../app-service/manage-create-arc-environment) for an Azure Arc-enabled Kubernetes cluster.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen you create the environment, make sure to make note of both the custom location name and the name of the resource group that contains the custom location. You can use these to find the custom location ID, which you\\'ll need when creating your function app in the environment.\\r\\n\\r\\nIf you didn\\'t create the environment, check with your cluster administrator.\\r\\n\\r\\n## Add Azure CLI extensions\\r\\n\\r\\nLaunch the Bash environment in [Azure Cloud Shell](/en-us/azure/cloud-shell/get-started).\\r\\n\\r\\nBecause these CLI commands are not yet part of the core CLI set, add them with the following commands:\\r\\n\\r\\n```azurecli\\r\\naz extension add --upgrade --yes --name customlocation\\r\\naz extension remove --name appservice-kube\\r\\naz extension add --upgrade --yes --name appservice-kube\\r\\n```\\r\\n\\r\\n## Create Azure resources\\r\\n\\r\\nBefore you can deploy your container to your new App Service Kubernetes environment, you need to create two more resources:\\r\\n\\r\\n- A [Storage account](../storage/common/storage-account-create). While this article creates a storage account, in some cases a storage account may not be required. For more information, see [Azure Arc-enabled clusters](storage-considerations#azure-arc-enabled-clusters) in the storage considerations article.\\r\\n- A function app, which provides the context for running your container. The function app runs in the App Service Kubernetes environment and maps to your local function project. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nNote\\r\\n\\r\\nFunction apps run in an App Service Kubernetes environment on a Dedicated (App Service) plan. When you create your function app without an existing plan, a plan is created for you.\\r\\n\\r\\n### Create Storage account\\r\\n\\r\\nUse the [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command to create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n```azurecli\\r\\naz storage account create --name <STORAGE_NAME> --location westeurope --resource-group myResourceGroup --sku Standard_LRS\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nIn some cases, a storage account may not be required. For more information, see [Azure Arc-enabled clusters](storage-considerations#azure-arc-enabled-clusters) in the storage considerations article.\\r\\n\\r\\nIn the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements). The `--location` value is a standard Azure region.\\r\\n\\r\\n### Create the function app\\r\\n\\r\\nRun the [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command to create a new function app in the environment.\\r\\n\\r\\n**Azure Container Registry**\\r\\n\\r\\n```azurecli\\r\\naz functionapp create --name <APP_NAME> --custom-location <CUSTOM_LOCATION_ID> --storage-account <STORAGE_NAME> --resource-group AzureFunctionsContainers-rg --image <LOGIN_SERVER>/azurefunctionsimage:v1.0.0 --registry-username <USERNAME> --registry-password <SECURE_PASSWORD> \\r\\n```\\r\\n\\r\\n**Docker Hub**\\r\\n\\r\\n```azurecli\\r\\naz functionapp create --name <APP_NAME> --custom-location <CUSTOM_LOCATION_ID> --storage-account <STORAGE_NAME> --resource-group AzureFunctionsContainers-rg --image <DOCKER_ID>/azurefunctionsimage:v1.0.0\\r\\n```\\r\\n\\r\\nIn this example, replace `<CUSTOM_LOCATION_ID>` with the ID of the custom location you determined for the App Service Kubernetes environment. Also, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, `<APP_NAME>` with a globally unique name, and `<DOCKER_ID>` or `<LOGIN_SERVER>` with your Docker Hub account ID or Container Registry server, respectively. When you\\'re deploying from a custom container registry, the image name indicates the URL of the registry.\\r\\n\\r\\nWhen you first create the function app, it pulls the initial image from your Docker Hub.\\r\\n\\r\\n### Set required app settings\\r\\n\\r\\nRun the following commands to create an app setting for the storage account connection string:\\r\\n\\r\\n```azurecli\\r\\nstorageConnectionString=$(az storage account show-connection-string --resource-group AzureFunctionsContainers-rg --name <STORAGE_NAME> --query connectionString --output tsv)\\r\\naz functionapp config appsettings set --name <app_name> --resource-group AzureFunctionsContainers-rg --settings AzureWebJobsStorage=$storageConnectionString\\r\\n```\\r\\n\\r\\nThis code must be run either in Cloud Shell or in Bash on your local computer. Replace `<STORAGE_NAME>` with the name of the storage account and `<APP_NAME>` with the function app name.\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you want to continue working with Azure Function using the resources you created in this article, you can leave all those resources in place.\\r\\n\\r\\nWhen you are done working with this function app deployment, delete the `AzureFunctionsContainers-rg` resource group to clean up all the resources in that group:\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsContainers-rg\\r\\n```\\r\\n\\r\\nThis only removes the resources created in this article. The underlying Azure Arc environment remains in place.\\r\\n\\r\\n# Create your first containerized Azure Functions on Azure Arc (preview) (programming-language-java)\\r\\n\\r\\nIn this article, you create a function app running in a Linux container and deploy it to an [Azure Arc-enabled Kubernetes cluster](/en-us/azure/azure-arc/kubernetes/overview) from a container registry. When you create your own container, you can customize the execution environment for your function app. To learn more, see [App Service, Functions, and Logic Apps on Azure Arc](../app-service/overview-arc-integration).\\r\\n\\r\\nNote\\r\\n\\r\\nSupport for deploying a custom container to an Azure Arc-enabled Kubernetes cluster is currently in preview.\\r\\n\\r\\nYou can also publish your functions to an Azure Arc-enabled Kubernetes cluster without first creating a container. To learn more, see [Create your first function on Azure Arc (preview)](create-first-function-arc-cli)\\r\\n\\r\\n## Choose your development language\\r\\n\\r\\nFirst, you use Azure Functions tools to create your project code as a function app in a Docker container using a language-specific Linux base image. Make sure to select your language of choice at the top of the article.\\r\\n\\r\\nCore Tools automatically generates a Dockerfile for your project that uses the most up-to-date version of the correct base image for your functions language. You should regularly update your container from the latest base image and redeploy from the updated version of your container. For more information, see [Creating containerized function apps](functions-how-to-custom-container#creating-containerized-function-apps).\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nBefore you begin, you must have the following requirements in place:\\r\\n\\r\\n- Install [Azure Functions Core Tools](functions-run-local#v2) version 4.x.\\r\\n\\r\\n- Install a version of the [Java Developer Kit](/en-us/azure/developer/java/fundamentals/java-jdk-long-term-support) that is [supported by Azure Functions](functions-reference-java#supported-versions).\\r\\n- Install [Apache Maven](https://maven.apache.org) version 3.0 or above.\\r\\n\\r\\n- [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or a later version.\\r\\n\\r\\nIf you don\\'t have an [Azure subscription](../guides/developer/azure-developer-guide#understanding-accounts-subscriptions-and-billing), create an [Azure free account](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio) before you begin.\\r\\n\\r\\nTo publish the containerized function app image you create to a container registry, you need a Docker ID and [Docker](https://docs.docker.com/install/) running on your local computer. If you don\\'t have a Docker ID, you can [create a Docker account](https://hub.docker.com/signup).\\r\\n\\r\\n**Azure Container Registry**\\r\\nYou also need to complete the [Create a container registry](/en-us/azure/container-registry/container-registry-get-started-portal#create-a-container-registry) section of the Container Registry quickstart to create a registry instance. Make a note of your fully qualified login server name.\\r\\n\\r\\n**Docker Hub**\\r\\nYou should be all set.\\r\\n\\r\\n## Create and test the local functions project\\r\\n\\r\\nIn an empty folder, run the following command to generate the Functions project from a [Maven archetype](https://maven.apache.org/guides/introduction/introduction-to-archetypes.html):\\r\\n\\r\\n**Bash**\\r\\n\\r\\n```bash\\r\\nmvn archetype:generate -DarchetypeGroupId=com.microsoft.azure -DarchetypeArtifactId=azure-functions-archetype -DjavaVersion=8 -Ddocker\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n```powershell\\r\\nmvn archetype:generate \"-DarchetypeGroupId=com.microsoft.azure\" \"-DarchetypeArtifactId=azure-functions-archetype\" \"-DjavaVersion=8\" \"-Ddocker\"\\r\\n```\\r\\n\\r\\n**Cmd**\\r\\n\\r\\n```cmd\\r\\nmvn archetype:generate \"-DarchetypeGroupId=com.microsoft.azure\" \"-DarchetypeArtifactId=azure-functions-archetype\" \"-DjavaVersion=8\" \"-Ddocker\"\\r\\n```\\r\\n\\r\\nThe `-DjavaVersion` parameter tells the Functions runtime which version of Java to use. Use `-DjavaVersion=11` if you want your functions to run on Java 11. When you don\\'t specify `-DjavaVersion`, Maven defaults to Java 8. For more information, see [Java versions](functions-reference-java#java-versions).\\r\\n\\r\\nImportant\\r\\n\\r\\nThe `JAVA_HOME` environment variable must be set to the install location of the correct version of the JDK to complete this article.\\r\\n\\r\\nMaven asks you for values needed to finish generating the project on deployment. Follow the prompts and provide the following information:\\r\\n\\r\\n| Prompt | Value | Description |\\r\\n| --- | --- | --- |\\r\\n| **groupId** | `com.fabrikam` | A value that uniquely identifies your project across all projects, following the [package naming rules](https://docs.oracle.com/javase/specs/jls/se6/html/packages.html#7.7) for Java. |\\r\\n| **artifactId** | `fabrikam-functions` | A value that is the name of the jar, without a version number. |\\r\\n| **version** | `1.0-SNAPSHOT` | Select the default value. |\\r\\n| **package** | `com.fabrikam.functions` | A value that is the Java package for the generated function code. Use the default. |\\r\\n\\r\\nType `Y` or press Enter to confirm.\\r\\n\\r\\nMaven creates the project files in a new folder named *artifactId*, which in this example is `fabrikam-functions`.\\r\\n\\r\\nThe `--docker` option generates a *Dockerfile* for the project, which defines a suitable container for use with Azure Functions and the selected runtime.\\r\\n\\r\\nNavigate into the project folder:\\r\\n\\r\\n```console\\r\\ncd fabrikam-functions\\r\\n```\\r\\n\\r\\nTo test the function locally, start the local Azure Functions runtime host in the root of the project folder.\\r\\n\\r\\n```console\\r\\nmvn clean package  \\r\\nmvn azure-functions:run\\r\\n```\\r\\n\\r\\nAfter you see the `HttpExample` endpoint written to the output, navigate to `http://localhost:7071/api/HttpExample?name=Functions`. The browser must display a \"hello\" message that echoes back `Functions`, the value supplied to the `name` query parameter.\\r\\n\\r\\nPress **Ctrl**+**C** (**Command**+**C** on macOS) to stop the host.\\r\\n\\r\\n## Build the container image and verify locally\\r\\n\\r\\n(Optional) Examine the *Dockerfile* in the root of the project folder. The *Dockerfile* describes the required environment to run the function app on Linux. The complete list of supported base images for Azure Functions can be found in the [Azure Functions base image page](https://hub.docker.com/_/microsoft-azure-functions-base).\\r\\n\\r\\nIn the root project folder, run the [docker build](https://docs.docker.com/engine/reference/commandline/build/) command, provide a name as `azurefunctionsimage`, and tag as `v1.0.0`. Replace `<DOCKER_ID>` with your Docker Hub account ID. This command builds the Docker image for the container.\\r\\n\\r\\n```console\\r\\ndocker build --tag <DOCKER_ID>/azurefunctionsimage:v1.0.0 .\\r\\n```\\r\\n\\r\\nWhen the command completes, you can run the new container locally.\\r\\n\\r\\nTo verify the build, run the image in a local container using the [docker run](https://docs.docker.com/engine/reference/commandline/run/) command, replace `<DOCKER_ID>` again with your Docker Hub account ID, and add the ports argument as `-p 8080:80`:\\r\\n\\r\\n```console\\r\\ndocker run -p 8080:80 -it <DOCKER_ID>/azurefunctionsimage:v1.0.0\\r\\n```\\r\\n\\r\\nAfter the image starts in the local container, browse to `http://localhost:8080/api/HttpExample?name=Functions`, which must display the same \"hello\" message as before. Because the HTTP triggered function you created uses anonymous authorization, you can call the function running in the container without having to obtain an access key. For more information, see [authorization keys](functions-bindings-http-webhook-trigger#authorization-keys).\\r\\n\\r\\nAfter verifying the function app in the container, press **Ctrl**+**C** (**Command**+**C** on macOS) to stop execution.\\r\\n\\r\\n## Publish the container image to a registry\\r\\n\\r\\nTo make your container image available for deployment to a hosting environment, you must push it to a container registry.\\r\\n\\r\\n**Azure Container Registry**\\r\\nAzure Container Registry is a private registry service for building, storing, and managing container images and related artifacts. You should use a private registry service for publishing your containers to Azure services.\\r\\n\\r\\n1. Use this command to sign in to your registry instance using your current Azure credentials:\\r\\n\\r\\n    ```azurecli\\r\\n    az acr login --name <REGISTRY_NAME>\\r\\n    ```\\r\\n\\r\\n    In the previous command, replace `<REGISTRY_NAME>` with the name of your Container Registry instance.\\r\\n2. Use this command to tag your image with the fully qualified name of your registry login server:\\r\\n\\r\\n    ```docker\\r\\n    docker tag <DOCKER_ID>/azurefunctionsimage:v1.0.0 <LOGIN_SERVER>/azurefunctionsimage:v1.0.0 \\r\\n    ```\\r\\n\\r\\n    Replace `<LOGIN_SERVER>` with the fully qualified name of your registry login server and `<DOCKER_ID>` with your Docker ID.\\r\\n3. Use this command to push the container to your registry instance:\\r\\n\\r\\n    ```docker\\r\\n    docker push <LOGIN_SERVER>/azurefunctionsimage:v1.0.0\\r\\n    ```\\r\\n\\r\\n**Docker Hub**\\r\\nDocker Hub is a container registry that hosts images and provides image and container services.\\r\\n\\r\\n1. If you haven\\'t already signed in to Docker, do so with the [`docker login`](https://docs.docker.com/engine/reference/commandline/login/) command, replacing `<docker_id>` with your Docker Hub account ID. This command prompts you for your username and password. A \"sign in Succeeded\" message confirms that you\\'re signed in.\\r\\n\\r\\n    ```console\\r\\n    docker login\\r\\n    ```\\r\\n2. After you\\'ve signed in, push the image to Docker Hub by using the [`docker push`](https://docs.docker.com/engine/reference/commandline/push/) command, again replace the `<docker_id>` with your Docker Hub account ID.\\r\\n\\r\\n    ```console\\r\\n    docker push <DOCKER_ID>/azurefunctionsimage:v1.0.0\\r\\n    ```\\r\\n\\r\\n    Depending on your network speed, pushing the image for the first time might take a few minutes. Subsequent changes are pushed faster.\\r\\n\\r\\n## Create an App Service Kubernetes environment\\r\\n\\r\\nBefore you begin, you must [create an App Service Kubernetes environment](../app-service/manage-create-arc-environment) for an Azure Arc-enabled Kubernetes cluster.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen you create the environment, make sure to make note of both the custom location name and the name of the resource group that contains the custom location. You can use these to find the custom location ID, which you\\'ll need when creating your function app in the environment.\\r\\n\\r\\nIf you didn\\'t create the environment, check with your cluster administrator.\\r\\n\\r\\n## Add Azure CLI extensions\\r\\n\\r\\nLaunch the Bash environment in [Azure Cloud Shell](/en-us/azure/cloud-shell/get-started).\\r\\n\\r\\nBecause these CLI commands are not yet part of the core CLI set, add them with the following commands:\\r\\n\\r\\n```azurecli\\r\\naz extension add --upgrade --yes --name customlocation\\r\\naz extension remove --name appservice-kube\\r\\naz extension add --upgrade --yes --name appservice-kube\\r\\n```\\r\\n\\r\\n## Create Azure resources\\r\\n\\r\\nBefore you can deploy your container to your new App Service Kubernetes environment, you need to create two more resources:\\r\\n\\r\\n- A [Storage account](../storage/common/storage-account-create). While this article creates a storage account, in some cases a storage account may not be required. For more information, see [Azure Arc-enabled clusters](storage-considerations#azure-arc-enabled-clusters) in the storage considerations article.\\r\\n- A function app, which provides the context for running your container. The function app runs in the App Service Kubernetes environment and maps to your local function project. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nNote\\r\\n\\r\\nFunction apps run in an App Service Kubernetes environment on a Dedicated (App Service) plan. When you create your function app without an existing plan, a plan is created for you.\\r\\n\\r\\n### Create Storage account\\r\\n\\r\\nUse the [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command to create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n```azurecli\\r\\naz storage account create --name <STORAGE_NAME> --location westeurope --resource-group myResourceGroup --sku Standard_LRS\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nIn some cases, a storage account may not be required. For more information, see [Azure Arc-enabled clusters](storage-considerations#azure-arc-enabled-clusters) in the storage considerations article.\\r\\n\\r\\nIn the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements). The `--location` value is a standard Azure region.\\r\\n\\r\\n### Create the function app\\r\\n\\r\\nRun the [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command to create a new function app in the environment.\\r\\n\\r\\n**Azure Container Registry**\\r\\n\\r\\n```azurecli\\r\\naz functionapp create --name <APP_NAME> --custom-location <CUSTOM_LOCATION_ID> --storage-account <STORAGE_NAME> --resource-group AzureFunctionsContainers-rg --image <LOGIN_SERVER>/azurefunctionsimage:v1.0.0 --registry-username <USERNAME> --registry-password <SECURE_PASSWORD> \\r\\n```\\r\\n\\r\\n**Docker Hub**\\r\\n\\r\\n```azurecli\\r\\naz functionapp create --name <APP_NAME> --custom-location <CUSTOM_LOCATION_ID> --storage-account <STORAGE_NAME> --resource-group AzureFunctionsContainers-rg --image <DOCKER_ID>/azurefunctionsimage:v1.0.0\\r\\n```\\r\\n\\r\\nIn this example, replace `<CUSTOM_LOCATION_ID>` with the ID of the custom location you determined for the App Service Kubernetes environment. Also, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, `<APP_NAME>` with a globally unique name, and `<DOCKER_ID>` or `<LOGIN_SERVER>` with your Docker Hub account ID or Container Registry server, respectively. When you\\'re deploying from a custom container registry, the image name indicates the URL of the registry.\\r\\n\\r\\nWhen you first create the function app, it pulls the initial image from your Docker Hub.\\r\\n\\r\\n### Set required app settings\\r\\n\\r\\nRun the following commands to create an app setting for the storage account connection string:\\r\\n\\r\\n```azurecli\\r\\nstorageConnectionString=$(az storage account show-connection-string --resource-group AzureFunctionsContainers-rg --name <STORAGE_NAME> --query connectionString --output tsv)\\r\\naz functionapp config appsettings set --name <app_name> --resource-group AzureFunctionsContainers-rg --settings AzureWebJobsStorage=$storageConnectionString\\r\\n```\\r\\n\\r\\nThis code must be run either in Cloud Shell or in Bash on your local computer. Replace `<STORAGE_NAME>` with the name of the storage account and `<APP_NAME>` with the function app name.\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you want to continue working with Azure Function using the resources you created in this article, you can leave all those resources in place.\\r\\n\\r\\nWhen you are done working with this function app deployment, delete the `AzureFunctionsContainers-rg` resource group to clean up all the resources in that group:\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsContainers-rg\\r\\n```\\r\\n\\r\\nThis only removes the resources created in this article. The underlying Azure Arc environment remains in place.\\r\\n\\r\\n# Create your first containerized Azure Functions on Azure Arc (preview) (programming-language-javascript)\\r\\n\\r\\nIn this article, you create a function app running in a Linux container and deploy it to an [Azure Arc-enabled Kubernetes cluster](/en-us/azure/azure-arc/kubernetes/overview) from a container registry. When you create your own container, you can customize the execution environment for your function app. To learn more, see [App Service, Functions, and Logic Apps on Azure Arc](../app-service/overview-arc-integration).\\r\\n\\r\\nNote\\r\\n\\r\\nSupport for deploying a custom container to an Azure Arc-enabled Kubernetes cluster is currently in preview.\\r\\n\\r\\nYou can also publish your functions to an Azure Arc-enabled Kubernetes cluster without first creating a container. To learn more, see [Create your first function on Azure Arc (preview)](create-first-function-arc-cli)\\r\\n\\r\\n## Choose your development language\\r\\n\\r\\nFirst, you use Azure Functions tools to create your project code as a function app in a Docker container using a language-specific Linux base image. Make sure to select your language of choice at the top of the article.\\r\\n\\r\\nCore Tools automatically generates a Dockerfile for your project that uses the most up-to-date version of the correct base image for your functions language. You should regularly update your container from the latest base image and redeploy from the updated version of your container. For more information, see [Creating containerized function apps](functions-how-to-custom-container#creating-containerized-function-apps).\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nBefore you begin, you must have the following requirements in place:\\r\\n\\r\\n- Install [Azure Functions Core Tools](functions-run-local#v2) version 4.x.\\r\\n\\r\\n- Install a version of [Node.js](https://nodejs.org/) that is [supported by Azure Functions](functions-reference-node#supported-versions).\\r\\n\\r\\n- [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or a later version.\\r\\n\\r\\nIf you don\\'t have an [Azure subscription](../guides/developer/azure-developer-guide#understanding-accounts-subscriptions-and-billing), create an [Azure free account](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio) before you begin.\\r\\n\\r\\nTo publish the containerized function app image you create to a container registry, you need a Docker ID and [Docker](https://docs.docker.com/install/) running on your local computer. If you don\\'t have a Docker ID, you can [create a Docker account](https://hub.docker.com/signup).\\r\\n\\r\\n**Azure Container Registry**\\r\\nYou also need to complete the [Create a container registry](/en-us/azure/container-registry/container-registry-get-started-portal#create-a-container-registry) section of the Container Registry quickstart to create a registry instance. Make a note of your fully qualified login server name.\\r\\n\\r\\n**Docker Hub**\\r\\nYou should be all set.\\r\\n\\r\\n## Create and test the local functions project\\r\\n\\r\\nIn a terminal or command prompt, run the following command for your chosen language to create a function app project in the current folder:\\r\\n\\r\\n```console\\r\\nfunc init --worker-runtime node --language javascript --docker\\r\\n```\\r\\n\\r\\nThe `--docker` option generates a *Dockerfile* for the project, which defines a suitable container for use with Azure Functions and the selected runtime.\\r\\n\\r\\nUse the following command to add a function to your project, where the `--name` argument is the unique name of your function and the `--template` argument specifies the function\\'s trigger. `func new` creates a subfolder matching the function name that contains a configuration file named *function.json*.\\r\\n\\r\\n```console\\r\\nfunc new --name HttpExample --template \"HTTP trigger\"\\r\\n```\\r\\n\\r\\nTo test the function locally, start the local Azure Functions runtime host in the root of the project folder.\\r\\n\\r\\n```console\\r\\nfunc start  \\r\\n```\\r\\n\\r\\nAfter you see the `HttpExample` endpoint written to the output, navigate to `http://localhost:7071/api/HttpExample?name=Functions`. The browser must display a \"hello\" message that echoes back `Functions`, the value supplied to the `name` query parameter.\\r\\n\\r\\nPress **Ctrl**+**C** (**Command**+**C** on macOS) to stop the host.\\r\\n\\r\\n## Build the container image and verify locally\\r\\n\\r\\n(Optional) Examine the *Dockerfile* in the root of the project folder. The *Dockerfile* describes the required environment to run the function app on Linux. The complete list of supported base images for Azure Functions can be found in the [Azure Functions base image page](https://hub.docker.com/_/microsoft-azure-functions-base).\\r\\n\\r\\nIn the root project folder, run the [docker build](https://docs.docker.com/engine/reference/commandline/build/) command, provide a name as `azurefunctionsimage`, and tag as `v1.0.0`. Replace `<DOCKER_ID>` with your Docker Hub account ID. This command builds the Docker image for the container.\\r\\n\\r\\n```console\\r\\ndocker build --tag <DOCKER_ID>/azurefunctionsimage:v1.0.0 .\\r\\n```\\r\\n\\r\\nWhen the command completes, you can run the new container locally.\\r\\n\\r\\nTo verify the build, run the image in a local container using the [docker run](https://docs.docker.com/engine/reference/commandline/run/) command, replace `<DOCKER_ID>` again with your Docker Hub account ID, and add the ports argument as `-p 8080:80`:\\r\\n\\r\\n```console\\r\\ndocker run -p 8080:80 -it <DOCKER_ID>/azurefunctionsimage:v1.0.0\\r\\n```\\r\\n\\r\\nAfter the image starts in the local container, browse to `http://localhost:8080/api/HttpExample?name=Functions`, which must display the same \"hello\" message as',\n",
       "  'metadata': {}},\n",
       " {'_id': 'a3ed70a7-c40e-b750-39ab-95f9eaf33715',\n",
       "  'title': 'Create functions in Azure using the Azure Developer CLI',\n",
       "  'text': '# Quickstart: Create and deploy functions to Azure Functions using the Azure Developer CLI (programming-language-csharp)\\r\\n\\r\\nIn this Quickstart, you use Azure Developer command-line tools to create functions that respond to HTTP requests. After testing the code locally, you deploy it to a new serverless function app you create running in a Flex Consumption plan in Azure Functions.\\r\\n\\r\\nThe project source uses the Azure Developer CLI (azd) to simplify deploying your code to Azure. This deployment follows current best practices for secure and scalable Azure Functions deployments.\\r\\n\\r\\nBy default, the Flex Consumption plan follows a *pay-for-what-you-use* billing model, which means to complete this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [Azure Developer CLI](/en-us/azure/developer/azure-developer-cli/install-azd).\\r\\n- [Azure Functions Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n- [.NET 8.0 SDK](https://dotnet.microsoft.com/download)\\r\\n- [Azurite storage emulator](../storage/common/storage-use-azurite?tabs=npm#install-azurite)\\r\\n\\r\\n- A [secure HTTP test tool](functions-develop-local#http-test-tools) for sending requests with JSON payloads to your function endpoints. This article uses `curl`.\\r\\n\\r\\n## Initialize the project\\r\\n\\r\\nYou can use the `azd init` command to create a local Azure Functions code project from a template.\\r\\n\\r\\n1. In your local terminal or command prompt, run this `azd init` command in an empty folder:\\r\\n\\r\\n    ```console\\r\\n    azd init --template functions-quickstart-dotnet-azd -e flexquickstart-dotnet\\r\\n    ```\\r\\n\\r\\n    This command pulls the project files from the [template repository](https://github.com/Azure-Samples/functions-quickstart-dotnet-azd) and initializes the project in the current folder. The `-e` flag sets a name for the current environment. In `azd`, the environment is used to maintain a unique deployment context for your app, and you can define more than one. It\\'s also used in the name of the resource group you create in Azure.\\r\\n2. Run this command to navigate to the `http` app folder:\\r\\n\\r\\n    ```console\\r\\n    cd http\\r\\n    ```\\r\\n3. Create a file named *local.settings.json* in the `http` folder that contains this JSON data:\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n        \"IsEncrypted\": false,\\r\\n        \"Values\": {\\r\\n            \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\\r\\n            \"FUNCTIONS_WORKER_RUNTIME\": \"dotnet-isolated\"\\r\\n        }\\r\\n    }\\r\\n    ```\\r\\n\\r\\n    This file is required when running locally.\\r\\n\\r\\n## Run in your local environment\\r\\n\\r\\n1. Run this command from your app folder in a terminal or command prompt:\\r\\n\\r\\n    ```console\\r\\n    func start\\r\\n    ```\\r\\n\\r\\n    When the Functions host starts in your local project folder, it writes the URL endpoints of your HTTP triggered functions to the terminal output.\\r\\n2. In your browser, navigate to the `httpget` endpoint, which should look like this URL:\\r\\n\\r\\n    http://localhost:7071/api/httpget\\r\\n3. From a new terminal or command prompt window, run this `curl` command to send a POST request with a JSON payload to the `httppost` endpoint:\\r\\n\\r\\n    ```console\\r\\n    curl -i http://localhost:7071/api/httppost -H \"Content-Type: text/json\" -d @testdata.json\\r\\n    ```\\r\\n\\r\\n    This command reads JSON payload data from the `testdata.json` project file. You can find examples of both HTTP requests in the `test.http` project file.\\r\\n4. When you\\'re done, press Ctrl+C in the terminal window to stop the `func.exe` host process.\\r\\n\\r\\n## Review the code (optional)\\r\\n\\r\\nYou can review the code that defines the two HTTP trigger function endpoints:\\r\\n\\r\\n**`httpget`**\\r\\n\\r\\n```csharp\\r\\n       [Function(\"httpget\")]\\r\\n       public IActionResult Run([HttpTrigger(AuthorizationLevel.Function, \"get\")]\\r\\n         HttpRequest req,\\r\\n         string name)\\r\\n       {\\r\\n           var returnValue = string.IsNullOrEmpty(name)\\r\\n               ? \"Hello, World.\"\\r\\n               : $\"Hello, {name}.\";\\r\\n\\r\\n           _logger.LogInformation($\"C# HTTP trigger function processed a request for {returnValue}.\");\\r\\n\\r\\n           return new OkObjectResult(returnValue);\\r\\n       }\\r\\n```\\r\\n\\r\\n**`httppost`**\\r\\n\\r\\n```csharp\\r\\n[Function(\"httppost\")]\\r\\npublic IActionResult Run([HttpTrigger(AuthorizationLevel.Function, \"post\")] HttpRequest req,\\r\\n    [FromBody] Person person)\\r\\n{\\r\\n    _logger.LogInformation($\"C# HTTP POST trigger function processed a request for url {req.Body}\");\\r\\n\\r\\n    if (string.IsNullOrEmpty(person.Name) | string.IsNullOrEmpty(person.Age.ToString()) | person.Age == 0)\\r\\n    {\\r\\n        _logger.LogInformation(\"C# HTTP POST trigger function processed a request with no name/age provided.\");\\r\\n        return new BadRequestObjectResult(\"Please provide both name and age in the request body.\");\\r\\n    }\\r\\n\\r\\n    var returnValue = $\"Hello, {person.Name}! You are {person.Age} years old.\";\\r\\n    \\r\\n    _logger.LogInformation($\"C# HTTP POST trigger function processed a request for {person.Name} who is {person.Age} years old.\");\\r\\n    return new OkObjectResult(returnValue);\\r\\n}\\r\\n```\\r\\n\\r\\nYou can review the complete template project [here](https://github.com/Azure-Samples/functions-quickstart-dotnet-azd).\\r\\n\\r\\nAfter you verify your functions locally, it\\'s time to publish them to Azure.\\r\\n\\r\\n## Deploy to Azure\\r\\n\\r\\nThis project is configured to use the `azd up` command to deploy this project to a new function app in a Flex Consumption plan in Azure.\\r\\n\\r\\nTip\\r\\n\\r\\nThis project includes a set of Bicep files that `azd` uses to create a secure deployment to a Flex consumption plan that follows best practices.\\r\\n\\r\\n1. Run this command to have `azd` create the required Azure resources in Azure and deploy your code project to the new function app:\\r\\n\\r\\n    ```console\\r\\n    azd up\\r\\n    ```\\r\\n\\r\\n    The root folder contains the `azure.yaml` definition file required by `azd`.\\r\\n\\r\\n    If you aren\\'t already signed-in, you\\'re asked to authenticate with your Azure account.\\r\\n2. When prompted, provide these required deployment parameters:\\r\\n\\r\\n    | Parameter | Description |\\r\\n    | --- | --- |\\r\\n    | *Azure subscription* | Subscription in which your resources are created. |\\r\\n    | *Azure location* | Azure region in which to create the resource group that contains the new Azure resources. Only regions that currently support the Flex Consumption plan are shown. |\\r\\n\\r\\n    The `azd up` command uses your response to these prompts with the Bicep configuration files to complete these deployment tasks:\\r\\n\\r\\n    - Create and configure these required Azure resources (equivalent to `azd provision`):\\r\\n\\r\\n        - Flex Consumption plan and function app\\r\\n        - Azure Storage (required) and Application Insights (recommended)\\r\\n        - Access policies and roles for your account\\r\\n        - Service-to-service connections using managed identities (instead of stored connection strings)\\r\\n        - Virtual network to securely run both the function app and the other Azure resources\\r\\n    - Package and deploy your code to the deployment container (equivalent to `azd deploy`). The app is then started and runs in the deployed package.\\r\\n\\r\\n    After the command completes successfully, you see links to the resources you created.\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nYou can now invoke your function endpoints in Azure by making HTTP requests to their URLs using your HTTP test tool or from the browser (for GET requests). When your functions run in Azure, access key authorization is enforced, and you must provide a function access key with your request.\\r\\n\\r\\nYou can use the Core Tools to obtain the URL endpoints of your functions running in Azure.\\r\\n\\r\\n1. In your local terminal or command prompt, run these commands to get the URL endpoint values:\\r\\n\\r\\n**bash**\\r\\n\\r\\n    ```bash\\r\\n    SET APP_NAME=(azd env get-value AZURE_FUNCTION_NAME)\\r\\n    func azure functionapp list-functions $APP_NAME --show-keys\\r\\n    ```\\r\\n\\r\\n**Cmd**\\r\\n\\r\\n    ```cmd\\r\\n    for /f \"tokens=*\" %i in (\\'azd env get-value AZURE_FUNCTION_NAME\\') do set APP_NAME=%i\\r\\n    func azure functionapp list-functions %APP_NAME% --show-keys \\r\\n    ```\\r\\n\\r\\n    The `azd env get-value` command gets your function app name from the local environment. Using the `--show-keys` option with `func azure functionapp list-functions` means that the returned **Invoke URL:** value for each endpoint includes a function-level access key.\\r\\n2. As before, use your HTTP test tool to validate these URLs in your function app running in Azure.\\r\\n\\r\\n## Redeploy your code\\r\\n\\r\\nYou can run the `azd up` command as many times as you need to both provision your Azure resources and deploy code updates to your function app.\\r\\n\\r\\nNote\\r\\n\\r\\nDeployed code files are always overwritten by the latest deployment package.\\r\\n\\r\\nYour initial responses to `azd` prompts and any environment variables generated by `azd` are stored locally in your named environment. Use the `azd env get-values` command to review all of the variables in your environment that were used when creating Azure resources.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you\\'re done working with your function app and related resources, you can use this command to delete the function app and its related resources from Azure and avoid incurring any further costs:\\r\\n\\r\\n```console\\r\\nazd down --no-prompt\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe `--no-prompt` option instructs `azd` to delete your resource group without a confirmation from you.\\r\\n\\r\\nThis command doesn\\'t affect your local code project.\\r\\n\\r\\n# Quickstart: Create and deploy functions to Azure Functions using the Azure Developer CLI (programming-language-java)\\r\\n\\r\\nIn this Quickstart, you use Azure Developer command-line tools to create functions that respond to HTTP requests. After testing the code locally, you deploy it to a new serverless function app you create running in a Flex Consumption plan in Azure Functions.\\r\\n\\r\\nThe project source uses the Azure Developer CLI (azd) to simplify deploying your code to Azure. This deployment follows current best practices for secure and scalable Azure Functions deployments.\\r\\n\\r\\nBy default, the Flex Consumption plan follows a *pay-for-what-you-use* billing model, which means to complete this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [Azure Developer CLI](/en-us/azure/developer/azure-developer-cli/install-azd).\\r\\n- [Azure Functions Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n- [Java 17 Developer Kit](/en-us/azure/developer/java/fundamentals/java-support-on-azure)\\r\\n    - If you use another [supported version of Java](supported-languages?pivots=programming-language-java#languages-by-runtime-version), you must update the project\\'s pom.xml file.\\r\\n    - The `JAVA_HOME` environment variable must be set to the install location of the correct version of the JDK.\\r\\n- [Apache Maven 3.8.x](https://maven.apache.org)\\r\\n\\r\\n- A [secure HTTP test tool](functions-develop-local#http-test-tools) for sending requests with JSON payloads to your function endpoints. This article uses `curl`.\\r\\n\\r\\n## Initialize the project\\r\\n\\r\\nYou can use the `azd init` command to create a local Azure Functions code project from a template.\\r\\n\\r\\n1. In your local terminal or command prompt, run this `azd init` command in an empty folder:\\r\\n\\r\\n    ```console\\r\\n    azd init --template azure-functions-java-flex-consumption-azd -e flexquickstart-java \\r\\n    ```\\r\\n\\r\\n    This command pulls the project files from the [template repository](https://github.com/Azure-Samples/azure-functions-java-flex-consumption-azd) and initializes the project in the current folder. The `-e` flag sets a name for the current environment. In `azd`, the environment is used to maintain a unique deployment context for your app, and you can define more than one. It\\'s also used in the name of the resource group you create in Azure.\\r\\n2. Run this command to navigate to the `http` app folder:\\r\\n\\r\\n    ```console\\r\\n    cd http\\r\\n    ```\\r\\n3. Create a file named *local.settings.json* in the `http` folder that contains this JSON data:\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n        \"IsEncrypted\": false,\\r\\n        \"Values\": {\\r\\n            \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\\r\\n            \"FUNCTIONS_WORKER_RUNTIME\": \"java\"\\r\\n        }\\r\\n    }\\r\\n    ```\\r\\n\\r\\n    This file is required when running locally.\\r\\n\\r\\n## Run in your local environment\\r\\n\\r\\n1. Run this command from your app folder in a terminal or command prompt:\\r\\n\\r\\n    ```console\\r\\n    mvn clean package\\r\\n    mvn azure-functions:run\\r\\n    ```\\r\\n\\r\\n    When the Functions host starts in your local project folder, it writes the URL endpoints of your HTTP triggered functions to the terminal output.\\r\\n2. In your browser, navigate to the `httpget` endpoint, which should look like this URL:\\r\\n\\r\\n    http://localhost:7071/api/httpget\\r\\n3. From a new terminal or command prompt window, run this `curl` command to send a POST request with a JSON payload to the `httppost` endpoint:\\r\\n\\r\\n    This command reads JSON payload data from the `testdata.json` project file. You can find examples of both HTTP requests in the `test.http` project file.\\r\\n4. When you\\'re done, press Ctrl+C in the terminal window to stop the `func.exe` host process.\\r\\n\\r\\n## Review the code (optional)\\r\\n\\r\\nYou can review the code that defines the two HTTP trigger function endpoints:\\r\\n\\r\\n**`httpget`**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"httpget\")\\r\\npublic HttpResponseMessage run(\\r\\n        @HttpTrigger(\\r\\n            name = \"req\",\\r\\n            methods = {HttpMethod.GET},\\r\\n            authLevel = AuthorizationLevel.FUNCTION)\\r\\n            HttpRequestMessage<Optional<String>> request,\\r\\n        final ExecutionContext context) {\\r\\n    context.getLogger().info(\"Java HTTP trigger processed a request.\");\\r\\n\\r\\n    // Parse query parameter\\r\\n    String name = Optional.ofNullable(request.getQueryParameters().get(\"name\")).orElse(\"World\");\\r\\n\\r\\n    return request.createResponseBuilder(HttpStatus.OK).body(\"Hello, \" + name).build();\\r\\n}\\r\\n```\\r\\n\\r\\n**`httppost`**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"httppost\")\\r\\npublic HttpResponseMessage runPost(\\r\\n        @HttpTrigger(\\r\\n            name = \"req\",\\r\\n            methods = {HttpMethod.POST},\\r\\n            authLevel = AuthorizationLevel.FUNCTION)\\r\\n            HttpRequestMessage<Optional<String>> request,\\r\\n        final ExecutionContext context) {\\r\\n    context.getLogger().info(\"Java HTTP trigger processed a POST request.\");\\r\\n\\r\\n    // Parse request body\\r\\n    String name;\\r\\n    Integer age;\\r\\n    try {\\r\\n        ObjectMapper mapper = new ObjectMapper();\\r\\n        JsonNode jsonNode = mapper.readTree(request.getBody().orElse(\"{}\"));\\r\\n        name = Optional.ofNullable(jsonNode.get(\"name\")).map(JsonNode::asText).orElse(null);\\r\\n        age = Optional.ofNullable(jsonNode.get(\"age\")).map(JsonNode::asInt).orElse(null);\\r\\n        if (name == null || age == null) {\\r\\n            return request.createResponseBuilder(HttpStatus.BAD_REQUEST)\\r\\n                    .body(\"Please provide both name and age in the request body.\").build();\\r\\n        }\\r\\n    } catch (Exception e) {\\r\\n        context.getLogger().severe(\"Error parsing request body: \" + e.getMessage());\\r\\n        return request.createResponseBuilder(HttpStatus.BAD_REQUEST)\\r\\n                .body(\"Error parsing request body\").build();\\r\\n    }\\r\\n```\\r\\n\\r\\nYou can review the complete template project [here](https://github.com/Azure-Samples/azure-functions-java-flex-consumption-azd).\\r\\n\\r\\nAfter you verify your functions locally, it\\'s time to publish them to Azure.\\r\\n\\r\\n## Create Azure resources\\r\\n\\r\\nThis project is configured to use the `azd provision` command to create a function app in a Flex Consumption plan, along with other required Azure resources.\\r\\n\\r\\nNote\\r\\n\\r\\nThis project includes a set of Bicep files that `azd` uses to create a secure deployment to a Flex consumption plan that follows best practices.\\r\\n\\r\\nThe `azd up` and `azd deploy` commands aren\\'t currently supported for Java apps.\\r\\n\\r\\n1. In the root folder of the project, run this command to create the required Azure resources:\\r\\n\\r\\n    ```console\\r\\n    azd provision\\r\\n    ```\\r\\n\\r\\n    The root folder contains the `azure.yaml` definition file required by `azd`.\\r\\n\\r\\n    If you aren\\'t already signed-in, you\\'re asked to authenticate with your Azure account.\\r\\n2. When prompted, provide these required deployment parameters:\\r\\n\\r\\n    | Parameter | Description |\\r\\n    | --- | --- |\\r\\n    | *Azure subscription* | Subscription in which your resources are created. |\\r\\n    | *Azure location* | Azure region in which to create the resource group that contains the new Azure resources. Only regions that currently support the Flex Consumption plan are shown. |\\r\\n\\r\\n    The `azd provision` command uses your response to these prompts with the Bicep configuration files to create and configure these required Azure resources:\\r\\n\\r\\n    - Flex Consumption plan and function app\\r\\n    - Azure Storage (required) and Application Insights (recommended)\\r\\n    - Access policies and roles for your account\\r\\n    - Service-to-service connections using managed identities (instead of stored connection strings)\\r\\n    - Virtual network to securely run both the function app and the other Azure resources\\r\\n\\r\\n    After the command completes successfully, you can deploy your project code to this new function app in Azure.\\r\\n\\r\\n## Deploy to Azure\\r\\n\\r\\nYou can use Core Tools to package your code and deploy it to Azure from the `target` output folder.\\r\\n\\r\\n1. Navigate to the app folder equivalent in the `target` output folder:\\r\\n\\r\\n    ```console\\r\\n    cd http/target/azure-functions/contoso-functions\\r\\n    ```\\r\\n\\r\\n    This folder should have a host.json file, which indicates that it\\'s the root of your compiled Java function app.\\r\\n2. Run these commands to deploy your compiled Java code project to the new function app resource in Azure using Core Tools:\\r\\n\\r\\n**bash**\\r\\n\\r\\n    ```bash\\r\\n    APP_NAME=$(azd env get-value AZURE_FUNCTION_NAME)\\r\\n    func azure functionapp publish $APP_NAME\\r\\n    ```\\r\\n\\r\\n**Cmd**\\r\\n\\r\\n    ```cmd\\r\\n    for /f \"tokens=*\" %i in (\\'azd env get-value AZURE_FUNCTION_NAME\\') do set APP_NAME=%i\\r\\n    func azure functionapp publish %APP_NAME% \\r\\n    ```\\r\\n\\r\\n    The `azd env get-value` command gets your function app name from the local environment, which is required for deployment using `func azure functionapp publish`. After publishing completes successfully, you see links to the HTTP trigger endpoints in Azure.\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nYou can now invoke your function endpoints in Azure by making HTTP requests to their URLs using your HTTP test tool or from the browser (for GET requests). When your functions run in Azure, access key authorization is enforced, and you must provide a function access key with your request.\\r\\n\\r\\nYou can use the Core Tools to obtain the URL endpoints of your functions running in Azure.\\r\\n\\r\\n1. In your local terminal or command prompt, run these commands to get the URL endpoint values:\\r\\n\\r\\n**bash**\\r\\n\\r\\n    ```bash\\r\\n    SET APP_NAME=(azd env get-value AZURE_FUNCTION_NAME)\\r\\n    func azure functionapp list-functions $APP_NAME --show-keys\\r\\n    ```\\r\\n\\r\\n**Cmd**\\r\\n\\r\\n    ```cmd\\r\\n    for /f \"tokens=*\" %i in (\\'azd env get-value AZURE_FUNCTION_NAME\\') do set APP_NAME=%i\\r\\n    func azure functionapp list-functions %APP_NAME% --show-keys \\r\\n    ```\\r\\n\\r\\n    The `azd env get-value` command gets your function app name from the local environment. Using the `--show-keys` option with `func azure functionapp list-functions` means that the returned **Invoke URL:** value for each endpoint includes a function-level access key.\\r\\n2. As before, use your HTTP test tool to validate these URLs in your function app running in Azure.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you\\'re done working with your function app and related resources, you can use this command to delete the function app and its related resources from Azure and avoid incurring any further costs:\\r\\n\\r\\n```console\\r\\nazd down --no-prompt\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe `--no-prompt` option instructs `azd` to delete your resource group without a confirmation from you.\\r\\n\\r\\nThis command doesn\\'t affect your local code project.\\r\\n\\r\\n# Quickstart: Create and deploy functions to Azure Functions using the Azure Developer CLI (programming-language-javascript)\\r\\n\\r\\nIn this Quickstart, you use Azure Developer command-line tools to create functions that respond to HTTP requests. After testing the code locally, you deploy it to a new serverless function app you create running in a Flex Consumption plan in Azure Functions.\\r\\n\\r\\nThe project source uses the Azure Developer CLI (azd) to simplify deploying your code to Azure. This deployment follows current best practices for secure and scalable Azure Functions deployments.\\r\\n\\r\\nBy default, the Flex Consumption plan follows a *pay-for-what-you-use* billing model, which means to complete this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [Azure Developer CLI](/en-us/azure/developer/azure-developer-cli/install-azd).\\r\\n- [Azure Functions Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n- [Node.js 20](https://nodejs.org/)\\r\\n\\r\\n- A [secure HTTP test tool](functions-develop-local#http-test-tools) for sending requests with JSON payloads to your function endpoints. This article uses `curl`.\\r\\n\\r\\n## Initialize the project\\r\\n\\r\\nYou can use the `azd init` command to create a local Azure Functions code project from a template.\\r\\n\\r\\n1. In your local terminal or command prompt, run this `azd init` command in an empty folder:\\r\\n\\r\\n    ```console\\r\\n    azd init --template functions-quickstart-javascript-azd -e flexquickstart-js\\r\\n    ```\\r\\n\\r\\n    This command pulls the project files from the [template repository](https://github.com/Azure-Samples/functions-quickstart-javascript-azd) and initializes the project in the root folder. The `-e` flag sets a name for the current environment. In `azd`, the environment is used to maintain a unique deployment context for your app, and you can define more than one. It\\'s also used in the name of the resource group you create in Azure.\\r\\n2. Create a file named *local.settings.json* in the root folder that contains this JSON data:\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n        \"IsEncrypted\": false,\\r\\n        \"Values\": {\\r\\n            \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\\r\\n            \"FUNCTIONS_WORKER_RUNTIME\": \"node\"\\r\\n        }\\r\\n    }\\r\\n    ```\\r\\n\\r\\n    This file is required when running locally.\\r\\n\\r\\n## Run in your local environment\\r\\n\\r\\n1. Run this command from your app folder in a terminal or command prompt:\\r\\n\\r\\n    ```console\\r\\n    npm install\\r\\n    func start  \\r\\n    ```\\r\\n\\r\\n    When the Functions host starts in your local project folder, it writes the URL endpoints of your HTTP triggered functions to the terminal output.\\r\\n2. In your browser, navigate to the `httpget` endpoint, which should look like this URL:\\r\\n\\r\\n    http://localhost:7071/api/httpget\\r\\n3. From a new terminal or command prompt window, run this `curl` command to send a POST request with a JSON payload to the `httppost` endpoint:\\r\\n\\r\\n    ```console\\r\\n    curl -i http://localhost:7071/api/httppost -H \"Content-Type: text/json\" -d \"@src/functions/testdata.json\"\\r\\n    ```\\r\\n\\r\\n    This command reads JSON payload data from the `testdata.json` project file. You can find examples of both HTTP requests in the `test.http` project file.\\r\\n4. When you\\'re done, press Ctrl+C in the terminal window to stop the `func.exe` host process.\\r\\n\\r\\n## Review the code (optional)\\r\\n\\r\\nYou can review the code that defines the two HTTP trigger function endpoints:\\r\\n\\r\\n**`httpget`**\\r\\n\\r\\n```javascript\\r\\nconst { app } = require(\\'@azure/functions\\');\\r\\n\\r\\napp.http(\\'httpget\\', {\\r\\n    methods: [\\'GET\\'],\\r\\n    authLevel: \\'function\\',\\r\\n    handler: async (request, context) => {\\r\\n        context.log(`Http function processed request for url \"${request.url}\"`);\\r\\n\\r\\n        const name = request.query.get(\\'name\\') || await request.text() || \\'world\\';\\r\\n\\r\\n        return { body: `Hello, ${name}!` };\\r\\n    }\\r\\n});\\r\\n```\\r\\n\\r\\n**`httppost`**\\r\\n\\r\\n```javascript\\r\\nconst { app } = require(\\'@azure/functions\\');\\r\\n\\r\\napp.http(\\'httppost\\', {\\r\\n    methods: [\\'POST\\'],\\r\\n    authLevel: \\'function\\',\\r\\n    handler: async (request, context) => {\\r\\n        context.log(`Http function processed request for url \"${request.url}\"`);\\r\\n\\r\\n        try {\\r\\n            const person = await request.json();\\r\\n            const { name, age } = person;\\r\\n\\r\\n            if (!name || !age) {\\r\\n                return {\\r\\n                    status: 400,\\r\\n                    body: \\'Please provide both name and age in the request body.\\'\\r\\n                };\\r\\n            }\\r\\n\\r\\n            return {\\r\\n                status: 200,\\r\\n                body: `Hello, ${name}! You are ${age} years old.`\\r\\n            };\\r\\n        } catch (error) {\\r\\n            return {\\r\\n                status: 400,\\r\\n                body: \\'Invalid request body. Please provide a valid JSON object with name and age.\\'\\r\\n            };\\r\\n        }\\r\\n    }\\r\\n});\\r\\n```\\r\\n\\r\\nYou can review the complete template project [here](https://github.com/Azure-Samples/functions-quickstart-javascript-azd).\\r\\n\\r\\nAfter you verify your functions locally, it\\'s time to publish them to Azure.\\r\\n\\r\\n## Deploy to Azure\\r\\n\\r\\nThis project is configured to use the `azd up` command to deploy this project to a new function app in a Flex Consumption plan in Azure.\\r\\n\\r\\nTip\\r\\n\\r\\nThis project includes a set of Bicep files that `azd` uses to create a secure deployment to a Flex consumption plan that follows best practices.\\r\\n\\r\\n1. Run this command to have `azd` create the required Azure resources in Azure and deploy your code project to the new function app:\\r\\n\\r\\n    ```console\\r\\n    azd up\\r\\n    ```\\r\\n\\r\\n    The root folder contains the `azure.yaml` definition file required by `azd`.\\r\\n\\r\\n    If you aren\\'t already signed-in, you\\'re asked to authenticate with your Azure account.\\r\\n2. When prompted, provide these required deployment parameters:\\r\\n\\r\\n    | Parameter | Description |\\r\\n    | --- | --- |\\r\\n    | *Azure subscription* | Subscription in which your resources are created. |\\r\\n    | *Azure location* | Azure region in which to create the resource group that contains the new Azure resources. Only regions that currently support the Flex Consumption plan are shown. |\\r\\n\\r\\n    The `azd up` command uses your response to these prompts with the Bicep configuration files to complete these deployment tasks:\\r\\n\\r\\n    - Create and configure these required Azure resources (equivalent to `azd provision`):\\r\\n\\r\\n        - Flex Consumption plan and function app\\r\\n        - Azure Storage (required) and Application Insights (recommended)\\r\\n        - Access policies and roles for your account\\r\\n        - Service-to-service connections using managed identities (instead of stored connection strings)\\r\\n        - Virtual network to securely run both the function app and the other Azure resources\\r\\n    - Package and deploy your code to the deployment container (equivalent to `azd deploy`). The app is then started and runs in the deployed package.\\r\\n\\r\\n    After the command completes successfully, you see links to the resources you created.\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nYou can now invoke your function endpoints in Azure by making HTTP requests to their URLs using your HTTP test tool or from the browser (for GET requests). When your functions run in Azure, access key authorization is enforced, and you must provide a function access key with your request.\\r\\n\\r\\nYou can use the Core Tools to obtain the URL endpoints of your functions running in Azure.\\r\\n\\r\\n1. In your local terminal or command prompt, run these commands to get the URL endpoint values:\\r\\n\\r\\n**bash**\\r\\n\\r\\n    ```bash\\r\\n    SET APP_NAME=(azd env get-value AZURE_FUNCTION_NAME)\\r\\n    func azure functionapp list-functions $APP_NAME --show-keys\\r\\n    ```\\r\\n\\r\\n**Cmd**\\r\\n\\r\\n    ```cmd\\r\\n    for /f \"tokens=*\" %i in (\\'azd env get-value AZURE_FUNCTION_NAME\\') do set APP_NAME=%i\\r\\n    func azure functionapp list-functions %APP_NAME% --show-keys \\r\\n    ```\\r\\n\\r\\n    The `azd env get-value` command gets your function app name from the local environment. Using the `--show-keys` option with `func azure functionapp list-functions` means that the returned **Invoke URL:** value for each endpoint includes a function-level access key.\\r\\n2. As before, use your HTTP test tool to validate these URLs in your function app running in Azure.\\r\\n\\r\\n## Redeploy your code\\r\\n\\r\\nYou can run the `azd up` command as many times as you need to both provision your Azure resources and deploy code updates to your function app.\\r\\n\\r\\nNote\\r\\n\\r\\nDeployed code files are always overwritten by the latest deployment package.\\r\\n\\r\\nYour initial responses to `azd` prompts and any environment variables generated by `azd` are stored locally in your named environment. Use the `azd env get-values` command to review all of the variables in your environment that were used when creating Azure resources.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you\\'re done working with your function app and related resources, you can use this command to delete the function app and its related resources from Azure and avoid incurring any further costs:\\r\\n\\r\\n```console\\r\\nazd down --no-prompt\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe `--no-prompt` option instructs `azd` to delete your resource group without a confirmation from you.\\r\\n\\r\\nThis command doesn\\'t affect your local code project.\\r\\n\\r\\n# Quickstart: Create and deploy functions to Azure Functions using the Azure Developer CLI (programming-language-typescript)\\r\\n\\r\\nIn this Quickstart, you use Azure Developer command-line tools to create functions that respond to HTTP requests. After testing the code locally, you deploy it to a new serverless function app you create running in a Flex Consumption plan in Azure Functions.\\r\\n\\r\\nThe project source uses the Azure Developer CLI (azd) to simplify deploying your code to Azure. This deployment follows current best practices for secure and scalable Azure Functions deployments.\\r\\n\\r\\nBy default, the Flex Consumption plan follows a *pay-for-what-you-use* billing model, which means to complete this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [Azure Developer CLI](/en-us/azure/developer/azure-developer-cli/install-azd).\\r\\n- [Azure Functions Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n- [Node.js 20](https://nodejs.org/)\\r\\n\\r\\n- A [secure HTTP test tool](functions-develop-local#http-test-tools) for sending requests with JSON payloads to your function endpoints. This article uses `curl`.\\r\\n\\r\\n## Initialize the project\\r\\n\\r\\nYou can use the `azd init` command to create a local Azure Functions code project from a template.\\r\\n\\r\\n1. In your local terminal or command prompt, run this `azd init` command in an empty folder:\\r\\n\\r\\n    ```console\\r\\n    azd init --template functions-quickstart-typescript-azd -e flexquickstart-ts\\r\\n    ```\\r\\n\\r\\n    This command pulls the project files from the [template repository](https://github.com/Azure-Samples/functions-quickstart-typescript-azd) and initializes the project in the root folder. The `-e` flag sets a name for the current environment. In `azd`, the environment is used to maintain a unique deployment context for your app, and you can define more than one. It\\'s also used in the name of the resource group you create in Azure.\\r\\n2. Create a file named *local.settings.json* in the root folder that contains this JSON data:\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n        \"IsEncrypted\": false,\\r\\n        \"Values\": {\\r\\n            \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\\r\\n            \"FUNCTIONS_WORKER_RUNTIME\": \"node\"\\r\\n        }\\r\\n    }\\r\\n    ```\\r\\n\\r\\n    This file is required when running locally.\\r\\n\\r\\n## Run in your local environment\\r\\n\\r\\n1. Run this command from your app folder in a terminal or command prompt:\\r\\n\\r\\n    ```console\\r\\n    npm install\\r\\n    npm start  \\r\\n    ```\\r\\n\\r\\n    When the Functions host starts in your local project folder, it writes the URL endpoints of your HTTP triggered functions to the terminal output.\\r\\n2. In your browser, navigate to the `httpget` endpoint, which should look like this URL:\\r\\n\\r\\n    http://localhost:7071/api/httpget\\r\\n3. From a new terminal or command prompt window, run this `curl` command to send a POST request with a JSON payload to the `httppost` endpoint:\\r\\n\\r\\n    ```console\\r\\n    curl -i http://localhost:7071/api/httppost -H \"Content-Type: text/json\" -d \"@src/functions/testdata.json\"\\r\\n    ```\\r\\n\\r\\n    This command reads JSON payload data from the `testdata.json` project file. You can find examples of both HTTP requests in the `test.http` project file.\\r\\n4. When you\\'re done, press Ctrl+C in the terminal window to stop the `func.exe` host process.\\r\\n\\r\\n## Review the code (optional)\\r\\n\\r\\nYou can review the code that defines the two HTTP trigger function endpoints:\\r\\n\\r\\n**`httpget`**\\r\\n\\r\\n```typescript\\r\\nimport { app, HttpRequest, HttpResponseInit, InvocationContext } from \"@azure/functions\";\\r\\n\\r\\nexport async function httpGetFunction(request: HttpRequest, context: InvocationContext): Promise<HttpResponseInit> {\\r\\n    context.log(`Http function processed request for url \"${request.url}\"`);\\r\\n\\r\\n    const name = request.query.get(\\'name\\') || await request.text() || \\'world\\';\\r\\n\\r\\n    return { body: `Hello, ${name}!` };\\r\\n};\\r\\n\\r\\napp.http(\\'httpget\\', {\\r\\n    methods: [\\'GET\\'],\\r\\n    authLevel: \\'function\\',\\r\\n    handler: httpGetFunction\\r\\n});\\r\\n```\\r\\n\\r\\n**`httppost`**\\r\\n\\r\\n```typescript\\r\\nimport { app, HttpRequest, HttpResponseInit, InvocationContext } from \"@azure/functions\";\\r\\n\\r\\ninterface Person {\\r\\n    name: string;\\r\\n    age: number;\\r\\n}\\r\\n\\r\\nfunction isPerson(obj: any): obj is Person {\\r\\n    return typeof obj === \\'object\\' && obj !== null && \\r\\n           typeof obj.name === \\'string\\' && \\r\\n           typeof obj.age === \\'number\\';\\r\\n}\\r\\n\\r\\nexport async function httpPostBodyFunction(request: HttpRequest, context: InvocationContext): Promise<HttpResponseInit> {\\r\\n    context.log(`Http function processed request for url \"${request.url}\"`);\\r\\n\\r\\n        try {\\r\\n            const data: any  = await request.json();\\r\\n    \\r\\n            if (!isPerson(data)) {\\r\\n                return {\\r\\n                    status: 400,\\r\\n                    body: \\'Please provide both name and age in the request body.\\'\\r\\n                };\\r\\n            }\\r\\n\\r\\n            return {\\r\\n                status: 200,\\r\\n                body: `Hello, ${data.name}! You are ${data.age} years old.`\\r\\n            };\\r\\n        } catch (error) {\\r\\n            return {\\r\\n                status: 400,\\r\\n                body: \\'Invalid request body. Please provide a valid JSON object with name and age.\\'\\r\\n            };\\r\\n       ',\n",
       "  'metadata': {}},\n",
       " {'_id': 'fa74d715-575a-e7bb-0f77-5dda3a567767',\n",
       "  'title': 'Create a C# function from the command line - Azure Functions',\n",
       "  'text': '# Quickstart: Create a C# function in Azure from the command line\\r\\n\\r\\nIn this article, you use command-line tools to create a C# function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nThis article creates an HTTP triggered function that runs on .NET 8 in an isolated worker process. For information about .NET versions supported for C# functions, see [Supported versions](dotnet-isolated-process-guide#supported-versions). There\\'s also a [Visual Studio Code-based version](create-first-function-vs-code-csharp) of this article.\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\n## Configure your local environment\\r\\n\\r\\nBefore you begin, you must have the following:\\r\\n\\r\\n- [.NET 8.0 SDK](https://dotnet.microsoft.com/download).\\r\\n- One of the following tools for creating Azure resources:\\r\\n\\r\\n    - [Azure CLI](/en-us/cli/azure/install-azure-cli)[version 2.4](/en-us/cli/azure/release-notes-azure-cli#april-21-2020) or later.\\r\\n    - The Azure [Az PowerShell module](/en-us/powershell/azure/install-azure-powershell) version 5.9.0 or later.\\r\\n\\r\\nYou also need an Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\n## Create a local function project\\r\\n\\r\\nIn Azure Functions, a function project is a container for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations. In this section, you create a function project that contains a single function.\\r\\n\\r\\n1. Run the `func init` command, as follows, to create a functions project in a folder named *LocalFunctionProj* with the specified runtime:\\r\\n\\r\\n    ```console\\r\\n    func init LocalFunctionProj --worker-runtime dotnet-isolated --target-framework net8.0\\r\\n    ```\\r\\n2. Navigate into the project folder:\\r\\n\\r\\n    ```console\\r\\n    cd LocalFunctionProj\\r\\n    ```\\r\\n\\r\\n    This folder contains various files for the project, including configurations files named [local.settings.json](functions-develop-local#local-settings-file) and [host.json](functions-host-json). Because *local.settings.json* can contain secrets downloaded from Azure, the file is excluded from source control by default in the *.gitignore* file.\\r\\n3. Add a function to your project by using the following command, where the `--name` argument is the unique name of your function (HttpExample) and the `--template` argument specifies the function\\'s trigger (HTTP).\\r\\n\\r\\n    ```console\\r\\n    func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\"\\r\\n    ```\\r\\n\\r\\n    `func new` creates an HttpExample.cs code file.\\r\\n\\r\\n### (Optional) Examine the file contents\\r\\n\\r\\nIf desired, you can skip to Run the function locally and examine the file contents later.\\r\\n\\r\\n#### HttpExample.cs\\r\\n\\r\\n*HttpExample.cs* contains a `Run` method that receives request data in the `req` variable as an [HttpRequest](/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest) object. That parameter is decorated with the **HttpTriggerAttribute**, to define the trigger behavior.\\r\\n\\r\\n```csharp\\r\\nusing System.Net;\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Extensions.Logging;\\r\\nusing Microsoft.AspNetCore.Http;\\r\\nusing Microsoft.AspNetCore.Mvc;\\r\\n\\r\\nnamespace Company.Function\\r\\n{\\r\\n    public class HttpExample\\r\\n    {\\r\\n        private readonly ILogger<HttpExample> _logger;\\r\\n\\r\\n        public HttpExample(ILogger<HttpExample> logger)\\r\\n        {\\r\\n            _logger = logger;\\r\\n        }\\r\\n\\r\\n        [Function(\"HttpExample\")]\\r\\n        public IActionResult Run([HttpTrigger(AuthorizationLevel.AuthLevelValue, \"get\", \"post\")] HttpRequest req)\\r\\n        {            \\r\\n            _logger.LogInformation(\"C# HTTP trigger function processed a request.\");\\r\\n\\r\\n            return new OkObjectResult(\"Welcome to Azure Functions!\");\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe return object is an [IActionResult](/en-us/dotnet/api/microsoft.aspnetcore.mvc.iactionresult) object that contains the data that\\'s handed back to the HTTP response.\\r\\n\\r\\nTo learn more, see [Azure Functions HTTP triggers and bindings](functions-bindings-http-webhook?tabs=csharp).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder:\\r\\n\\r\\n    ```\\r\\n    func start\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following lines should appear:\\r\\n\\r\\n    ```\\r\\n     ...\\r\\n    \\r\\n     Now listening on: http://0.0.0.0:7071\\r\\n     Application started. Press Ctrl+C to shut down.\\r\\n    \\r\\n     Http Functions:\\r\\n    \\r\\n             HttpExample: [GET,POST] http://localhost:7071/api/HttpExample\\r\\n     ...\\r\\n\\r\\n    ```\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown above, you likely started the host from outside the root folder of the project. In that case, use **Ctrl**+**C** to stop the host, navigate to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your `HttpExample` function from this output to a browser and browse to the function URL and you should receive a *Welcome to Azure Functions* message.\\r\\n3. When you\\'re done, use **Ctrl**+**C** and choose `y` to stop the functions host.\\r\\n\\r\\n## Create supporting Azure resources for your function\\r\\n\\r\\nBefore you can deploy your function code to Azure, you need to create three resources:\\r\\n\\r\\n- A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n- A [Storage account](../storage/common/storage-account-create), which is used to maintain state and other information about your functions.\\r\\n- A function app, which provides the environment for executing your function code. A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nUse the following commands to create these items. Both Azure CLI and PowerShell are supported.\\r\\n\\r\\n1. If you haven\\'t done so already, sign in to Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az login\\r\\n    ```\\r\\n\\r\\nThe [az login](/en-us/cli/azure/reference-index#az-login) command signs you into your Azure account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    Connect-AzAccount\\r\\n    ```\\r\\n\\r\\nThe [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet signs you into your Azure account.\\r\\n2. Create a resource group named `AzureFunctionsQuickstart-rg` in your chosen region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az group create --name AzureFunctionsQuickstart-rg --location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [az group create](/en-us/cli/azure/group#az-group-create) command creates a resource group. In the above command, replace `<REGION>` with a region near you, using an available region code returned from the [az account list-locations](/en-us/cli/azure/account#az-account-list-locations) command.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzResourceGroup -Name AzureFunctionsQuickstart-rg -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzResourceGroup](/en-us/powershell/module/az.resources/new-azresourcegroup) command creates a resource group. You generally create your resource group and resources in a region near you, using an available region returned from the [Get-AzLocation](/en-us/powershell/module/az.resources/get-azlocation) cmdlet.\\r\\n3. Create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az storage account create --name <STORAGE_NAME> --location <REGION> --resource-group AzureFunctionsQuickstart-rg --sku Standard_LRS --allow-blob-public-access false\\r\\n    ```\\r\\n\\r\\nThe [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command creates the storage account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzStorageAccount -ResourceGroupName AzureFunctionsQuickstart-rg -Name <STORAGE_NAME> -SkuName Standard_LRS -Location <REGION> -AllowBlobPublicAccess $false\\r\\n    ```\\r\\n\\r\\nThe [New-AzStorageAccount](/en-us/powershell/module/az.storage/new-azstorageaccount) cmdlet creates the storage account.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements).\\r\\n\\r\\nImportant\\r\\n\\r\\nThe storage account is used to store important app data, sometimes including the application code itself. You should limit access from other apps and users to the storage account.\\r\\n\\r\\n1. Create the function app in Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az functionapp create --resource-group AzureFunctionsQuickstart-rg --consumption-plan-location <REGION> --runtime dotnet-isolated --functions-version 4 --name <APP_NAME> --storage-account <STORAGE_NAME>\\r\\n    ```\\r\\n\\r\\nThe [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command creates the function app in Azure.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzFunctionApp -Name <APP_NAME> -ResourceGroupName AzureFunctionsQuickstart-rg -StorageAccount <STORAGE_NAME> -Runtime dotnet-isolated -FunctionsVersion 4 -Location \\'<REGION>\\'\\r\\n    ```\\r\\n\\r\\nThe [New-AzFunctionApp](/en-us/powershell/module/az.functions/new-azfunctionapp) cmdlet creates the function app in Azure.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, and replace `<APP_NAME>` with a globally unique name appropriate to you. The `<APP_NAME>` is also the default DNS domain for the function app.\\r\\n\\r\\n    This command creates a function app running in your specified language runtime under the [Azure Functions Consumption Plan](consumption-plan), which is free for the amount of usage you incur here. The command also creates an associated Azure Application Insights instance in the same resource group, with which you can monitor your function app and view logs. For more information, see [Monitor Azure Functions](functions-monitoring). The instance incurs no costs until you activate it.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nAfter you\\'ve successfully created your function app in Azure, you\\'re now ready to deploy your local functions project by using the [`func azure functionapp publish`](functions-run-local#project-file-deployment) command.\\r\\n\\r\\nIn your root project folder, run this [`func azure functionapp publish`](functions-core-tools-reference#func-azure-functionapp-publish) command:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp publish <APP_NAME>\\r\\n```\\r\\n\\r\\nIn this example, replace `<APP_NAME>` with the name of your app. A successful deployment shows results similar to the following output (truncated for simplicity):\\r\\n\\r\\n```\\r\\n...\\r\\n\\r\\nGetting site publishing info...\\r\\nCreating archive for current directory...\\r\\nPerforming remote build for functions project.\\r\\n\\r\\n...\\r\\n\\r\\nDeployment successful.\\r\\nRemote build succeeded!\\r\\nSyncing triggers...\\r\\nFunctions in msdocs-azurefunctions-qs:\\r\\n    HttpExample - [httpTrigger]\\r\\n        Invoke url: https://msdocs-azurefunctions-qs.azurewebsites.net/api/httpexample\\r\\n```\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger and supports GET requests, you invoke it by making an HTTP request to its URL. It\\'s easiest to do this in a browser.\\r\\n\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar. When you navigate to this URL, the browser should display similar output as when you ran the function locally.\\r\\n\\r\\nRun the following command to view near real-time streaming logs:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp logstream <APP_NAME> \\r\\n```\\r\\n\\r\\nIn a separate terminal window or in the browser, call the remote function again. A verbose log of the function execution in Azure is shown in the terminal.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you continue to the next step and add an Azure Storage queue output binding, keep all your resources in place as you\\'ll build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, use the following command to delete the resource group and all its contained resources to avoid incurring further costs.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsQuickstart-rg\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n```azurepowershell\\r\\nRemove-AzResourceGroup -Name AzureFunctionsQuickstart-rg\\r\\n```',\n",
       "  'metadata': {}},\n",
       " {'_id': '1d622912-7821-bfb4-11e4-72169ac12368',\n",
       "  'title': 'Create a Java function from the command line - Azure Functions',\n",
       "  'text': '# Quickstart: Create a Java function in Azure from the command line\\r\\n\\r\\n- [C#](create-first-function-cli-csharp)\\r\\n- [Java](create-first-function-cli-java)\\r\\n- [JavaScript](create-first-function-cli-node)\\r\\n- [PowerShell](create-first-function-cli-powershell)\\r\\n- [Python](create-first-function-cli-python)\\r\\n- [TypeScript](create-first-function-cli-typescript)\\r\\n\\r\\nIn this article, you use command-line tools to create a Java function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nIf Maven isn\\'t your preferred development tool, check out our similar tutorials for Java developers:\\r\\n\\r\\n- [Gradle](functions-create-first-java-gradle)\\r\\n- [IntelliJ IDEA](functions-create-maven-intellij)\\r\\n- [Visual Studio Code](create-first-function-vs-code-java)\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\n## Configure your local environment\\r\\n\\r\\nBefore you begin, you must have the following:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- The [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later.\\r\\n- The [Java Developer Kit](/en-us/azure/developer/java/fundamentals/java-support-on-azure), version 8, 11, 17, 21(Linux only). The `JAVA_HOME` environment variable must be set to the install location of the correct version of the JDK.\\r\\n- [Apache Maven](https://maven.apache.org), version 3.0 or above.\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\n## Create a local function project\\r\\n\\r\\nIn Azure Functions, a function project is a container for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations. In this section, you create a function project that contains a single function.\\r\\n\\r\\n1. In an empty folder, run the following command to generate the Functions project from a [Maven archetype](https://maven.apache.org/guides/introduction/introduction-to-archetypes.html).\\r\\n\\r\\n**Bash**\\r\\n\\r\\n    ```bash\\r\\n    mvn archetype:generate -DarchetypeGroupId=com.microsoft.azure -DarchetypeArtifactId=azure-functions-archetype -DjavaVersion=8\\r\\n    ```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n    ```powershell\\r\\n    mvn archetype:generate \"-DarchetypeGroupId=com.microsoft.azure\" \"-DarchetypeArtifactId=azure-functions-archetype\" \"-DjavaVersion=8\" \\r\\n    ```\\r\\n\\r\\n**Cmd**\\r\\n\\r\\n    ```cmd\\r\\n    mvn archetype:generate \"-DarchetypeGroupId=com.microsoft.azure\" \"-DarchetypeArtifactId=azure-functions-archetype\" \"-DjavaVersion=8\"\\r\\n    ```\\r\\n\\r\\n    Important\\r\\n\\r\\n    - Use `-DjavaVersion=11` if you want your functions to run on Java 11. To learn more, see [Java versions](functions-reference-java#java-versions).\\r\\n    - The `JAVA_HOME` environment variable must be set to the install location of the correct version of the JDK to complete this article.\\r\\n2. Maven asks you for values needed to finish generating the project on deployment. Provide the following values when prompted:\\r\\n\\r\\n    | Prompt | Value | Description |\\r\\n    | --- | --- | --- |\\r\\n    | **groupId** | `com.fabrikam` | A value that uniquely identifies your project across all projects, following the [package naming rules](https://docs.oracle.com/javase/specs/jls/se6/html/packages.html#7.7) for Java. |\\r\\n    | **artifactId** | `fabrikam-functions` | A value that is the name of the jar, without a version number. |\\r\\n    | **version** | `1.0-SNAPSHOT` | Choose the default value. |\\r\\n    | **package** | `com.fabrikam` | A value that is the Java package for the generated function code. Use the default. |\\r\\n3. Type `Y` or press Enter to confirm.\\r\\n\\r\\n    Maven creates the project files in a new folder with a name of *artifactId*, which in this example is `fabrikam-functions`.\\r\\n4. Navigate into the project folder:\\r\\n\\r\\n    ```console\\r\\n    cd fabrikam-functions\\r\\n    ```\\r\\n\\r\\n    This folder contains various files for the project, including configurations files named [local.settings.json](functions-develop-local#local-settings-file) and [host.json](functions-host-json). Because *local.settings.json* can contain secrets downloaded from Azure, the file is excluded from source control by default in the *.gitignore* file.\\r\\n\\r\\n### (Optional) Examine the file contents\\r\\n\\r\\nIf desired, you can skip to Run the function locally and examine the file contents later.\\r\\n\\r\\n#### Function.java\\r\\n\\r\\n*Function.java* contains a `run` method that receives request data in the `request` variable is an [HttpRequestMessage](/en-us/java/api/com.microsoft.azure.functions.httprequestmessage) that\\'s decorated with the [HttpTrigger](/en-us/java/api/com.microsoft.azure.functions.annotation.httptrigger) annotation, which defines the trigger behavior.\\r\\n\\r\\n```java\\r\\npackage com.fabrikam;\\r\\n\\r\\nimport com.microsoft.azure.functions.ExecutionContext;\\r\\nimport com.microsoft.azure.functions.HttpMethod;\\r\\nimport com.microsoft.azure.functions.HttpRequestMessage;\\r\\nimport com.microsoft.azure.functions.HttpResponseMessage;\\r\\nimport com.microsoft.azure.functions.HttpStatus;\\r\\nimport com.microsoft.azure.functions.annotation.AuthorizationLevel;\\r\\nimport com.microsoft.azure.functions.annotation.FunctionName;\\r\\nimport com.microsoft.azure.functions.annotation.HttpTrigger;\\r\\n\\r\\nimport java.util.Optional;\\r\\n\\r\\n/**\\r\\n * Azure Functions with HTTP Trigger.\\r\\n */\\r\\npublic class Function {\\r\\n    /**\\r\\n     * This function listens at endpoint \"/api/HttpExample\". Two ways to invoke it using \"curl\" command in bash:\\r\\n     * 1. curl -d \"HTTP Body\" {your host}/api/HttpExample\\r\\n     * 2. curl \"{your host}/api/HttpExample?name=HTTP%20Query\"\\r\\n     */\\r\\n    @FunctionName(\"HttpExample\")\\r\\n    public HttpResponseMessage run(\\r\\n            @HttpTrigger(\\r\\n                name = \"req\",\\r\\n                methods = {HttpMethod.GET, HttpMethod.POST},\\r\\n                authLevel = AuthorizationLevel.ANONYMOUS)\\r\\n                HttpRequestMessage<Optional<String>> request,\\r\\n            final ExecutionContext context) {\\r\\n        context.getLogger().info(\"Java HTTP trigger processed a request.\");\\r\\n\\r\\n        // Parse query parameter\\r\\n        final String query = request.getQueryParameters().get(\"name\");\\r\\n        final String name = request.getBody().orElse(query);\\r\\n\\r\\n        if (name == null) {\\r\\n            return request.createResponseBuilder(HttpStatus.BAD_REQUEST).body(\"Please pass a name on the query string or in the request body\").build();\\r\\n        } else {\\r\\n            return request.createResponseBuilder(HttpStatus.OK).body(\"Hello, \" + name).build();\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe response message is generated by the [HttpResponseMessage.Builder](/en-us/java/api/com.microsoft.azure.functions.httpresponsemessage.builder) API.\\r\\n\\r\\n#### pom.xml\\r\\n\\r\\nSettings for the Azure resources created to host your app are defined in the **configuration** element of the plugin with a **groupId** of `com.microsoft.azure` in the generated pom.xml file. For example, the configuration element below instructs a Maven-based deployment to create a function app in the `java-functions-group` resource group in the `westus` region. The function app itself runs on Windows hosted in the `java-functions-app-service-plan` plan, which by default is a serverless Consumption plan.\\r\\n\\r\\n```xml\\r\\n<plugin>\\r\\n    <groupId>com.microsoft.azure</groupId>\\r\\n    <artifactId>azure-functions-maven-plugin</artifactId>\\r\\n    <version>${azure.functions.maven.plugin.version}</version>\\r\\n    <configuration>\\r\\n        <!-- function app name -->\\r\\n        <appName>${functionAppName}</appName>\\r\\n        <!-- function app resource group -->\\r\\n        <resourceGroup>java-functions-group</resourceGroup>\\r\\n        <!-- function app service plan name -->\\r\\n        <appServicePlanName>java-functions-app-service-plan</appServicePlanName>\\r\\n        <!-- function app region-->\\r\\n        <!-- refers https://github.com/microsoft/azure-maven-plugins/wiki/Azure-Functions:-Configuration-Details#supported-regions for all valid values -->\\r\\n        <region>westus</region>\\r\\n        <!-- function pricingTier, default to be consumption if not specified -->\\r\\n        <!-- refers https://github.com/microsoft/azure-maven-plugins/wiki/Azure-Functions:-Configuration-Details#supported-pricing-tiers for all valid values -->\\r\\n        <!-- <pricingTier></pricingTier> -->\\r\\n        <!-- Whether to disable application insights, default is false -->\\r\\n        <!-- refers https://github.com/microsoft/azure-maven-plugins/wiki/Azure-Functions:-Configuration-Details for all valid configurations for application insights-->\\r\\n        <!-- <disableAppInsights></disableAppInsights> -->\\r\\n        <runtime>\\r\\n            <!-- runtime os, could be windows, linux or docker-->\\r\\n            <os>windows</os>\\r\\n            <javaVersion>8</javaVersion>\\r\\n        </runtime>\\r\\n        <appSettings>\\r\\n            <property>\\r\\n                <name>FUNCTIONS_EXTENSION_VERSION</name>\\r\\n                <value>~4</value>\\r\\n            </property>\\r\\n        </appSettings>\\r\\n    </configuration>\\r\\n    <executions>\\r\\n        <execution>\\r\\n            <id>package-functions</id>\\r\\n            <goals>\\r\\n                <goal>package</goal>\\r\\n            </goals>\\r\\n        </execution>\\r\\n    </executions>\\r\\n</plugin>\\r\\n```\\r\\n\\r\\nYou can change these settings to control how resources are created in Azure, such as by changing `runtime.os` from `windows` to `linux` before initial deployment. For a complete list of settings supported by the Maven plug-in, see the [configuration details](https://github.com/microsoft/azure-maven-plugins/wiki/Azure-Functions:-Configuration-Details).\\r\\n\\r\\n#### FunctionTest.java\\r\\n\\r\\nThe archetype also generates a unit test for your function. When you change your function to add bindings or add new functions to the project, you\\'ll also need to modify the tests in the *FunctionTest.java* file.\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder:\\r\\n\\r\\n    ```console\\r\\n    mvn clean package\\r\\n    mvn azure-functions:run\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following lines should appear:\\r\\n\\r\\n    ```\\r\\n     ...\\r\\n    \\r\\n     Now listening on: http://0.0.0.0:7071\\r\\n     Application started. Press Ctrl+C to shut down.\\r\\n    \\r\\n     Http Functions:\\r\\n    \\r\\n             HttpExample: [GET,POST] http://localhost:7071/api/HttpExample\\r\\n     ...\\r\\n\\r\\n    ```\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown above, you likely started the host from outside the root folder of the project. In that case, use **Ctrl**+**C** to stop the host, navigate to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your `HttpExample` function from this output to a browser and append the query string `?name=<YOUR_NAME>`, making the full URL like `http://localhost:7071/api/HttpExample?name=Functions`. The browser should display a message that echoes back your query string value. The terminal in which you started your project also shows log output as you make requests.\\r\\n3. When you\\'re done, use **Ctrl**+**C** and choose `y` to stop the functions host.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nA function app and related resources are created in Azure when you first deploy your functions project. Settings for the Azure resources created to host your app are defined in the pom.xml file. In this article, you\\'ll accept the defaults.\\r\\n\\r\\nTip\\r\\n\\r\\nTo create a function app running on Linux instead of Windows, change the `runtime.os` element in the pom.xml file from `windows` to `linux`. Running Linux in a consumption plan is supported in [these regions](https://github.com/Azure/azure-functions-host/wiki/Linux-Consumption-Regions). You can\\'t have apps that run on Linux and apps that run on Windows in the same resource group.\\r\\n\\r\\n1. Before you can deploy, sign in to your Azure subscription using either Azure CLI or Azure PowerShell.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az login\\r\\n    ```\\r\\n\\r\\nThe [az login](/en-us/cli/azure/reference-index#az-login) command signs you into your Azure account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    Connect-AzAccount\\r\\n    ```\\r\\n\\r\\nThe [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet signs you into your Azure account.\\r\\n2. Use the following command to deploy your project to a new function app.\\r\\n\\r\\n    ```console\\r\\n    mvn azure-functions:deploy\\r\\n    ```\\r\\n\\r\\n    This creates the following resources in Azure:\\r\\n\\r\\n    - Resource group. Named as *java-functions-group*.\\r\\n    - Storage account. Required by Functions. The name is generated randomly based on Storage account name requirements.\\r\\n    - Hosting plan. Serverless hosting for your function app in the *westus* region. The name is *java-functions-app-service-plan*.\\r\\n    - Function app. A function app is the deployment and execution unit for your functions. The name is randomly generated based on your *artifactId*, appended with a randomly generated number.\\r\\n\\r\\n    The deployment packages the project files and deploys them to the new function app using [zip deployment](functions-deployment-technologies#zip-deploy). The code runs from the deployment package in Azure.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe storage account is used to store important app data, sometimes including the application code itself. You should limit access from other apps and users to the storage account.\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\nRun the following command to view near real-time streaming logs:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp logstream <APP_NAME> \\r\\n```\\r\\n\\r\\nIn a separate terminal window or in the browser, call the remote function again. A verbose log of the function execution in Azure is shown in the terminal.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you continue to the next step and add an Azure Storage queue output binding, keep all your resources in place as you\\'ll build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, use the following command to delete the resource group and all its contained resources to avoid incurring further costs.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz group delete --name java-functions-group\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n```azurepowershell\\r\\nRemove-AzResourceGroup -Name java-functions-group\\r\\n```',\n",
       "  'metadata': {}},\n",
       " {'_id': '43318d61-fd77-6ebb-78f3-e88bf86d482e',\n",
       "  'title': 'Create a JavaScript function from the command line - Azure Functions',\n",
       "  'text': '# Quickstart: Create a JavaScript function in Azure from the command line (nodejs-model-v3)\\r\\n\\r\\nIn this article, you use command-line tools to create a JavaScript function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe content of this article changes based on your choice of the Node.js programming model in the selector at the top of the page. The v4 model is generally available and is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](functions-node-upgrade-v4).\\r\\n\\r\\nCompletion of this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [Visual Studio Code-based version](create-first-function-vs-code-node) of this article.\\r\\n\\r\\n## Configure your local environment\\r\\n\\r\\nBefore you begin, you must have the following prerequisites:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- One of the following tools for creating Azure resources:\\r\\n\\r\\n    - [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later.\\r\\n    - The Azure [Az PowerShell module](/en-us/powershell/azure/install-azure-powershell) version 5.9.0 or later.\\r\\n\\r\\n- [Node.js](https://nodejs.org/) version 14 or above.\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\n## Create a local function project\\r\\n\\r\\nIn Azure Functions, a function project is a container for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations. In this section, you create a function project that contains a single function.\\r\\n\\r\\n1. In a suitable folder, run the [`func init`](functions-core-tools-reference#func-init) command, as follows, to create a JavaScript Node.js v3 project in the current folder:\\r\\n\\r\\n    ```console\\r\\n    func init --javascript --model V3\\r\\n    ```\\r\\n\\r\\n    This folder now contains various files for the project, including configurations files named [local.settings.json](functions-develop-local#local-settings-file) and [host.json](functions-host-json). Because *local.settings.json* can contain secrets downloaded from Azure, the file is excluded from source control by default in the *.gitignore* file.\\r\\n2. Add a function to your project by using the following command, where the `--name` argument is the unique name of your function (HttpExample) and the `--template` argument specifies the function\\'s trigger (HTTP).\\r\\n\\r\\n    ```console\\r\\n    func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\"\\r\\n    ```\\r\\n\\r\\n    The [`func new`](functions-core-tools-reference#func-new) command creates a subfolder matching the function name that contains a code file appropriate to the project\\'s chosen language and a configuration file named *function.json*.\\r\\n\\r\\nYou may find the [Azure Functions Core Tools reference](functions-core-tools-reference) helpful.\\r\\n\\r\\n### (Optional) Examine the file contents\\r\\n\\r\\nIf desired, you can skip to Run the function locally and examine the file contents later.\\r\\n\\r\\n#### index.js\\r\\n\\r\\n*index.js* exports a function that\\'s triggered according to the configuration in *function.json*.\\r\\n\\r\\n```javascript\\r\\nmodule.exports = async function (context, req) {\\r\\n    context.log(\\'JavaScript HTTP trigger function processed a request.\\');\\r\\n\\r\\n    const name = (req.query.name || (req.body && req.body.name));\\r\\n    const responseMessage = name\\r\\n        ? \"Hello, \" + name + \". This HTTP triggered function executed successfully.\"\\r\\n        : \"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\";\\r\\n\\r\\n    context.res = {\\r\\n        // status: 200, /* Defaults to 200 */\\r\\n        body: responseMessage\\r\\n    };\\r\\n}\\r\\n```\\r\\n\\r\\nFor an HTTP trigger, the function receives request data in the variable `req` as defined in *function.json*. The response is defined as `res` in *function.json* and can be accessed using `context.res`. To learn more, see [Azure Functions HTTP triggers and bindings](functions-bindings-http-webhook?tabs=javascript).\\r\\n\\r\\n#### function.json\\r\\n\\r\\n*function.json* is a configuration file that defines the input and output `bindings` for the function, including the trigger type.\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"bindings\": [\\r\\n        {\\r\\n            \"authLevel\": \"function\",\\r\\n            \"type\": \"httpTrigger\",\\r\\n            \"direction\": \"in\",\\r\\n            \"name\": \"req\",\\r\\n            \"methods\": [\\r\\n                \"get\",\\r\\n                \"post\"\\r\\n            ]\\r\\n        },\\r\\n        {\\r\\n            \"type\": \"http\",\\r\\n            \"direction\": \"out\",\\r\\n            \"name\": \"res\"\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n```\\r\\n\\r\\nEach binding requires a direction, a type, and a unique name. The HTTP trigger has an input binding of type [`httpTrigger`](functions-bindings-http-webhook-trigger) and output binding of type [`http`](functions-bindings-http-webhook-output).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder.\\r\\n\\r\\n    ```console\\r\\n    func start\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following lines must appear:\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown above, you likely started the host from outside the root folder of the project. In that case, use **Ctrl**+**C** to stop the host, go to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your HTTP function from this output to a browser and append the query string `?name=<YOUR_NAME>`, making the full URL like `http://localhost:7071/api/HttpExample?name=Functions`. The browser should display a response message that echoes back your query string value. The terminal in which you started your project also shows log output as you make requests.\\r\\n3. When you\\'re done, press Ctrl + C and type `y` to stop the functions host.\\r\\n\\r\\n## Create supporting Azure resources for your function\\r\\n\\r\\nBefore you can deploy your function code to Azure, you need to create three resources:\\r\\n\\r\\n- A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n- A [Storage account](../storage/common/storage-account-create), which is used to maintain state and other information about your functions.\\r\\n- A function app, which provides the environment for executing your function code. A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nUse the following commands to create these items. Both Azure CLI and PowerShell are supported.\\r\\n\\r\\n1. If you haven\\'t done so already, sign in to Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az login\\r\\n    ```\\r\\n\\r\\nThe [az login](/en-us/cli/azure/reference-index#az-login) command signs you into your Azure account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    Connect-AzAccount\\r\\n    ```\\r\\n\\r\\nThe [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet signs you into your Azure account.\\r\\n2. Create a resource group named `AzureFunctionsQuickstart-rg` in your chosen region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az group create --name AzureFunctionsQuickstart-rg --location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [az group create](/en-us/cli/azure/group#az-group-create) command creates a resource group. In the above command, replace `<REGION>` with a region near you, using an available region code returned from the [az account list-locations](/en-us/cli/azure/account#az-account-list-locations) command.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzResourceGroup -Name AzureFunctionsQuickstart-rg -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzResourceGroup](/en-us/powershell/module/az.resources/new-azresourcegroup) command creates a resource group. You generally create your resource group and resources in a region near you, using an available region returned from the [Get-AzLocation](/en-us/powershell/module/az.resources/get-azlocation) cmdlet.\\r\\n3. Create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az storage account create --name <STORAGE_NAME> --location <REGION> --resource-group AzureFunctionsQuickstart-rg --sku Standard_LRS --allow-blob-public-access false\\r\\n    ```\\r\\n\\r\\nThe [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command creates the storage account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzStorageAccount -ResourceGroupName AzureFunctionsQuickstart-rg -Name <STORAGE_NAME> -SkuName Standard_LRS -Location <REGION> -AllowBlobPublicAccess $false\\r\\n    ```\\r\\n\\r\\nThe [New-AzStorageAccount](/en-us/powershell/module/az.storage/new-azstorageaccount) cmdlet creates the storage account.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements).\\r\\n\\r\\nImportant\\r\\n\\r\\nThe storage account is used to store important app data, sometimes including the application code itself. You should limit access from other apps and users to the storage account.\\r\\n\\r\\n1. Create the function app in Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az functionapp create --resource-group AzureFunctionsQuickstart-rg --consumption-plan-location <REGION> --runtime node --runtime-version 18 --functions-version 4 --name <APP_NAME> --storage-account <STORAGE_NAME>\\r\\n    ```\\r\\n\\r\\nThe [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command creates the function app in Azure. It\\'s recommended that you use the latest LTS version of Node.js, which is currently 18. You can specify the version by setting `--runtime-version` to `18`.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzFunctionApp -Name <APP_NAME> -ResourceGroupName AzureFunctionsQuickstart-rg -StorageAccount <STORAGE_NAME> -Runtime node -RuntimeVersion 18 -FunctionsVersion 4 -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzFunctionApp](/en-us/powershell/module/az.functions/new-azfunctionapp) cmdlet creates the function app in Azure. It\\'s recommended that you use the latest LTS version of Node.js, which is currently 18. You can specify the version by setting `--runtime-version` to `18`.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, and replace `<APP_NAME>` with a globally unique name appropriate to you. The `<APP_NAME>` is also the default DNS domain for the function app.\\r\\n\\r\\n    This command creates a function app running in your specified language runtime under the [Azure Functions Consumption Plan](consumption-plan), which is free for the amount of usage you incur here. The command also creates an associated Azure Application Insights instance in the same resource group, with which you can monitor your function app and view logs. For more information, see [Monitor Azure Functions](functions-monitoring). The instance incurs no costs until you activate it.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nAfter you\\'ve successfully created your function app in Azure, you\\'re now ready to deploy your local functions project by using the [`func azure functionapp publish`](functions-run-local#project-file-deployment) command.\\r\\n\\r\\nIn your root project folder, run this [`func azure functionapp publish`](functions-core-tools-reference#func-azure-functionapp-publish) command:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp publish <APP_NAME>\\r\\n```\\r\\n\\r\\nIn this example, replace `<APP_NAME>` with the name of your app. A successful deployment shows results similar to the following output (truncated for simplicity):\\r\\n\\r\\n```\\r\\n...\\r\\n\\r\\nGetting site publishing info...\\r\\nCreating archive for current directory...\\r\\nPerforming remote build for functions project.\\r\\n\\r\\n...\\r\\n\\r\\nDeployment successful.\\r\\nRemote build succeeded!\\r\\nSyncing triggers...\\r\\nFunctions in msdocs-azurefunctions-qs:\\r\\n    HttpExample - [httpTrigger]\\r\\n        Invoke url: https://msdocs-azurefunctions-qs.azurewebsites.net/api/httpexample\\r\\n```\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\nRun the following command to view near real-time streaming logs:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp logstream <APP_NAME> \\r\\n```\\r\\n\\r\\nIn a separate terminal window or in the browser, call the remote function again. A verbose log of the function execution in Azure is shown in the terminal.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you continue to the next step and add an Azure Storage queue output binding, keep all your resources in place as you\\'ll build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, use the following command to delete the resource group and all its contained resources to avoid incurring further costs.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsQuickstart-rg\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n```azurepowershell\\r\\nRemove-AzResourceGroup -Name AzureFunctionsQuickstart-rg\\r\\n```\\r\\n\\r\\n# Quickstart: Create a JavaScript function in Azure from the command line (nodejs-model-v4)\\r\\n\\r\\nIn this article, you use command-line tools to create a JavaScript function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe content of this article changes based on your choice of the Node.js programming model in the selector at the top of the page. The v4 model is generally available and is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](functions-node-upgrade-v4).\\r\\n\\r\\nCompletion of this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [Visual Studio Code-based version](create-first-function-vs-code-node) of this article.\\r\\n\\r\\n## Configure your local environment\\r\\n\\r\\nBefore you begin, you must have the following prerequisites:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- One of the following tools for creating Azure resources:\\r\\n\\r\\n    - [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later.\\r\\n    - The Azure [Az PowerShell module](/en-us/powershell/azure/install-azure-powershell) version 5.9.0 or later.\\r\\n\\r\\n- [Node.js](https://nodejs.org/) version 18 or above.\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\n- Make sure you install version v4.0.5382 of the Core Tools, or a later version.\\r\\n\\r\\n## Create a local function project\\r\\n\\r\\nIn Azure Functions, a function project is a container for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations. In this section, you create a function project that contains a single function.\\r\\n\\r\\n1. In a suitable folder, run the [`func init`](functions-core-tools-reference#func-init) command, as follows, to create a JavaScript Node.js v4 project in the current folder:\\r\\n\\r\\n    ```console\\r\\n    func init --javascript\\r\\n    ```\\r\\n\\r\\nThis folder now contains various files for the project, including configurations files named *local.settings.json* and *host.json*. Because *local.settings.json* can contain secrets downloaded from Azure, the file is excluded from source control by default in the *.gitignore* file. Required npm packages are also installed in *node\\\\_modules*.\\r\\n\\r\\n1. Add a function to your project by using the following command, where the `--name` argument is the unique name of your function (HttpExample) and the `--template` argument specifies the function\\'s trigger (HTTP).\\r\\n\\r\\n    ```console\\r\\n    func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\" \\r\\n    \\r\\n    [`func new`](functions-core-tools-reference.md#func-new) creates a file named *HttpExample.js* in the *src/functions* directory, which contains your function\\'s code. \\r\\n    \\r\\n    ```\\r\\n2. Add Azure Storage connection information in *local.settings.json*.\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n        \"Values\": {       \\r\\n            \"AzureWebJobsStorage\": \"<Azure Storage connection information>\",\\r\\n            \"FUNCTIONS_WORKER_RUNTIME\": \"node\"\\r\\n        }\\r\\n    }\\r\\n    ```\\r\\n3. (Optional) If you want to learn more about a particular function, say HTTP trigger, you can run the following command:\\r\\n\\r\\n    ```console\\r\\n    func help httptrigger\\r\\n    ```\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder.\\r\\n\\r\\n    ```console\\r\\n    func start\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following lines must appear:\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown above, you likely started the host from outside the root folder of the project. In that case, use **Ctrl**+**C** to stop the host, go to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your HTTP function from this output to a browser and append the query string `?name=<YOUR_NAME>`, making the full URL like `http://localhost:7071/api/HttpExample?name=Functions`. The browser should display a response message that echoes back your query string value. The terminal in which you started your project also shows log output as you make requests.\\r\\n3. When you\\'re done, press Ctrl + C and type `y` to stop the functions host.\\r\\n\\r\\n## Create supporting Azure resources for your function\\r\\n\\r\\nBefore you can deploy your function code to Azure, you need to create three resources:\\r\\n\\r\\n- A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n- A [Storage account](../storage/common/storage-account-create), which is used to maintain state and other information about your functions.\\r\\n- A function app, which provides the environment for executing your function code. A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nUse the following commands to create these items. Both Azure CLI and PowerShell are supported.\\r\\n\\r\\n1. If you haven\\'t done so already, sign in to Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az login\\r\\n    ```\\r\\n\\r\\nThe [az login](/en-us/cli/azure/reference-index#az-login) command signs you into your Azure account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    Connect-AzAccount\\r\\n    ```\\r\\n\\r\\nThe [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet signs you into your Azure account.\\r\\n2. Create a resource group named `AzureFunctionsQuickstart-rg` in your chosen region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az group create --name AzureFunctionsQuickstart-rg --location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [az group create](/en-us/cli/azure/group#az-group-create) command creates a resource group. In the above command, replace `<REGION>` with a region near you, using an available region code returned from the [az account list-locations](/en-us/cli/azure/account#az-account-list-locations) command.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzResourceGroup -Name AzureFunctionsQuickstart-rg -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzResourceGroup](/en-us/powershell/module/az.resources/new-azresourcegroup) command creates a resource group. You generally create your resource group and resources in a region near you, using an available region returned from the [Get-AzLocation](/en-us/powershell/module/az.resources/get-azlocation) cmdlet.\\r\\n3. Create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az storage account create --name <STORAGE_NAME> --location <REGION> --resource-group AzureFunctionsQuickstart-rg --sku Standard_LRS --allow-blob-public-access false\\r\\n    ```\\r\\n\\r\\nThe [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command creates the storage account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzStorageAccount -ResourceGroupName AzureFunctionsQuickstart-rg -Name <STORAGE_NAME> -SkuName Standard_LRS -Location <REGION> -AllowBlobPublicAccess $false\\r\\n    ```\\r\\n\\r\\nThe [New-AzStorageAccount](/en-us/powershell/module/az.storage/new-azstorageaccount) cmdlet creates the storage account.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements).\\r\\n\\r\\nImportant\\r\\n\\r\\nThe storage account is used to store important app data, sometimes including the application code itself. You should limit access from other apps and users to the storage account.\\r\\n\\r\\n1. Create the function app in Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az functionapp create --resource-group AzureFunctionsQuickstart-rg --consumption-plan-location <REGION> --runtime node --runtime-version 18 --functions-version 4 --name <APP_NAME> --storage-account <STORAGE_NAME>\\r\\n    ```\\r\\n\\r\\nThe [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command creates the function app in Azure. It\\'s recommended that you use the latest LTS version of Node.js, which is currently 18. You can specify the version by setting `--runtime-version` to `18`.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzFunctionApp -Name <APP_NAME> -ResourceGroupName AzureFunctionsQuickstart-rg -StorageAccount <STORAGE_NAME> -Runtime node -RuntimeVersion 18 -FunctionsVersion 4 -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzFunctionApp](/en-us/powershell/module/az.functions/new-azfunctionapp) cmdlet creates the function app in Azure. It\\'s recommended that you use the latest LTS version of Node.js, which is currently 18. You can specify the version by setting `--runtime-version` to `18`.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, and replace `<APP_NAME>` with a globally unique name appropriate to you. The `<APP_NAME>` is also the default DNS domain for the function app.\\r\\n\\r\\n    This command creates a function app running in your specified language runtime under the [Azure Functions Consumption Plan](consumption-plan), which is free for the amount of usage you incur here. The command also creates an associated Azure Application Insights instance in the same resource group, with which you can monitor your function app and view logs. For more information, see [Monitor Azure Functions](functions-monitoring). The instance incurs no costs until you activate it.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nAfter you\\'ve successfully created your function app in Azure, you\\'re now ready to deploy your local functions project by using the [`func azure functionapp publish`](functions-run-local#project-file-deployment) command.\\r\\n\\r\\nIn your root project folder, run this [`func azure functionapp publish`](functions-core-tools-reference#func-azure-functionapp-publish) command:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp publish <APP_NAME>\\r\\n```\\r\\n\\r\\nIn this example, replace `<APP_NAME>` with the name of your app. A successful deployment shows results similar to the following output (truncated for simplicity):\\r\\n\\r\\n```\\r\\n...\\r\\n\\r\\nGetting site publishing info...\\r\\nCreating archive for current directory...\\r\\nPerforming remote build for functions project.\\r\\n\\r\\n...\\r\\n\\r\\nDeployment successful.\\r\\nRemote build succeeded!\\r\\nSyncing triggers...\\r\\nFunctions in msdocs-azurefunctions-qs:\\r\\n    HttpExample - [httpTrigger]\\r\\n        Invoke url: https://msdocs-azurefunctions-qs.azurewebsites.net/api/httpexample\\r\\n```\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\nRun the following command to view near real-time streaming logs:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp logstream <APP_NAME> \\r\\n```\\r\\n\\r\\nIn a separate terminal window or in the browser, call the remote function again. A verbose log of the function execution in Azure is shown in the terminal.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you continue to the next step and add an Azure Storage queue output binding, keep all your resources in place as you\\'ll build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, use the following command to delete the resource group and all its contained resources to avoid incurring further costs.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsQuickstart-rg\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n```azurepowershell\\r\\nRemove-AzResourceGroup -Name AzureFunctionsQuickstart-rg\\r\\n```',\n",
       "  'metadata': {}},\n",
       " {'_id': '30bd7cb9-e3aa-9f2d-f3b4-56cf5357467d',\n",
       "  'title': 'Create a PowerShell function from the command line - Azure Functions',\n",
       "  'text': '# Quickstart: Create a PowerShell function in Azure from the command line\\r\\n\\r\\n- [C#](create-first-function-cli-csharp)\\r\\n- [Java](create-first-function-cli-java)\\r\\n- [JavaScript](create-first-function-cli-node)\\r\\n- [PowerShell](create-first-function-cli-powershell)\\r\\n- [Python](create-first-function-cli-python)\\r\\n- [TypeScript](create-first-function-cli-typescript)\\r\\n\\r\\nIn this article, you use command-line tools to create a PowerShell function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere is also a [Visual Studio Code-based version](create-first-function-vs-code-powershell) of this article.\\r\\n\\r\\n## Configure your local environment\\r\\n\\r\\nBefore you begin, you must have the following:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- One of the following tools for creating Azure resources:\\r\\n\\r\\n    - The Azure [Az PowerShell module](/en-us/powershell/azure/install-azure-powershell) version 9.4.0 or later.\\r\\n    - [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later.\\r\\n- The [.NET 6.0 SDK](https://dotnet.microsoft.com/download)\\r\\n- [PowerShell 7.4](/en-us/powershell/scripting/install/installing-powershell-core-on-windows)\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\n## Create a local function project\\r\\n\\r\\nIn Azure Functions, a function project is a container for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations. In this section, you create a function project that contains a single function.\\r\\n\\r\\n1. Run the `func init` command, as follows, to create a functions project in a folder named *LocalFunctionProj* with the specified runtime:\\r\\n\\r\\n    ```console\\r\\n    func init LocalFunctionProj --powershell\\r\\n    ```\\r\\n2. Navigate into the project folder:\\r\\n\\r\\n    ```console\\r\\n    cd LocalFunctionProj\\r\\n    ```\\r\\n\\r\\n    This folder contains various files for the project, including configurations files named [local.settings.json](functions-develop-local#local-settings-file) and [host.json](functions-host-json). Because *local.settings.json* can contain secrets downloaded from Azure, the file is excluded from source control by default in the *.gitignore* file.\\r\\n3. Add a function to your project by using the following command, where the `--name` argument is the unique name of your function (HttpExample) and the `--template` argument specifies the function\\'s trigger (HTTP).\\r\\n\\r\\n    ```console\\r\\n    func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\"\\r\\n    ```\\r\\n\\r\\n    `func new` creates a subfolder matching the function name that contains a code file appropriate to the project\\'s chosen language and a configuration file named *function.json*.\\r\\n\\r\\n### (Optional) Examine the file contents\\r\\n\\r\\nIf desired, you can skip to Run the function locally and examine the file contents later.\\r\\n\\r\\n#### run.ps1\\r\\n\\r\\n*run.ps1* defines a function script that\\'s triggered according to the configuration in *function.json*.\\r\\n\\r\\n```powershell\\r\\nusing namespace System.Net\\r\\n\\r\\n# Input bindings are passed in via param block.\\r\\nparam($Request, $TriggerMetadata)\\r\\n\\r\\n# Write to the Azure Functions log stream.\\r\\nWrite-Host \"PowerShell HTTP trigger function processed a request.\"\\r\\n\\r\\n# Interact with query parameters or the body of the request.\\r\\n$name = $Request.Query.Name\\r\\nif (-not $name) {\\r\\n    $name = $Request.Body.Name\\r\\n}\\r\\n\\r\\n$body = \"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\"\\r\\n\\r\\nif ($name) {\\r\\n    $body = \"Hello, $name. This HTTP triggered function executed successfully.\"\\r\\n}\\r\\n\\r\\n# Associate values to output bindings by calling \\'Push-OutputBinding\\'.\\r\\nPush-OutputBinding -Name Response -Value ([HttpResponseContext]@{\\r\\n    StatusCode = [HttpStatusCode]::OK\\r\\n    Body = $body\\r\\n})\\r\\n```\\r\\n\\r\\nFor an HTTP trigger, the function receives request data passed to the `$Request` param defined in *function.json*. The return object, defined as `Response` in *function.json*, is passed to the `Push-OutputBinding` cmdlet as the response.\\r\\n\\r\\n#### function.json\\r\\n\\r\\n*function.json* is a configuration file that defines the input and output `bindings` for the function, including the trigger type.\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"authLevel\": \"function\",\\r\\n      \"type\": \"httpTrigger\",\\r\\n      \"direction\": \"in\",\\r\\n      \"name\": \"Request\",\\r\\n      \"methods\": [\\r\\n        \"get\",\\r\\n        \"post\"\\r\\n      ]\\r\\n    },\\r\\n    {\\r\\n      \"type\": \"http\",\\r\\n      \"direction\": \"out\",\\r\\n      \"name\": \"Response\"\\r\\n    }\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\nEach binding requires a direction, a type, and a unique name. The HTTP trigger has an input binding of type [`httpTrigger`](functions-bindings-http-webhook-trigger) and output binding of type [`http`](functions-bindings-http-webhook-output).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder.\\r\\n\\r\\n    ```console\\r\\n    func start\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following lines must appear:\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown above, you likely started the host from outside the root folder of the project. In that case, use **Ctrl**+**C** to stop the host, go to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your HTTP function from this output to a browser and append the query string `?name=<YOUR_NAME>`, making the full URL like `http://localhost:7071/api/HttpExample?name=Functions`. The browser should display a response message that echoes back your query string value. The terminal in which you started your project also shows log output as you make requests.\\r\\n3. When you\\'re done, press Ctrl + C and type `y` to stop the functions host.\\r\\n\\r\\n## Create supporting Azure resources for your function\\r\\n\\r\\nBefore you can deploy your function code to Azure, you need to create three resources:\\r\\n\\r\\n- A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n- A [Storage account](../storage/common/storage-account-create), which is used to maintain state and other information about your functions.\\r\\n- A function app, which provides the environment for executing your function code. A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nUse the following commands to create these items. Both Azure CLI and PowerShell are supported.\\r\\n\\r\\n1. If you haven\\'t done so already, sign in to Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az login\\r\\n    ```\\r\\n\\r\\nThe [az login](/en-us/cli/azure/reference-index#az-login) command signs you into your Azure account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    Connect-AzAccount\\r\\n    ```\\r\\n\\r\\nThe [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet signs you into your Azure account.\\r\\n2. Create a resource group named `AzureFunctionsQuickstart-rg` in your chosen region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az group create --name AzureFunctionsQuickstart-rg --location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [az group create](/en-us/cli/azure/group#az-group-create) command creates a resource group. In the above command, replace `<REGION>` with a region near you, using an available region code returned from the [az account list-locations](/en-us/cli/azure/account#az-account-list-locations) command.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzResourceGroup -Name AzureFunctionsQuickstart-rg -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzResourceGroup](/en-us/powershell/module/az.resources/new-azresourcegroup) command creates a resource group. You generally create your resource group and resources in a region near you, using an available region returned from the [Get-AzLocation](/en-us/powershell/module/az.resources/get-azlocation) cmdlet.\\r\\n3. Create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az storage account create --name <STORAGE_NAME> --location <REGION> --resource-group AzureFunctionsQuickstart-rg --sku Standard_LRS --allow-blob-public-access false\\r\\n    ```\\r\\n\\r\\nThe [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command creates the storage account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzStorageAccount -ResourceGroupName AzureFunctionsQuickstart-rg -Name <STORAGE_NAME> -SkuName Standard_LRS -Location <REGION> -AllowBlobPublicAccess $false\\r\\n    ```\\r\\n\\r\\nThe [New-AzStorageAccount](/en-us/powershell/module/az.storage/new-azstorageaccount) cmdlet creates the storage account.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements).\\r\\n\\r\\nImportant\\r\\n\\r\\nThe storage account is used to store important app data, sometimes including the application code itself. You should limit access from other apps and users to the storage account.\\r\\n\\r\\n1. Create the function app in Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az functionapp create --resource-group AzureFunctionsQuickstart-rg --consumption-plan-location <REGION> --runtime powershell --functions-version 4 --name <APP_NAME> --storage-account <STORAGE_NAME>\\r\\n    ```\\r\\n\\r\\nThe [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command creates the function app in Azure.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzFunctionApp -Name <APP_NAME> -ResourceGroupName AzureFunctionsQuickstart-rg -StorageAccount <STORAGE_NAME> -Runtime PowerShell -FunctionsVersion 4 -Location \\'<REGION>\\'\\r\\n    ```\\r\\n\\r\\nThe [New-AzFunctionApp](/en-us/powershell/module/az.functions/new-azfunctionapp) cmdlet creates the function app in Azure.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, and replace `<APP_NAME>` with a globally unique name appropriate to you. The `<APP_NAME>` is also the default DNS domain for the function app.\\r\\n\\r\\n    This command creates a function app running in your specified language runtime under the [Azure Functions Consumption Plan](consumption-plan), which is free for the amount of usage you incur here. The command also provisions an associated Azure Application Insights instance in the same resource group, with which you can monitor your function app and view logs. For more information, see [Monitor Azure Functions](functions-monitoring). The instance incurs no costs until you activate it.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nAfter you\\'ve successfully created your function app in Azure, you\\'re now ready to deploy your local functions project by using the [`func azure functionapp publish`](functions-run-local#project-file-deployment) command.\\r\\n\\r\\nIn your root project folder, run this [`func azure functionapp publish`](functions-core-tools-reference#func-azure-functionapp-publish) command:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp publish <APP_NAME>\\r\\n```\\r\\n\\r\\nIn this example, replace `<APP_NAME>` with the name of your app. A successful deployment shows results similar to the following output (truncated for simplicity):\\r\\n\\r\\n```\\r\\n...\\r\\n\\r\\nGetting site publishing info...\\r\\nCreating archive for current directory...\\r\\nPerforming remote build for functions project.\\r\\n\\r\\n...\\r\\n\\r\\nDeployment successful.\\r\\nRemote build succeeded!\\r\\nSyncing triggers...\\r\\nFunctions in msdocs-azurefunctions-qs:\\r\\n    HttpExample - [httpTrigger]\\r\\n        Invoke url: https://msdocs-azurefunctions-qs.azurewebsites.net/api/httpexample\\r\\n```\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\nRun the following command to view near real-time streaming logs:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp logstream <APP_NAME> \\r\\n```\\r\\n\\r\\nIn a separate terminal window or in the browser, call the remote function again. A verbose log of the function execution in Azure is shown in the terminal.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you continue to the next step and add an Azure Storage queue output binding, keep all your resources in place as you\\'ll build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, use the following command to delete the resource group and all its contained resources to avoid incurring further costs.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsQuickstart-rg\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n```azurepowershell\\r\\nRemove-AzResourceGroup -Name AzureFunctionsQuickstart-rg\\r\\n```',\n",
       "  'metadata': {}},\n",
       " {'_id': '79f0ae77-7c48-ae83-2698-eaaea9eb2ef2',\n",
       "  'title': 'Create a Python function from the command line - Azure Functions',\n",
       "  'text': '# Quickstart: Create a Python function in Azure from the command line\\r\\n\\r\\nIn this article, you use command-line tools to create a Python function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nThis article uses the Python v2 programming model for Azure Functions, which provides a decorator-based approach for creating functions. To learn more about the Python v2 programming model, see the [Developer Reference Guide](functions-reference-python?pivots=python-mode-decorators)\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [Visual Studio Code-based version](create-first-function-vs-code-python) of this article.\\r\\n\\r\\n## Configure your local environment\\r\\n\\r\\nBefore you begin, you must have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- One of the following tools for creating Azure resources:\\r\\n\\r\\n    - [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later.\\r\\n    - The Azure [Az PowerShell module](/en-us/powershell/azure/install-azure-powershell) version 5.9.0 or later.\\r\\n- [A Python version supported by Azure Functions](supported-languages#languages-by-runtime-version).\\r\\n- The [Azurite storage emulator](../storage/common/storage-use-azurite?tabs=npm#install-azurite). While you can also use an actual Azure Storage account, the article assumes you\\'re using this emulator.\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\nUse the `func --version` command to make sure your version of Core Tools is at least `4.0.5530`.\\r\\n\\r\\n## Create and activate a virtual environment\\r\\n\\r\\nIn a suitable folder, run the following commands to create and activate a virtual environment named `.venv`. Make sure that you\\'re using a [version of Python supported by Azure Functions](supported-languages?pivots=programming-language-python#languages-by-runtime-version).\\r\\n\\r\\n**bash**\\r\\n\\r\\n```bash\\r\\npython -m venv .venv\\r\\n```\\r\\n\\r\\n```bash\\r\\nsource .venv/bin/activate\\r\\n```\\r\\n\\r\\nIf Python didn\\'t install the venv package on your Linux distribution, run the following command:\\r\\n\\r\\n```bash\\r\\nsudo apt-get install python3-venv\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n```powershell\\r\\npy -m venv .venv\\r\\n```\\r\\n\\r\\n```powershell\\r\\n.venv\\\\scripts\\\\activate\\r\\n```\\r\\n\\r\\n**Cmd**\\r\\n\\r\\n```cmd\\r\\npy -m venv .venv\\r\\n```\\r\\n\\r\\n```cmd\\r\\n.venv\\\\scripts\\\\activate\\r\\n```\\r\\n\\r\\nYou run all subsequent commands in this activated virtual environment.\\r\\n\\r\\n## Create a local function\\r\\n\\r\\nIn Azure Functions, a function project is a container for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations.\\r\\n\\r\\nIn this section, you create a function project and add an HTTP triggered function.\\r\\n\\r\\n1. Run the [`func init`](functions-core-tools-reference#func-init) command as follows to create a Python v2 functions project in the virtual environment.\\r\\n\\r\\n    ```console\\r\\n    func init --python\\r\\n    ```\\r\\n\\r\\n    The environment now contains various files for the project, including configuration files named [*local.settings.json*](functions-develop-local#local-settings-file) and [*host.json*](functions-host-json). Because *local.settings.json* can contain secrets downloaded from Azure, the file is excluded from source control by default in the *.gitignore* file.\\r\\n2. Add a function to your project by using the following command, where the `--name` argument is the unique name of your function (HttpExample) and the `--template` argument specifies the function\\'s trigger (HTTP).\\r\\n\\r\\n    ```console\\r\\n    func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\"\\r\\n    ```\\r\\n\\r\\n    If prompted, choose the **ANONYMOUS** option. [`func new`](functions-core-tools-reference#func-new) adds an HTTP trigger endpoint named `HttpExample` to the `function_app.py` file, which is accessible without authentication.\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder.\\r\\n\\r\\n    ```console\\r\\n    func start\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following lines must appear:\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown above, you likely started the host from outside the root folder of the project. In that case, use **Ctrl**+**C** to stop the host, go to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your HTTP function from this output to a browser and append the query string `?name=<YOUR_NAME>`, making the full URL like `http://localhost:7071/api/HttpExample?name=Functions`. The browser should display a response message that echoes back your query string value. The terminal in which you started your project also shows log output as you make requests.\\r\\n3. When you\\'re done, press Ctrl + C and type `y` to stop the functions host.\\r\\n\\r\\n## Create supporting Azure resources for your function\\r\\n\\r\\nBefore you can deploy your function code to Azure, you need to create three resources:\\r\\n\\r\\n- A resource group, which is a logical container for related resources.\\r\\n- A storage account, which maintains the state and other information about your projects.\\r\\n- A function app, which provides the environment for executing your function code. A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nUse the following commands to create these items. Both Azure CLI and PowerShell are supported.\\r\\n\\r\\n1. If needed, sign in to Azure.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az login\\r\\n    ```\\r\\n\\r\\nThe [`az login`](/en-us/cli/azure/reference-index#az-login) command signs you into your Azure account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    Connect-AzAccount\\r\\n    ```\\r\\n\\r\\nThe [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet signs you into your Azure account.\\r\\n2. Create a resource group named `AzureFunctionsQuickstart-rg` in your chosen region.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az group create --name AzureFunctionsQuickstart-rg --location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [az group create](/en-us/cli/azure/group#az-group-create) command creates a resource group. In the above command, replace `<REGION>` with a region near you, using an available region code returned from the [az account list-locations](/en-us/cli/azure/account#az-account-list-locations) command.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzResourceGroup -Name AzureFunctionsQuickstart-rg -Location \\'<REGION>\\'\\r\\n    ```\\r\\n\\r\\nThe [New-AzResourceGroup](/en-us/powershell/module/az.resources/new-azresourcegroup) command creates a resource group. You generally create your resource group and resources in a region near you, using an available region returned from the [Get-AzLocation](/en-us/powershell/module/az.resources/get-azlocation) cmdlet.\\r\\n\\r\\n    Note\\r\\n\\r\\n    You can\\'t host Linux and Windows apps in the same resource group. If you have an existing resource group named `AzureFunctionsQuickstart-rg` with a Windows function app or web app, you must use a different resource group.\\r\\n3. Create a general-purpose storage account in your resource group and region.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az storage account create --name <STORAGE_NAME> --location <REGION> --resource-group AzureFunctionsQuickstart-rg --sku Standard_LRS\\r\\n    ```\\r\\n\\r\\nThe [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command creates the storage account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzStorageAccount -ResourceGroupName AzureFunctionsQuickstart-rg -Name <STORAGE_NAME> -SkuName Standard_LRS -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzStorageAccount](/en-us/powershell/module/az.storage/new-azstorageaccount) cmdlet creates the storage account.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with a name that\\'s appropriate to you and unique in Azure Storage. Names must contain 3 to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account [supported by Functions](storage-considerations#storage-account-requirements).\\r\\n\\r\\n    The storage account incurs only a few cents (USD) for this quickstart.\\r\\n4. Create the function app in Azure.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az functionapp create --resource-group AzureFunctionsQuickstart-rg --consumption-plan-location westeurope --runtime python --runtime-version <PYTHON_VERSION> --functions-version 4 --name <APP_NAME> --os-type linux --storage-account <STORAGE_NAME>\\r\\n    ```\\r\\n\\r\\nThe [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command creates the function app in Azure. You must supply `--os-type linux` because Python functions only run on Linux.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzFunctionApp -Name <APP_NAME> -ResourceGroupName AzureFunctionsQuickstart-rg -StorageAccountName <STORAGE_NAME> -FunctionsVersion 4 -RuntimeVersion <PYTHON_VERSION> -Runtime python -Location \\'<REGION>\\'\\r\\n    ```\\r\\n\\r\\nThe [New-AzFunctionApp](/en-us/powershell/module/az.functions/new-azfunctionapp) cmdlet creates the function app in Azure.\\r\\n\\r\\n    In the previous example, replace `<APP_NAME>` with a globally unique name appropriate to you. The `<APP_NAME>` is also the default subdomain for the function app. Make sure that the value you set for `<PYTHON_VERSION>` is a [version supported by Functions](supported-languages#languages-by-runtime-version) and is the same version you used during local development.\\r\\n\\r\\n    This command creates a function app running in your specified language runtime under the [Azure Functions Consumption Plan](consumption-plan), which is free for the amount of usage you incur here. The command also creates an associated Azure Application Insights instance in the same resource group, with which you can monitor your function app and view logs. For more information, see [Monitor Azure Functions](functions-monitoring). The instance incurs no costs until you activate it.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nAfter you\\'ve successfully created your function app in Azure, you\\'re now ready to deploy your local functions project by using the [`func azure functionapp publish`](functions-run-local#project-file-deployment) command.\\r\\n\\r\\nIn your root project folder, run this [`func azure functionapp publish`](functions-core-tools-reference#func-azure-functionapp-publish) command:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp publish <APP_NAME>\\r\\n```\\r\\n\\r\\nIn this example, replace `<APP_NAME>` with the name of your app. A successful deployment shows results similar to the following output (truncated for simplicity):\\r\\n\\r\\n```\\r\\n...\\r\\n\\r\\nGetting site publishing info...\\r\\nCreating archive for current directory...\\r\\nPerforming remote build for functions project.\\r\\n\\r\\n...\\r\\n\\r\\nDeployment successful.\\r\\nRemote build succeeded!\\r\\nSyncing triggers...\\r\\nFunctions in msdocs-azurefunctions-qs:\\r\\n    HttpExample - [httpTrigger]\\r\\n        Invoke url: https://msdocs-azurefunctions-qs.azurewebsites.net/api/httpexample\\r\\n```\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the `publish` command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL** shown in the output of the `publish` command, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you continue to the next step and add an Azure Storage queue output binding, keep all your resources in place as you\\'ll build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, use the following command to delete the resource group and all its contained resources to avoid incurring further costs.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsQuickstart-rg\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n```azurepowershell\\r\\nRemove-AzResourceGroup -Name AzureFunctionsQuickstart-rg\\r\\n```',\n",
       "  'metadata': {}},\n",
       " {'_id': '7906f149-afe0-bb3b-2ca7-904bd5ab256b',\n",
       "  'title': 'Create a TypeScript function from the command line - Azure Functions',\n",
       "  'text': '# Quickstart: Create a TypeScript function in Azure from the command line (nodejs-model-v3)\\r\\n\\r\\nIn this article, you use command-line tools to create a TypeScript function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe content of this article changes based on your choice of the Node.js programming model in the selector at the top of the page. The v4 model is generally available and is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](functions-node-upgrade-v4).\\r\\n\\r\\nCompletion of this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [Visual Studio Code-based version](create-first-function-vs-code-typescript) of this article.\\r\\n\\r\\n## Configure your local environment\\r\\n\\r\\nBefore you begin, you must have the following prerequisites:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- One of the following tools for creating Azure resources:\\r\\n\\r\\n    - [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later.\\r\\n    - The Azure [Az PowerShell module](/en-us/powershell/azure/install-azure-powershell) version 5.9.0 or later.\\r\\n\\r\\n- [Node.js](https://nodejs.org/) version 14 or above.\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\n## Create a local function project\\r\\n\\r\\nIn Azure Functions, a function project is a container for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations. In this section, you create a function project that contains a single function.\\r\\n\\r\\n1. In a suitable folder, run the [`func init`](functions-core-tools-reference#func-init) command, as follows, to create a TypeScript Node.js v3 project in the current folder:\\r\\n\\r\\n    ```console\\r\\n    func init --typescript --model V3\\r\\n    ```\\r\\n\\r\\n    This folder now contains various files for the project, including configurations files named [local.settings.json](functions-develop-local#local-settings-file) and [host.json](functions-host-json). Because *local.settings.json* can contain secrets downloaded from Azure, the file is excluded from source control by default in the *.gitignore* file.\\r\\n2. Add a function to your project by using the following command, where the `--name` argument is the unique name of your function (HttpExample) and the `--template` argument specifies the function\\'s trigger (HTTP).\\r\\n\\r\\n    ```console\\r\\n    func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\"\\r\\n    ```\\r\\n\\r\\n    [`func new`](functions-core-tools-reference#func-new) creates a subfolder matching the function name that contains a code file appropriate to the project\\'s chosen language and a configuration file named *function.json*.\\r\\n\\r\\n### (Optional) Examine the file contents\\r\\n\\r\\nIf desired, you can skip to Run the function locally and examine the file contents later.\\r\\n\\r\\n#### index.ts\\r\\n\\r\\n*index.ts* exports a function that\\'s triggered according to the configuration in *function.json*.\\r\\n\\r\\n```typescript\\r\\nimport { AzureFunction, Context, HttpRequest } from \"@azure/functions\"\\r\\n\\r\\nconst httpTrigger: AzureFunction = async function (context: Context, req: HttpRequest): Promise<void> {\\r\\n    context.log(\\'HTTP trigger function processed a request.\\');\\r\\n    const name = (req.query.name || (req.body && req.body.name));\\r\\n    const responseMessage = name\\r\\n        ? \"Hello, \" + name + \". This HTTP triggered function executed successfully.\"\\r\\n        : \"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\";\\r\\n\\r\\n    context.res = {\\r\\n        // status: 200, /* Defaults to 200 */\\r\\n        body: responseMessage\\r\\n    };\\r\\n\\r\\n};\\r\\n\\r\\nexport default httpTrigger;\\r\\n```\\r\\n\\r\\nFor an HTTP trigger, the function receives request data in the variable `req` of type **HttpRequest** as defined in *function.json*. The return object, defined as `$return` in *function.json*, is the response.\\r\\n\\r\\n#### function.json\\r\\n\\r\\n*function.json* is a configuration file that defines the input and output `bindings` for the function, including the trigger type.\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"bindings\": [\\r\\n        {\\r\\n            \"authLevel\": \"function\",\\r\\n            \"type\": \"httpTrigger\",\\r\\n            \"direction\": \"in\",\\r\\n            \"name\": \"req\",\\r\\n            \"methods\": [\\r\\n                \"get\",\\r\\n                \"post\"\\r\\n            ]\\r\\n        },\\r\\n        {\\r\\n            \"type\": \"http\",\\r\\n            \"direction\": \"out\",\\r\\n            \"name\": \"res\"\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n```\\r\\n\\r\\nEach binding requires a direction, a type, and a unique name. The HTTP trigger has an input binding of type [`httpTrigger`](functions-bindings-http-webhook-trigger) and output binding of type [`http`](functions-bindings-http-webhook-output).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder:\\r\\n\\r\\n    ```console\\r\\n    npm install\\r\\n    npm start\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following logs should appear:\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown in the logs, you likely started the host from outside the root folder of the project. In that case, use Ctrl+c to stop the host, navigate to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your `HttpExample` function from this output to a browser and append the query string `?name=<your-name>`, making the full URL like `http://localhost:7071/api/HttpExample?name=Functions`. The browser should display a message like `Hello Functions`:\\r\\n\\r\\n    The terminal in which you started your project also shows log output as you make requests.\\r\\n3. When you\\'re ready, use Ctrl+c and choose y to stop the functions host.\\r\\n\\r\\n## Create supporting Azure resources for your function\\r\\n\\r\\nBefore you can deploy your function code to Azure, you need to create three resources:\\r\\n\\r\\n- A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n- A [Storage account](../storage/common/storage-account-create), which is used to maintain state and other information about your functions.\\r\\n- A function app, which provides the environment for executing your function code. A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nUse the following commands to create these items. Both Azure CLI and PowerShell are supported.\\r\\n\\r\\n1. If you haven\\'t done so already, sign in to Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az login\\r\\n    ```\\r\\n\\r\\nThe [az login](/en-us/cli/azure/reference-index#az-login) command signs you into your Azure account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    Connect-AzAccount\\r\\n    ```\\r\\n\\r\\nThe [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet signs you into your Azure account.\\r\\n2. Create a resource group named `AzureFunctionsQuickstart-rg` in your chosen region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az group create --name AzureFunctionsQuickstart-rg --location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [az group create](/en-us/cli/azure/group#az-group-create) command creates a resource group. In the above command, replace `<REGION>` with a region near you, using an available region code returned from the [az account list-locations](/en-us/cli/azure/account#az-account-list-locations) command.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzResourceGroup -Name AzureFunctionsQuickstart-rg -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzResourceGroup](/en-us/powershell/module/az.resources/new-azresourcegroup) command creates a resource group. You generally create your resource group and resources in a region near you, using an available region returned from the [Get-AzLocation](/en-us/powershell/module/az.resources/get-azlocation) cmdlet.\\r\\n3. Create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az storage account create --name <STORAGE_NAME> --location <REGION> --resource-group AzureFunctionsQuickstart-rg --sku Standard_LRS --allow-blob-public-access false\\r\\n    ```\\r\\n\\r\\nThe [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command creates the storage account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzStorageAccount -ResourceGroupName AzureFunctionsQuickstart-rg -Name <STORAGE_NAME> -SkuName Standard_LRS -Location <REGION> -AllowBlobPublicAccess $false\\r\\n    ```\\r\\n\\r\\nThe [New-AzStorageAccount](/en-us/powershell/module/az.storage/new-azstorageaccount) cmdlet creates the storage account.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements).\\r\\n\\r\\nImportant\\r\\n\\r\\nThe storage account is used to store important app data, sometimes including the application code itself. You should limit access from other apps and users to the storage account.\\r\\n\\r\\n1. Create the function app in Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az functionapp create --resource-group AzureFunctionsQuickstart-rg --consumption-plan-location <REGION> --runtime node --runtime-version 18 --functions-version 4 --name <APP_NAME> --storage-account <STORAGE_NAME>\\r\\n    ```\\r\\n\\r\\nThe [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command creates the function app in Azure. It\\'s recommended that you use the latest version of Node.js, which is currently 18. You can specify the version by setting `--runtime-version` to `18`.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzFunctionApp -Name <APP_NAME> -ResourceGroupName AzureFunctionsQuickstart-rg -StorageAccount <STORAGE_NAME> -Runtime node -RuntimeVersion 18 -FunctionsVersion 4 -Location \\'<REGION>\\'\\r\\n    ```\\r\\n\\r\\nThe [New-AzFunctionApp](/en-us/powershell/module/az.functions/new-azfunctionapp) cmdlet creates the function app in Azure. It\\'s recommended that you use the latest version of Node.js, which is currently 18. You can specify the version by setting `--runtime-version` to `18`.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, and replace `<APP_NAME>` with a globally unique name appropriate to you. The `<APP_NAME>` is also the default DNS domain for the function app.\\r\\n\\r\\n    This command creates a function app running in your specified language runtime under the [Azure Functions Consumption Plan](consumption-plan), which is free for the amount of usage you incur here. The command also creates an associated Azure Application Insights instance in the same resource group, with which you can monitor your function app and view logs. For more information, see [Monitor Azure Functions](functions-monitoring). The instance incurs no costs until you activate it.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nBefore you use Core Tools to deploy your project to Azure, you create a production-ready build of JavaScript files from the TypeScript source files.\\r\\n\\r\\n1. Use the following command to prepare your TypeScript project for deployment:\\r\\n\\r\\n    ```console\\r\\n    npm run build\\r\\n    ```\\r\\n2. With the necessary resources in place, you\\'re now ready to deploy your local functions project to the function app in Azure by using the [publish](functions-run-local#project-file-deployment) command. In the following example, replace `<APP_NAME>` with the name of your app.\\r\\n\\r\\n    ```console\\r\\n    func azure functionapp publish <APP_NAME>\\r\\n    ```\\r\\n\\r\\n    If you see the error, \"Can\\'t find app with name ...\", wait a few seconds and try again, as Azure may not have fully initialized the app after the previous `az functionapp create` command.\\r\\n\\r\\n    The publish command shows results similar to the following output (truncated for simplicity):\\r\\n\\r\\n    ```\\r\\n     ...\\r\\n    \\r\\n     Getting site publishing info...\\r\\n     Creating archive for current directory...\\r\\n     Performing remote build for functions project.\\r\\n    \\r\\n     ...\\r\\n    \\r\\n     Deployment successful.\\r\\n     Remote build succeeded!\\r\\n     Syncing triggers...\\r\\n     Functions in msdocs-azurefunctions-qs:\\r\\n         HttpExample - [httpTrigger]\\r\\n             Invoke url: https://msdocs-azurefunctions-qs.azurewebsites.net/api/httpexample?code=KYHrydo4GFe9y0000000qRgRJ8NdLFKpkakGJQfC3izYVidzzDN4gQ==\\r\\n     \\r\\n    ```\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\nRun the following command to view near real-time streaming logs:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp logstream <APP_NAME> \\r\\n```\\r\\n\\r\\nIn a separate terminal window or in the browser, call the remote function again. A verbose log of the function execution in Azure is shown in the terminal.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you continue to the next step and add an Azure Storage queue output binding, keep all your resources in place as you\\'ll build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, use the following command to delete the resource group and all its contained resources to avoid incurring further costs.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsQuickstart-rg\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n```azurepowershell\\r\\nRemove-AzResourceGroup -Name AzureFunctionsQuickstart-rg\\r\\n```\\r\\n\\r\\n# Quickstart: Create a TypeScript function in Azure from the command line (nodejs-model-v4)\\r\\n\\r\\nIn this article, you use command-line tools to create a TypeScript function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe content of this article changes based on your choice of the Node.js programming model in the selector at the top of the page. The v4 model is generally available and is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](functions-node-upgrade-v4).\\r\\n\\r\\nCompletion of this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [Visual Studio Code-based version](create-first-function-vs-code-typescript) of this article.\\r\\n\\r\\n## Configure your local environment\\r\\n\\r\\nBefore you begin, you must have the following prerequisites:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- One of the following tools for creating Azure resources:\\r\\n\\r\\n    - [Azure CLI](/en-us/cli/azure/install-azure-cli) version 2.4 or later.\\r\\n    - The Azure [Az PowerShell module](/en-us/powershell/azure/install-azure-powershell) version 5.9.0 or later.\\r\\n\\r\\n- [Node.js](https://nodejs.org/) version 18 or above.\\r\\n- [TypeScript](https://www.typescriptlang.org/) version 4+.\\r\\n\\r\\n## Install the Azure Functions Core Tools\\r\\n\\r\\nThe recommended way to install Core Tools depends on the operating system of your local development computer.\\r\\n\\r\\n**Windows**\\r\\nThe following steps use a Windows installer (MSI) to install Core Tools v4.x. For more information about other package-based installers, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#windows).\\r\\n\\r\\nDownload and run the Core Tools installer, based on your version of Windows:\\r\\n\\r\\n- [v4.x - Windows 64-bit](https://go.microsoft.com/fwlink/?linkid=2174087) (Recommended. [Visual Studio Code debugging](functions-develop-vs-code#debugging-functions-locally) requires 64-bit.)\\r\\n- [v4.x - Windows 32-bit](https://go.microsoft.com/fwlink/?linkid=2174159)\\r\\n\\r\\nIf you previously used Windows installer (MSI) to install Core Tools on Windows, you should uninstall the old version from Add Remove Programs before installing the latest version.\\r\\n\\r\\n**macOS**\\r\\nThe following steps use Homebrew to install the Core Tools on macOS.\\r\\n\\r\\n1. Install [Homebrew](https://brew.sh/), if it\\'s not already installed.\\r\\n2. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    brew tap azure/functions\\r\\n    brew install azure-functions-core-tools@4\\r\\n    # if upgrading on a machine that has 2.x or 3.x installed:\\r\\n    brew link --overwrite azure-functions-core-tools@4\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\nThe following steps use [APT](https://wiki.debian.org/Apt) to install Core Tools on your Ubuntu/Debian Linux distribution. For other Linux distributions, see the [Core Tools readme](https://github.com/Azure/azure-functions-core-tools/blob/v4.x/README.md#linux).\\r\\n\\r\\n1. Install the Microsoft package repository GPG key, to validate package integrity:\\r\\n\\r\\n    ```bash\\r\\n    curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg\\r\\n    sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg\\r\\n    ```\\r\\n2. Set up the APT source list before doing an APT update.\\r\\n\\r\\n##### Ubuntu\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs 2>/dev/null)-prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n\\r\\n##### Debian\\r\\n\\r\\n    ```bash\\r\\n    sudo sh -c \\'echo \"deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs 2>/dev/null | cut -d\\'.\\' -f 1)/prod $(lsb_release -cs 2>/dev/null) main\" > /etc/apt/sources.list.d/dotnetdev.list\\'\\r\\n    ```\\r\\n3. Check the `/etc/apt/sources.list.d/dotnetdev.list` file for one of the appropriate Linux version strings in the following table:\\r\\n\\r\\n    | Linux distribution | Version |\\r\\n    | --- | --- |\\r\\n    | Debian 12 | `bookworm` |\\r\\n    | Debian 11 | `bullseye` |\\r\\n    | Debian 10 | `buster` |\\r\\n    | Debian 9 | `stretch` |\\r\\n    | Ubuntu 22.04 | `jammy` |\\r\\n    | Ubuntu 20.04 | `focal` |\\r\\n    | Ubuntu 19.04 | `disco` |\\r\\n    | Ubuntu 18.10 | `cosmic` |\\r\\n    | Ubuntu 18.04 | `bionic` |\\r\\n    | Ubuntu 17.04 | `zesty` |\\r\\n    | Ubuntu 16.04/Linux Mint 18 | `xenial` |\\r\\n4. Start the APT source update:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get update\\r\\n    ```\\r\\n5. Install the Core Tools package:\\r\\n\\r\\n    ```bash\\r\\n    sudo apt-get install azure-functions-core-tools-4\\r\\n    ```\\r\\n\\r\\n- Make sure you install version v4.0.5382 of the Core Tools, or a later version.\\r\\n\\r\\n## Create a local function project\\r\\n\\r\\nIn Azure Functions, a function project is a container for one or more individual functions that each responds to a specific trigger. All functions in a project share the same local and hosting configurations. In this section, you create a function project that contains a single function.\\r\\n\\r\\n1. In a suitable folder, run the [`func init`](functions-core-tools-reference#func-init) command, as follows, to create a TypeScript Node.js v4 project in the current folder:\\r\\n\\r\\n    ```console\\r\\n    func init --typescript\\r\\n    ```\\r\\n\\r\\n    This folder now contains various files for the project, including configurations files named *local.settings.json* and *host.json*. Because *local.settings.json* can contain secrets downloaded from Azure, the file is excluded from source control by default in the *.gitignore* file. Required npm packages are also installed in *node\\\\_modules*.\\r\\n2. Add a function to your project by using the following command, where the `--name` argument is the unique name of your function (HttpExample) and the `--template` argument specifies the function\\'s trigger (HTTP).\\r\\n\\r\\n    ```console\\r\\n    func new --name HttpExample --template \"HTTP trigger\" --authlevel \"anonymous\"\\r\\n    ```\\r\\n\\r\\n    [`func new`](functions-core-tools-reference#func-new) creates a file named *HttpExample.ts* in the *src/functions* directory, which contains your function\\'s code.\\r\\n3. Add Azure Storage connection information in *local.settings.json*.\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n        \"Values\": {       \\r\\n            \"AzureWebJobsStorage\": \"<Azure Storage connection information>\",\\r\\n            \"FUNCTIONS_WORKER_RUNTIME\": \"node\"\\r\\n        }\\r\\n    }\\r\\n    ```\\r\\n4. (Optional) If you want to learn more about a particular function, say HTTP trigger, you can run the following command:\\r\\n\\r\\n    ```console\\r\\n    func help httptrigger\\r\\n    ```\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\n1. Run your function by starting the local Azure Functions runtime host from the *LocalFunctionProj* folder:\\r\\n\\r\\n    ```console\\r\\n    npm start\\r\\n    ```\\r\\n\\r\\n    Toward the end of the output, the following logs should appear:\\r\\n\\r\\n    Note\\r\\n\\r\\n    If HttpExample doesn\\'t appear as shown in the logs, you likely started the host from outside the root folder of the project. In that case, use Ctrl+c to stop the host, navigate to the project\\'s root folder, and run the previous command again.\\r\\n2. Copy the URL of your `HttpExample` function from this output to a browser and append the query string `?name=<your-name>`, making the full URL like `http://localhost:7071/api/HttpExample?name=Functions`. The browser should display a message like `Hello Functions`:\\r\\n\\r\\n    The terminal in which you started your project also shows log output as you make requests.\\r\\n3. When you\\'re ready, use Ctrl+c and choose y to stop the functions host.\\r\\n\\r\\n## Create supporting Azure resources for your function\\r\\n\\r\\nBefore you can deploy your function code to Azure, you need to create three resources:\\r\\n\\r\\n- A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n- A [Storage account](../storage/common/storage-account-create), which is used to maintain state and other information about your functions.\\r\\n- A function app, which provides the environment for executing your function code. A function app maps to your local function project and lets you group functions as a logical unit for easier management, deployment, and sharing of resources.\\r\\n\\r\\nUse the following commands to create these items. Both Azure CLI and PowerShell are supported.\\r\\n\\r\\n1. If you haven\\'t done so already, sign in to Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az login\\r\\n    ```\\r\\n\\r\\nThe [az login](/en-us/cli/azure/reference-index#az-login) command signs you into your Azure account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    Connect-AzAccount\\r\\n    ```\\r\\n\\r\\nThe [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet signs you into your Azure account.\\r\\n2. Create a resource group named `AzureFunctionsQuickstart-rg` in your chosen region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az group create --name AzureFunctionsQuickstart-rg --location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [az group create](/en-us/cli/azure/group#az-group-create) command creates a resource group. In the above command, replace `<REGION>` with a region near you, using an available region code returned from the [az account list-locations](/en-us/cli/azure/account#az-account-list-locations) command.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzResourceGroup -Name AzureFunctionsQuickstart-rg -Location <REGION>\\r\\n    ```\\r\\n\\r\\nThe [New-AzResourceGroup](/en-us/powershell/module/az.resources/new-azresourcegroup) command creates a resource group. You generally create your resource group and resources in a region near you, using an available region returned from the [Get-AzLocation](/en-us/powershell/module/az.resources/get-azlocation) cmdlet.\\r\\n3. Create a general-purpose storage account in your resource group and region:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az storage account create --name <STORAGE_NAME> --location <REGION> --resource-group AzureFunctionsQuickstart-rg --sku Standard_LRS --allow-blob-public-access false\\r\\n    ```\\r\\n\\r\\nThe [az storage account create](/en-us/cli/azure/storage/account#az-storage-account-create) command creates the storage account.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzStorageAccount -ResourceGroupName AzureFunctionsQuickstart-rg -Name <STORAGE_NAME> -SkuName Standard_LRS -Location <REGION> -AllowBlobPublicAccess $false\\r\\n    ```\\r\\n\\r\\nThe [New-AzStorageAccount](/en-us/powershell/module/az.storage/new-azstorageaccount) cmdlet creates the storage account.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with a name that is appropriate to you and unique in Azure Storage. Names must contain three to 24 characters numbers and lowercase letters only. `Standard_LRS` specifies a general-purpose account, which is [supported by Functions](storage-considerations#storage-account-requirements).\\r\\n\\r\\nImportant\\r\\n\\r\\nThe storage account is used to store important app data, sometimes including the application code itself. You should limit access from other apps and users to the storage account.\\r\\n\\r\\n1. Create the function app in Azure:\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n    ```azurecli\\r\\n    az functionapp create --resource-group AzureFunctionsQuickstart-rg --consumption-plan-location <REGION> --runtime node --runtime-version 18 --functions-version 4 --name <APP_NAME> --storage-account <STORAGE_NAME>\\r\\n    ```\\r\\n\\r\\nThe [az functionapp create](/en-us/cli/azure/functionapp#az-functionapp-create) command creates the function app in Azure. It\\'s recommended that you use the latest version of Node.js, which is currently 18. You can specify the version by setting `--runtime-version` to `18`.\\r\\n\\r\\n**Azure PowerShell**\\r\\n\\r\\n    ```azurepowershell\\r\\n    New-AzFunctionApp -Name <APP_NAME> -ResourceGroupName AzureFunctionsQuickstart-rg -StorageAccount <STORAGE_NAME> -Runtime node -RuntimeVersion 18 -FunctionsVersion 4 -Location \\'<REGION>\\'\\r\\n    ```\\r\\n\\r\\nThe [New-AzFunctionApp](/en-us/powershell/module/az.functions/new-azfunctionapp) cmdlet creates the function app in Azure. It\\'s recommended that you use the latest version of Node.js, which is currently 18. You can specify the version by setting `--runtime-version` to `18`.\\r\\n\\r\\n    In the previous example, replace `<STORAGE_NAME>` with the name of the account you used in the previous step, and replace `<APP_NAME>` with a globally unique name appropriate to you. The `<APP_NAME>` is also the default DNS domain for the function app.\\r\\n\\r\\n    This command creates a function app running in your specified language runtime under the [Azure Functions Consumption Plan](consumption-plan), which is free for the amount of usage you incur here. The command also creates an associated Azure Application Insights instance in the same resource group, with which you can monitor your function app and view logs. For more information, see [Monitor Azure Functions](functions-monitoring). The instance incurs no costs until you activate it.\\r\\n\\r\\n## Deploy the function project to Azure\\r\\n\\r\\nBefore you use Core Tools to deploy your project to Azure, you create a production-ready build of JavaScript files from the TypeScript source files.\\r\\n\\r\\n1. Use the following command to prepare your TypeScript project for deployment:\\r\\n\\r\\n    ```console\\r\\n    npm run build\\r\\n    ```\\r\\n2. With the necessary resources in place, you\\'re now ready to deploy your local functions project to the function app in Azure by using the [publish](functions-run-local#project-file-deployment) command. In the following example, replace `<APP_NAME>` with the name of your app.\\r\\n\\r\\n    ```console\\r\\n    func azure functionapp publish <APP_NAME>\\r\\n    ```\\r\\n\\r\\n    If you see the error, \"Can\\'t find app with name ...\", wait a few seconds and try again, as Azure may not have fully initialized the app after the previous `az functionapp create` command.\\r\\n\\r\\n    The publish command shows results similar to the following output (truncated for simplicity):\\r\\n\\r\\n    ```\\r\\n     ...\\r\\n    \\r\\n     Getting site publishing info...\\r\\n     Creating archive for current directory...\\r\\n     Performing remote build for functions project.\\r\\n    \\r\\n     ...\\r\\n    \\r\\n     Deployment successful.\\r\\n     Remote build succeeded!\\r\\n     Syncing triggers...\\r\\n     Functions in msdocs-azurefunctions-qs:\\r\\n         HttpExample - [httpTrigger]\\r\\n             Invoke url: https://msdocs-azurefunctions-qs.azurewebsites.net/api/httpexample?code=KYHrydo4GFe9y0000000qRgRJ8NdLFKpkakGJQfC3izYVidzzDN4gQ==\\r\\n     \\r\\n    ```\\r\\n\\r\\n## Invoke the function on Azure\\r\\n\\r\\nBecause your function uses an HTTP trigger, you invoke it by making an HTTP request to its URL in the browser or with a tool like curl.\\r\\n\\r\\n**Browser**\\r\\nCopy the complete **Invoke URL** shown in the output of the publish command into a browser address bar, appending the query parameter `?name=Functions`. The browser should display similar output as when you ran the function locally.\\r\\n\\r\\n**curl**\\r\\nRun [`curl`](https://curl.haxx.se/) with the **Invoke URL**, appending the parameter `?name=Functions`. The output of the command should be the text, \"Hello Functions.\"\\r\\n\\r\\nRun the following command to view near real-time streaming logs:\\r\\n\\r\\n```console\\r\\nfunc azure functionapp logstream <APP_NAME> \\r\\n```\\r\\n\\r\\nIn a separate terminal window or in the browser, call the remote function again. A verbose log of the function execution in Azure is shown in the terminal.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIf you continue to the next step and add an Azure Storage queue output binding, keep all your resources in place as you\\'ll build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, use the following command to delete the resource group and all its contained resources to avoid incurring further costs.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz group delete --name AzureFunctionsQuickstart',\n",
       "  'metadata': {}},\n",
       " {'_id': '62f699a7-275f-8332-26f7-9cc416e34b5b',\n",
       "  'title': 'Create a C# function using Visual Studio Code - Azure Functions',\n",
       "  'text': '# Quickstart: Create a C# function in Azure using Visual Studio Code\\r\\n\\r\\nThis article creates an HTTP triggered function that runs on .NET 8 in an isolated worker process. For information about .NET versions supported for C# functions, see [Supported versions](dotnet-isolated-process-guide#supported-versions).\\r\\n\\r\\nThere\\'s also a [CLI-based version](create-first-function-cli-csharp) of this article.\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThis video shows you how to create a C# function in Azure using VS Code.\\r\\n\\r\\nThe steps in the video are also described in the following sections.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you get started, make sure you have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [.NET 8.0 SDK](https://dotnet.microsoft.com/download/dotnet/8.0).\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- [C# extension](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp) for Visual Studio Code.\\r\\n- [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) for Visual Studio Code.\\r\\n\\r\\n## Install or update Core Tools\\r\\n\\r\\nThe Azure Functions extension for Visual Studio Code integrates with Azure Functions Core Tools so that you can run and debug your functions locally in Visual Studio Code using the Azure Functions runtime. Before getting started, it\\'s a good idea to install Core Tools locally or update an existing installation to use the latest version.\\r\\n\\r\\nIn Visual Studio Code, select F1 to open the command palette, and then search for and run the command **Azure Functions: Install or Update Core Tools**.\\r\\n\\r\\nThis command tries to either start a package-based installation of the latest version of Core Tools or update an existing package-based installation. If you don\\'t have npm or Homebrew installed on your local computer, you must instead [manually install or update Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions project in C#. Later in this article, you\\'ll publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Select the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n3. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language for your function project** | Choose `C#`. |\\r\\n    | **Select a .NET runtime** | Choose `.NET 8.0 Isolated (LTS)`. |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`.^1^ |\\r\\n    | **Provide a function name** | Type `HttpExample`. |\\r\\n    | **Provide a namespace** | Type `My.Functions`. |\\r\\n    | **Authorization level** | Choose `Anonymous`, which enables anyone to call your function endpoint. For more information, see [Authorization level](functions-bindings-http-webhook-trigger#http-auth). |\\r\\n    | **Select how you would like to open your project** | Select `Open in current window`. |\\r\\n\\r\\n    ^1^ Depending on your VS Code settings, you may need to use the `Change template filter` option to see the full list of templates.\\r\\n4. Visual Studio Code uses the provided information and generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer. For more information about the files that are created, see [Generated project files](functions-develop-vs-code?tabs=csharp#generated-project-files).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nVisual Studio Code integrates with [Azure Functions Core tools](functions-run-local) to let you run this project on your local development computer before you publish to Azure. If you don\\'t already have Core Tools installed locally, you are prompted to install it the first time you run your project.\\r\\n\\r\\n1. To call your function, press F5 to start the function app project. The **Terminal** panel displays the output from Core Tools. Your app starts in the **Terminal** panel. You can see the URL endpoint of your HTTP-triggered function running locally.\\r\\n\\r\\n    If you don\\'t already have Core Tools installed, select **Install** to install Core Tools when prompted to do so. If you have trouble running on Windows, make sure that the default terminal for Visual Studio Code isn\\'t set to **WSL Bash**.\\r\\n2. With the Core Tools running, go to the **Azure: Functions** area. Under **Functions**, expand **Local Project** &gt; **Functions**. Right-click (Windows) or Ctrl - click (macOS) the `HttpExample` function and choose **Execute Function Now...**.\\r\\n3. In the **Enter request body**, press Enter to send a request message to your function.\\r\\n4. When the function executes locally and returns a response, a notification is raised in Visual Studio Code. Information about the function execution is shown in the **Terminal** panel.\\r\\n5. Press Ctrl + C to stop Core Tools and disconnect the debugger.\\r\\n\\r\\nAfter checking that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription. Many of the resource creation decisions are made for you based on default behaviors. For more control over the created resources, you must instead [create your function app with advanced options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n1. In Visual Studio Code, select F1 to open the command palette. At the prompt (`>`), enter and then select **Azure Functions: Create Function App in Azure**.\\r\\n2. At the prompts, provide the following information:\\r\\n\\r\\n    | Prompt | Action |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Select the Azure subscription to use. The prompt doesn\\'t appear when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Enter a name that is valid in a URL path. The name you enter is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Select the language version you currently run locally. |\\r\\n    | **Select a location for new resources** | Select an Azure region. For better performance, select a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    In the **Azure: Activity Log** panel, the Azure extension shows the status of individual resources as they\\'re created in Azure.\\r\\n3. When the function app is created, the following related resources are created in your Azure subscription. The resources are named based on the name you entered for your function app.\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette. In the command palette, search for and select `Azure: Open in portal`.\\r\\n2. Choose your function app and press Enter. The function app page opens in the Azure portal.\\r\\n3. In the **Overview** tab, select the named link next to **Resource group**.\\r\\n4. On the **Resource group** page, review the list of included resources, and verify that they\\'re the ones you want to delete.\\r\\n5. Select **Delete resource group**, and follow the instructions.\\r\\n\\r\\n    Deletion may take a couple of minutes. When it\\'s done, a notification appears for a few seconds. You can also select the bell icon at the top of the page to view the notification.\\r\\n\\r\\nFor more information about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).',\n",
       "  'metadata': {}},\n",
       " {'_id': 'ca22e32f-d4b2-b2fa-e696-a7c9370a2ccb',\n",
       "  'title': 'Create a Java function using Visual Studio Code - Azure Functions',\n",
       "  'text': '# Quickstart: Create a Java function in Azure using Visual Studio Code\\r\\n\\r\\n- [C#](create-first-function-vs-code-csharp)\\r\\n- [Java](create-first-function-vs-code-java)\\r\\n- [JavaScript](create-first-function-vs-code-node)\\r\\n- [PowerShell](create-first-function-vs-code-powershell)\\r\\n- [Python](create-first-function-vs-code-python)\\r\\n- [TypeScript](create-first-function-vs-code-typescript)\\r\\n- [Other (Go/Rust)](create-first-function-vs-code-other)\\r\\n\\r\\nIn this article, you use Visual Studio Code to create a Java function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nIf Visual Studio Code isn\\'t your preferred development tool, check out our similar tutorials for Java developers:\\r\\n\\r\\n- [Gradle](functions-create-first-java-gradle)\\r\\n- [IntelliJ IDEA](/en-us/azure/developer/java/toolkit-for-intellij/quickstart-functions)\\r\\n- [Maven](create-first-function-cli-java)\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you get started, make sure you have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- The [Java Development Kit](/en-us/azure/developer/java/fundamentals/java-support-on-azure), version 8, 11, 17 or 21(Linux).\\r\\n- [Apache Maven](https://maven.apache.org), version 3.0 or above.\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [Java extension pack](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-pack)\\r\\n- The [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) for Visual Studio Code.\\r\\n\\r\\n## Install or update Core Tools\\r\\n\\r\\nThe Azure Functions extension for Visual Studio Code integrates with Azure Functions Core Tools so that you can run and debug your functions locally in Visual Studio Code using the Azure Functions runtime. Before getting started, it\\'s a good idea to install Core Tools locally or update an existing installation to use the latest version.\\r\\n\\r\\nIn Visual Studio Code, select F1 to open the command palette, and then search for and run the command **Azure Functions: Install or Update Core Tools**.\\r\\n\\r\\nThis command tries to either start a package-based installation of the latest version of Core Tools or update an existing package-based installation. If you don\\'t have npm or Homebrew installed on your local computer, you must instead [manually install or update Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions project in Java. Later in this article, you\\'ll publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Choose the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n3. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language** | Choose `Java`. |\\r\\n    | **Select a version of Java** | Choose `Java 8`, `Java 11`, `Java 17` or `Java 21`, the Java version on which your functions run in Azure. Choose a Java version that you\\'ve verified locally. |\\r\\n    | **Provide a group ID** | Choose `com.function`. |\\r\\n    | **Provide an artifact ID** | Choose `myFunction`. |\\r\\n    | **Provide a version** | Choose `1.0-SNAPSHOT`. |\\r\\n    | **Provide a package name** | Choose `com.function`. |\\r\\n    | **Provide an app name** | Choose `myFunction-12345`. |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`. |\\r\\n    | **Select the build tool for Java project** | Choose `Maven`. |\\r\\n    | **Provide a function name** | Enter `HttpExample`. |\\r\\n    | **Authorization level** | Choose `Anonymous`, which lets anyone call your function endpoint. For more information, see [Authorization level](functions-bindings-http-webhook-trigger#http-auth). |\\r\\n    | **Select how you would like to open your project** | Choose `Open in current window`. |\\r\\n4. Visual Studio Code uses the provided information and generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer. For more information about the files that are created, see [Generated project files](functions-develop-vs-code?tabs=java#generated-project-files).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nVisual Studio Code integrates with [Azure Functions Core tools](functions-run-local) to let you run this project on your local development computer before you publish to Azure.\\r\\n\\r\\n1. To start the function locally, press F5 or the **Run and Debug** icon in the left-hand side Activity bar. The **Terminal** panel displays the Output from Core Tools. Your app starts in the **Terminal** panel. You can see the URL endpoint of your HTTP-triggered function running locally.\\r\\n\\r\\n    If you have trouble running on Windows, make sure that the default terminal for Visual Studio Code isn\\'t set to **WSL Bash**.\\r\\n2. With Core Tools still running in **Terminal**, choose the Azure icon in the activity bar. In the **Workspace** area, expand **Local Project** &gt; **Functions**. Right-click (Windows) or Ctrl - click (macOS) the new function and choose **Execute Function Now...**.\\r\\n3. In **Enter request body** you see the request message body value of `{ \"name\": \"Azure\" }`. Press Enter to send this request message to your function.\\r\\n4. When the function executes locally and returns a response, a notification is raised in Visual Studio Code. Information about the function execution is shown in **Terminal** panel.\\r\\n5. With the **Terminal** panel focused, press Ctrl + C to stop Core Tools and disconnect the debugger.\\r\\n\\r\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription. Many of the resource creation decisions are made for you based on default behaviors. For more control over the created resources, you must instead [create your function app with advanced options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n1. In Visual Studio Code, select F1 to open the command palette. At the prompt (`>`), enter and then select **Azure Functions: Create Function App in Azure**.\\r\\n2. At the prompts, provide the following information:\\r\\n\\r\\n    | Prompt | Action |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Select the Azure subscription to use. The prompt doesn\\'t appear when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Enter a name that is valid in a URL path. The name you enter is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Select the language version you currently run locally. |\\r\\n    | **Select a location for new resources** | Select an Azure region. For better performance, select a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    In the **Azure: Activity Log** panel, the Azure extension shows the status of individual resources as they\\'re created in Azure.\\r\\n3. When the function app is created, the following related resources are created in your Azure subscription. The resources are named based on the name you entered for your function app.\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette. In the command palette, search for and select `Azure: Open in portal`.\\r\\n2. Choose your function app and press Enter. The function app page opens in the Azure portal.\\r\\n3. In the **Overview** tab, select the named link next to **Resource group**.\\r\\n4. On the **Resource group** page, review the list of included resources, and verify that they\\'re the ones you want to delete.\\r\\n5. Select **Delete resource group**, and follow the instructions.\\r\\n\\r\\n    Deletion may take a couple of minutes. When it\\'s done, a notification appears for a few seconds. You can also select the bell icon at the top of the page to view the notification.\\r\\n\\r\\nFor more information about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).',\n",
       "  'metadata': {}},\n",
       " {'_id': '7ecc8797-7e20-13bc-fb1d-4a4916272ca0',\n",
       "  'title': 'Create a JavaScript function using Visual Studio Code - Azure Functions',\n",
       "  'text': '# Quickstart: Create a JavaScript function in Azure using Visual Studio Code (nodejs-model-v3)\\r\\n\\r\\nUse Visual Studio Code to create a JavaScript function that responds to HTTP requests. Test the code locally, then deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe content of this article changes based on your choice of the Node.js programming model in the selector at the top of the page. The v4 model is generally available and is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](functions-node-upgrade-v4).\\r\\n\\r\\nCompletion of this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [CLI-based version](create-first-function-cli-node) of this article.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you get started, make sure you have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [Node.js 14.x](https://nodejs.org/en/about/previous-releases) or above. Use the `node --version` command to check your version.\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) for Visual Studio Code. This extension installs [Azure Functions Core Tools](functions-run-local) for you the first time you locally run your functions.\\r\\n\\r\\n## Install or update Core Tools\\r\\n\\r\\nThe Azure Functions extension for Visual Studio Code integrates with Azure Functions Core Tools so that you can run and debug your functions locally in Visual Studio Code using the Azure Functions runtime. Before getting started, it\\'s a good idea to install Core Tools locally or update an existing installation to use the latest version.\\r\\n\\r\\nIn Visual Studio Code, select F1 to open the command palette, and then search for and run the command **Azure Functions: Install or Update Core Tools**.\\r\\n\\r\\nThis command tries to either start a package-based installation of the latest version of Core Tools or update an existing package-based installation. If you don\\'t have npm or Homebrew installed on your local computer, you must instead [manually install or update Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions project in JavaScript. Later in this article, you publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Choose the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n\\r\\n1. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language for your function project** | Choose `JavaScript`. |\\r\\n    | **Select a JavaScript programming model** | Choose `Model V3` |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`. |\\r\\n    | **Provide a function name** | Type `HttpExample`. |\\r\\n    | **Authorization level** | Choose `Anonymous`, which enables anyone to call your function endpoint. For more information, see [Authorization level](functions-bindings-http-webhook-trigger#http-auth). |\\r\\n    | **Select how you would like to open your project** | Choose `Open in current window`. |\\r\\n\\r\\n    Using this information, Visual Studio Code generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer. To learn more about files that are created, see [Generated project files](functions-develop-vs-code?tabs=javascript#generated-project-files).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nVisual Studio Code integrates with [Azure Functions Core tools](functions-run-local) to let you run this project on your local development computer before you publish to Azure.\\r\\n\\r\\n1. To start the function locally, press F5 or the **Run and Debug** icon in the left-hand side Activity bar. The **Terminal** panel displays the Output from Core Tools. Your app starts in the **Terminal** panel. You can see the URL endpoint of your HTTP-triggered function running locally.\\r\\n\\r\\n    If you have trouble running on Windows, make sure that the default terminal for Visual Studio Code isn\\'t set to **WSL Bash**.\\r\\n2. With Core Tools still running in **Terminal**, choose the Azure icon in the activity bar. In the **Workspace** area, expand **Local Project** &gt; **Functions**. Right-click (Windows) or Ctrl - click (macOS) the new function and choose **Execute Function Now...**.\\r\\n3. In **Enter request body** you see the request message body value of `{ \"name\": \"Azure\" }`. Press Enter to send this request message to your function.\\r\\n4. When the function executes locally and returns a response, a notification is raised in Visual Studio Code. Information about the function execution is shown in **Terminal** panel.\\r\\n5. With the **Terminal** panel focused, press Ctrl + C to stop Core Tools and disconnect the debugger.\\r\\n\\r\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription. Many of the resource creation decisions are made for you based on default behaviors. For more control over the created resources, you must instead [create your function app with advanced options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n1. In Visual Studio Code, select F1 to open the command palette. At the prompt (`>`), enter and then select **Azure Functions: Create Function App in Azure**.\\r\\n2. At the prompts, provide the following information:\\r\\n\\r\\n    | Prompt | Action |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Select the Azure subscription to use. The prompt doesn\\'t appear when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Enter a name that is valid in a URL path. The name you enter is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Select the language version you currently run locally. |\\r\\n    | **Select a location for new resources** | Select an Azure region. For better performance, select a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    In the **Azure: Activity Log** panel, the Azure extension shows the status of individual resources as they\\'re created in Azure.\\r\\n3. When the function app is created, the following related resources are created in your Azure subscription. The resources are named based on the name you entered for your function app.\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Change the code and redeploy to Azure\\r\\n\\r\\n1. In Visual Studio Code in the Explorer view, select the `./HttpExample/index.js` file.\\r\\n2. Replace the file with the following code to construct a JSON object and return it.\\r\\n\\r\\n    ```javascript\\r\\n    module.exports = async function (context, req) {\\r\\n    \\r\\n        try {\\r\\n            context.log(\\'JavaScript HTTP trigger function processed a request.\\');\\r\\n    \\r\\n            // Read incoming data\\r\\n            const name = (req.query.name || (req.body && req.body.name));\\r\\n            const sport = (req.query.sport || (req.body && req.body.sport));\\r\\n    \\r\\n            // fail if incoming data is required\\r\\n            if (!name || !sport) {\\r\\n    \\r\\n                context.res = {\\r\\n                    status: 400\\r\\n                };\\r\\n                return;\\r\\n            }\\r\\n    \\r\\n            // Add or change code here\\r\\n            const message = `${name} likes ${sport}`;\\r\\n    \\r\\n            // Construct response\\r\\n            const responseJSON = {\\r\\n                \"name\": name,\\r\\n                \"sport\": sport,\\r\\n                \"message\": message,\\r\\n                \"success\": true\\r\\n            }\\r\\n    \\r\\n            context.res = {\\r\\n                // status: 200, /* Defaults to 200 */\\r\\n                body: responseJSON,\\r\\n                contentType: \\'application/json\\'\\r\\n            };\\r\\n        } catch(err) {\\r\\n            context.res = {\\r\\n                status: 500\\r\\n            };\\r\\n        }\\r\\n    }\\r\\n    ```\\r\\n3. Rerun the function app locally.\\r\\n4. In the prompt **Enter request body**, change the request message body to { \"name\": \"Tom\",\"sport\":\"basketball\" }. Press Enter to send this request message to your function.\\r\\n5. View the response in the notification:\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n      \"name\": \"Tom\",\\r\\n      \"sport\": \"basketball\",\\r\\n      \"message\": \"Tom likes basketball\",\\r\\n      \"success\": true\\r\\n    }\\r\\n    ```\\r\\n6. Redeploy the function to Azure.\\r\\n\\r\\n## Troubleshooting\\r\\n\\r\\nUse the following table to resolve the most common issues encountered when using this quickstart.\\r\\n\\r\\n| Problem | Solution |\\r\\n| --- | --- |\\r\\n| Can\\'t create a local function project? | Make sure you have the [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) installed. |\\r\\n| Can\\'t run the function locally? | Make sure you have the latest version of [Azure Functions Core Tools installed](functions-run-local?tabs=node) installed. When running on Windows, make sure that the default terminal shell for Visual Studio Code isn\\'t set to WSL Bash. |\\r\\n| Can\\'t deploy function to Azure? | Review the Output for error information. The bell icon in the lower right corner is another way to view the output. Did you publish to an existing function app? That action overwrites the content of that app in Azure. |\\r\\n| Couldn\\'t run the cloud-based Function app? | Remember to use the query string to send in parameters. |\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, select the Azure icon to open the Azure explorer.\\r\\n2. In the Resource Groups section, find your resource group.\\r\\n3. Right-click the resource group and select **Delete**.\\r\\n\\r\\nTo learn more about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).\\r\\n\\r\\n# Quickstart: Create a JavaScript function in Azure using Visual Studio Code (nodejs-model-v4)\\r\\n\\r\\nUse Visual Studio Code to create a JavaScript function that responds to HTTP requests. Test the code locally, then deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe content of this article changes based on your choice of the Node.js programming model in the selector at the top of the page. The v4 model is generally available and is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](functions-node-upgrade-v4).\\r\\n\\r\\nCompletion of this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [CLI-based version](create-first-function-cli-node) of this article.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you get started, make sure you have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [Node.js 18.x](https://nodejs.org/en/about/previous-releases) or above. Use the `node --version` command to check your version.\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [Azure Functions extension v1.10.4](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) or above for Visual Studio Code. This extension installs [Azure Functions Core Tools](functions-run-local) for you the first time you locally run your functions. Node.js v4 requires version 4.0.5382, or a later version of Core Tools.\\r\\n\\r\\n## Install or update Core Tools\\r\\n\\r\\nThe Azure Functions extension for Visual Studio Code integrates with Azure Functions Core Tools so that you can run and debug your functions locally in Visual Studio Code using the Azure Functions runtime. Before getting started, it\\'s a good idea to install Core Tools locally or update an existing installation to use the latest version.\\r\\n\\r\\nIn Visual Studio Code, select F1 to open the command palette, and then search for and run the command **Azure Functions: Install or Update Core Tools**.\\r\\n\\r\\nThis command tries to either start a package-based installation of the latest version of Core Tools or update an existing package-based installation. If you don\\'t have npm or Homebrew installed on your local computer, you must instead [manually install or update Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions project in JavaScript. Later in this article, you publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Choose the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n\\r\\n1. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language for your function project** | Choose `JavaScript`. |\\r\\n    | **Select a JavaScript programming model** | Choose `Model V4` |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`. |\\r\\n    | **Provide a function name** | Type `HttpExample`. |\\r\\n    | **Select how you would like to open your project** | Choose `Open in current window` |\\r\\n\\r\\n    Using this information, Visual Studio Code generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer. To learn more about files that are created, see [Azure Functions JavaScript developer guide](functions-reference-node?tabs=javascript).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nVisual Studio Code integrates with [Azure Functions Core tools](functions-run-local) to let you run this project on your local development computer before you publish to Azure.\\r\\n\\r\\n1. To start the function locally, press F5 or the **Run and Debug** icon in the left-hand side Activity bar. The **Terminal** panel displays the Output from Core Tools. Your app starts in the **Terminal** panel. You can see the URL endpoint of your HTTP-triggered function running locally.\\r\\n\\r\\n    If you have trouble running on Windows, make sure that the default terminal for Visual Studio Code isn\\'t set to **WSL Bash**.\\r\\n2. With Core Tools still running in **Terminal**, choose the Azure icon in the activity bar. In the **Workspace** area, expand **Local Project** &gt; **Functions**. Right-click (Windows) or Ctrl - click (macOS) the new function and choose **Execute Function Now...**.\\r\\n3. In **Enter request body** you see the request message body value of `{ \"name\": \"Azure\" }`. Press Enter to send this request message to your function.\\r\\n4. When the function executes locally and returns a response, a notification is raised in Visual Studio Code. Information about the function execution is shown in **Terminal** panel.\\r\\n5. With the **Terminal** panel focused, press Ctrl + C to stop Core Tools and disconnect the debugger.\\r\\n\\r\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription. Many of the resource creation decisions are made for you based on default behaviors. For more control over the created resources, you must instead [create your function app with advanced options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n1. In Visual Studio Code, select F1 to open the command palette. At the prompt (`>`), enter and then select **Azure Functions: Create Function App in Azure**.\\r\\n2. At the prompts, provide the following information:\\r\\n\\r\\n    | Prompt | Action |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Select the Azure subscription to use. The prompt doesn\\'t appear when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Enter a name that is valid in a URL path. The name you enter is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Select the language version you currently run locally. |\\r\\n    | **Select a location for new resources** | Select an Azure region. For better performance, select a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    In the **Azure: Activity Log** panel, the Azure extension shows the status of individual resources as they\\'re created in Azure.\\r\\n3. When the function app is created, the following related resources are created in your Azure subscription. The resources are named based on the name you entered for your function app.\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Troubleshooting\\r\\n\\r\\nUse the following table to resolve the most common issues encountered when using this quickstart.\\r\\n\\r\\n| Problem | Solution |\\r\\n| --- | --- |\\r\\n| Can\\'t create a local function project? | Make sure you have the [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) installed. |\\r\\n| Can\\'t run the function locally? | Make sure you have the latest version of [Azure Functions Core Tools installed](functions-run-local?tabs=node) installed. When running on Windows, make sure that the default terminal shell for Visual Studio Code isn\\'t set to WSL Bash. |\\r\\n| Can\\'t deploy function to Azure? | Review the Output for error information. The bell icon in the lower right corner is another way to view the output. Did you publish to an existing function app? That action overwrites the content of that app in Azure. |\\r\\n| Couldn\\'t run the cloud-based Function app? | Remember to use the query string to send in parameters. |\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, select the Azure icon to open the Azure explorer.\\r\\n2. In the Resource Groups section, find your resource group.\\r\\n3. Right-click the resource group and select **Delete**.\\r\\n\\r\\nTo learn more about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).',\n",
       "  'metadata': {}},\n",
       " {'_id': '6a9e27d5-103d-84fe-c7f1-6d7afbc52fd6',\n",
       "  'title': 'Create a function in Go or Rust using Visual Studio Code - Azure Functions',\n",
       "  'text': '# Quickstart: Create a Go or Rust function in Azure using Visual Studio Code\\r\\n\\r\\n- [C#](create-first-function-vs-code-csharp)\\r\\n- [Java](create-first-function-vs-code-java)\\r\\n- [JavaScript](create-first-function-vs-code-node)\\r\\n- [PowerShell](create-first-function-vs-code-powershell)\\r\\n- [Python](create-first-function-vs-code-python)\\r\\n- [TypeScript](create-first-function-vs-code-typescript)\\r\\n- [Other (Go/Rust)](create-first-function-vs-code-other)\\r\\n\\r\\nIn this article, you use Visual Studio Code to create a [custom handler](functions-custom-handlers) function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nCustom handlers can be used to create functions in any language or runtime by running an HTTP server process. This article supports both [Go](create-first-function-vs-code-other?tabs=go) and [Rust](create-first-function-vs-code-other?tabs=rust).\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you get started, make sure you have the following requirements in place:\\r\\n\\r\\n**Go**\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) for Visual Studio Code.\\r\\n- [Go](https://go.dev/doc/install), latest version recommended. Use the `go version` command to check your version.\\r\\n\\r\\n**Rust**\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) for Visual Studio Code.\\r\\n- Rust toolchain using [rustup](https://www.rust-lang.org/tools/install). Use the `rustc --version` command to check your version.\\r\\n\\r\\n## Install or update Core Tools\\r\\n\\r\\nThe Azure Functions extension for Visual Studio Code integrates with Azure Functions Core Tools so that you can run and debug your functions locally in Visual Studio Code using the Azure Functions runtime. Before getting started, it\\'s a good idea to install Core Tools locally or update an existing installation to use the latest version.\\r\\n\\r\\nIn Visual Studio Code, select F1 to open the command palette, and then search for and run the command **Azure Functions: Install or Update Core Tools**.\\r\\n\\r\\nThis command tries to either start a package-based installation of the latest version of Core Tools or update an existing package-based installation. If you don\\'t have npm or Homebrew installed on your local computer, you must instead [manually install or update Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions custom handlers project. Later in this article, you\\'ll publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Choose the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n3. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language for your function project** | Choose `Custom Handler`. |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`. |\\r\\n    | **Provide a function name** | Type `HttpExample`. |\\r\\n    | **Authorization level** | Choose `Anonymous`, which enables anyone to call your function endpoint. For more information, see [Authorization level](functions-bindings-http-webhook-trigger#http-auth). |\\r\\n    | **Select how you would like to open your project** | Choose `Open in current window`. |\\r\\n\\r\\n    Using this information, Visual Studio Code generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer.\\r\\n\\r\\n## Create and build your function\\r\\n\\r\\nThe *function.json* file in the *HttpExample* folder declares an HTTP trigger function. You complete the function by adding a handler and compiling it into an executable.\\r\\n\\r\\n**Go**\\r\\n1. Press Ctrl + N (Cmd + N on macOS) to create a new file. Save it as *handler.go* in the function app root (in the same folder as *host.json*).\\r\\n2. In *handler.go*, add the following code and save the file. This is your Go custom handler.\\r\\n\\r\\n    ```go\\r\\n    package main\\r\\n    \\r\\n    import (\\r\\n        \"fmt\"\\r\\n        \"log\"\\r\\n        \"net/http\"\\r\\n        \"os\"\\r\\n    )\\r\\n    \\r\\n    func helloHandler(w http.ResponseWriter, r *http.Request) {\\r\\n        message := \"This HTTP triggered function executed successfully. Pass a name in the query string for a personalized response.\\\\n\"\\r\\n        name := r.URL.Query().Get(\"name\")\\r\\n        if name != \"\" {\\r\\n            message = fmt.Sprintf(\"Hello, %s. This HTTP triggered function executed successfully.\\\\n\", name)\\r\\n        }\\r\\n        fmt.Fprint(w, message)\\r\\n    }\\r\\n    \\r\\n    func main() {\\r\\n        listenAddr := \":8080\"\\r\\n        if val, ok := os.LookupEnv(\"FUNCTIONS_CUSTOMHANDLER_PORT\"); ok {\\r\\n            listenAddr = \":\" + val\\r\\n        }\\r\\n        http.HandleFunc(\"/api/HttpExample\", helloHandler)\\r\\n        log.Printf(\"About to listen on %s. Go to https://127.0.0.1%s/\", listenAddr, listenAddr)\\r\\n        log.Fatal(http.ListenAndServe(listenAddr, nil))\\r\\n    }\\r\\n    ```\\r\\n3. Press Ctrl + Shift + ` or select *New Terminal* from the *Terminal* menu to open a new integrated terminal in VS Code.\\r\\n4. Compile your custom handler using the following command. An executable file named `handler` (`handler.exe` on Windows) is output in the function app root folder.\\r\\n\\r\\n    ```bash\\r\\n    go build handler.go\\r\\n    ```\\r\\n\\r\\n**Rust**\\r\\n1. Press Ctrl + Shift + ` or select *New Terminal* from the *Terminal* menu to open a new integrated terminal in VS Code.\\r\\n2. In the function app root (the same folder as *host.json*), initialize a Rust project named `handler`.\\r\\n\\r\\n    ```bash\\r\\n    cargo init --name handler\\r\\n    ```\\r\\n3. In *Cargo.toml*, add the following dependencies necessary to complete this quickstart. The example uses the [warp](https://docs.rs/warp/) web server framework.\\r\\n\\r\\n    ```toml\\r\\n    [dependencies]\\r\\n    warp = \"0.3\"\\r\\n    tokio = { version = \"1\", features = [\"rt\", \"macros\", \"rt-multi-thread\"] }\\r\\n    ```\\r\\n4. In *src/main.rs*, add the following code and save the file. This is your Rust custom handler.\\r\\n\\r\\n    ```rust\\r\\n    use std::collections::HashMap;\\r\\n    use std::env;\\r\\n    use std::net::Ipv4Addr;\\r\\n    use warp::{http::Response, Filter};\\r\\n    \\r\\n    #[tokio::main]\\r\\n    async fn main() {\\r\\n        let example1 = warp::get()\\r\\n            .and(warp::path(\"api\"))\\r\\n            .and(warp::path(\"HttpExample\"))\\r\\n            .and(warp::query::<HashMap<String, String>>())\\r\\n            .map(|p: HashMap<String, String>| match p.get(\"name\") {\\r\\n                Some(name) => Response::builder().body(format!(\"Hello, {}. This HTTP triggered function executed successfully.\", name)),\\r\\n                None => Response::builder().body(String::from(\"This HTTP triggered function executed successfully. Pass a name in the query string for a personalized response.\")),\\r\\n            });\\r\\n    \\r\\n        let port_key = \"FUNCTIONS_CUSTOMHANDLER_PORT\";\\r\\n        let port: u16 = match env::var(port_key) {\\r\\n            Ok(val) => val.parse().expect(\"Custom Handler port is not a number!\"),\\r\\n            Err(_) => 3000,\\r\\n        };\\r\\n    \\r\\n        warp::serve(example1).run((Ipv4Addr::LOCALHOST, port)).await\\r\\n    }\\r\\n    ```\\r\\n5. Compile a binary for your custom handler. An executable file named `handler` (`handler.exe` on Windows) is output in the function app root folder.\\r\\n\\r\\n    ```bash\\r\\n    cargo build --release\\r\\n    cp target/release/handler .\\r\\n    ```\\r\\n\\r\\n## Configure your function app\\r\\n\\r\\nThe function host needs to be configured to run your custom handler binary when it starts.\\r\\n\\r\\n1. Open *host.json*.\\r\\n2. In the `customHandler.description` section, set the value of `defaultExecutablePath` to `handler` (on Windows, set it to `handler.exe`).\\r\\n3. In the `customHandler` section, add a property named `enableForwardingHttpRequest` and set its value to `true`. For functions consisting of only an HTTP trigger, this setting simplifies programming by allow you to work with a typical HTTP request instead of the custom handler [request payload](functions-custom-handlers#request-payload).\\r\\n4. Confirm the `customHandler` section looks like this example. Save the file.\\r\\n\\r\\n    ```\\r\\n    \"customHandler\": {\\r\\n      \"description\": {\\r\\n        \"defaultExecutablePath\": \"handler\",\\r\\n        \"workingDirectory\": \"\",\\r\\n        \"arguments\": []\\r\\n      },\\r\\n      \"enableForwardingHttpRequest\": true\\r\\n    }\\r\\n    ```\\r\\n\\r\\nThe function app is configured to start your custom handler executable.\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nYou can run this project on your local development computer before you publish to Azure.\\r\\n\\r\\n1. In the integrated terminal, start the function app using Azure Functions Core Tools.\\r\\n\\r\\n    ```bash\\r\\n    func start\\r\\n    ```\\r\\n2. With Core Tools running, navigate to the following URL to execute a GET request, which includes `?name=Functions` query string.\\r\\n\\r\\n    `http://localhost:7071/api/HttpExample?name=Functions`\\r\\n3. A response is returned, which looks like the following in a browser:\\r\\n4. Information about the request is shown in **Terminal** panel.\\r\\n5. Press Ctrl + C to stop Core Tools.\\r\\n\\r\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Compile the custom handler for Azure\\r\\n\\r\\nIn this section, you publish your project to Azure in a function app running Linux. In most cases, you must recompile your binary and adjust your configuration to match the target platform before publishing it to Azure.\\r\\n\\r\\n**Go**\\r\\n1. In the integrated terminal, compile the handler to Linux/x64. A binary named `handler` is created in the function app root.\\r\\n\\r\\n**macOS**\\r\\n\\r\\n    ```bash\\r\\n    GOOS=linux GOARCH=amd64 go build handler.go\\r\\n    ```\\r\\n\\r\\n**Linux**\\r\\n\\r\\n    ```bash\\r\\n    GOOS=linux GOARCH=amd64 go build handler.go\\r\\n    ```\\r\\n\\r\\n**Windows**\\r\\n\\r\\n    ```cmd\\r\\n    set GOOS=linux\\r\\n    set GOARCH=amd64\\r\\n    go build handler.go\\r\\n    ```\\r\\n\\r\\nChange the `defaultExecutablePath` in *host.json* from `handler.exe` to `handler`. This instructs the function app to run the Linux binary.\\r\\n\\r\\n**Rust**\\r\\n1. Create a file at *.cargo/config*. Add the following contents and save the file.\\r\\n\\r\\n    ```\\r\\n    [target.x86_64-unknown-linux-musl]\\r\\n    linker = \"rust-lld\"\\r\\n    ```\\r\\n2. In the integrated terminal, compile the handler to Linux/x64. A binary named `handler` is created. Copy it to the function app root.\\r\\n\\r\\n    ```bash\\r\\n    rustup target add x86_64-unknown-linux-musl\\r\\n    cargo build --release --target=x86_64-unknown-linux-musl\\r\\n    cp target/x86_64-unknown-linux-musl/release/handler .\\r\\n    ```\\r\\n3. If you are using Windows, change the `defaultExecutablePath` in *host.json* from `handler.exe` to `handler`. This instructs the function app to run the Linux binary.\\r\\n4. Add the following line to the *.funcignore* file:\\r\\n\\r\\n    ```\\r\\n    target\\r\\n    ```\\r\\n\\r\\n    This prevents publishing the contents of the *target* folder.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription.\\r\\n\\r\\n1. Choose the Azure icon in the Activity bar. Then in the **Resources** area, select the **+** icon and choose the **Create Function App in Azure** option.\\r\\n2. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Choose the subscription to use. You won\\'t see this when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Type a name that is valid in a URL path. The name you type is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Choose **Custom Handler**. |\\r\\n    | **Select a location for new resources** | For better performance, choose a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    The extension shows the status of individual resources as they are being created in Azure in the **Azure: Activity Log** panel.\\r\\n3. When the creation is complete, the following Azure resources are created in your subscription. The resources are named based on your function app name:\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette. In the command palette, search for and select `Azure: Open in portal`.\\r\\n2. Choose your function app and press Enter. The function app page opens in the Azure portal.\\r\\n3. In the **Overview** tab, select the named link next to **Resource group**.\\r\\n4. On the **Resource group** page, review the list of included resources, and verify that they\\'re the ones you want to delete.\\r\\n5. Select **Delete resource group**, and follow the instructions.\\r\\n\\r\\n    Deletion may take a couple of minutes. When it\\'s done, a notification appears for a few seconds. You can also select the bell icon at the top of the page to view the notification.\\r\\n\\r\\nFor more information about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).',\n",
       "  'metadata': {}},\n",
       " {'_id': 'dadbda4a-e801-f99e-18b1-e1d4dd111c86',\n",
       "  'title': 'Create a PowerShell function using Visual Studio Code - Azure Functions',\n",
       "  'text': '# Quickstart: Create a PowerShell function in Azure using Visual Studio Code\\r\\n\\r\\n- [C#](create-first-function-vs-code-csharp)\\r\\n- [Java](create-first-function-vs-code-java)\\r\\n- [JavaScript](create-first-function-vs-code-node)\\r\\n- [PowerShell](create-first-function-vs-code-powershell)\\r\\n- [Python](create-first-function-vs-code-python)\\r\\n- [TypeScript](create-first-function-vs-code-typescript)\\r\\n- [Other (Go/Rust)](create-first-function-vs-code-other)\\r\\n\\r\\nIn this article, you use Visual Studio Code to create a PowerShell function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [CLI-based version](create-first-function-cli-powershell) of this article.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you get started, make sure you have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- [PowerShell 7.2](/en-us/powershell/scripting/install/installing-powershell-core-on-windows)\\r\\n- [.NET 6.0 runtime](https://dotnet.microsoft.com/download/dotnet)\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [PowerShell extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-vscode.PowerShell).\\r\\n- The [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) for Visual Studio Code.\\r\\n\\r\\n## Install or update Core Tools\\r\\n\\r\\nThe Azure Functions extension for Visual Studio Code integrates with Azure Functions Core Tools so that you can run and debug your functions locally in Visual Studio Code using the Azure Functions runtime. Before getting started, it\\'s a good idea to install Core Tools locally or update an existing installation to use the latest version.\\r\\n\\r\\nIn Visual Studio Code, select F1 to open the command palette, and then search for and run the command **Azure Functions: Install or Update Core Tools**.\\r\\n\\r\\nThis command tries to either start a package-based installation of the latest version of Core Tools or update an existing package-based installation. If you don\\'t have npm or Homebrew installed on your local computer, you must instead [manually install or update Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions project in PowerShell. Later in this article, you\\'ll publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Choose the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n3. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language for your function project** | Choose `PowerShell`. |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`. |\\r\\n    | **Provide a function name** | Type `HttpExample`. |\\r\\n    | **Authorization level** | Choose `Anonymous`, which enables anyone to call your function endpoint. For more information, see [Authorization level](functions-bindings-http-webhook-trigger#http-auth). |\\r\\n    | **Select how you would like to open your project** | Choose `Open in current window`. |\\r\\n\\r\\n    Using this information, Visual Studio Code generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer. To learn more about files that are created, see [Generated project files](functions-develop-vs-code?tabs=powershell#generated-project-files).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nVisual Studio Code integrates with [Azure Functions Core tools](functions-run-local) to let you run this project on your local development computer before you publish to Azure.\\r\\n\\r\\n1. To start the function locally, press F5 or the **Run and Debug** icon in the left-hand side Activity bar. The **Terminal** panel displays the Output from Core Tools. Your app starts in the **Terminal** panel. You can see the URL endpoint of your HTTP-triggered function running locally.\\r\\n\\r\\n    If you have trouble running on Windows, make sure that the default terminal for Visual Studio Code isn\\'t set to **WSL Bash**.\\r\\n2. With Core Tools still running in **Terminal**, choose the Azure icon in the activity bar. In the **Workspace** area, expand **Local Project** &gt; **Functions**. Right-click (Windows) or Ctrl - click (macOS) the new function and choose **Execute Function Now...**.\\r\\n3. In **Enter request body** you see the request message body value of `{ \"name\": \"Azure\" }`. Press Enter to send this request message to your function.\\r\\n4. When the function executes locally and returns a response, a notification is raised in Visual Studio Code. Information about the function execution is shown in **Terminal** panel.\\r\\n5. With the **Terminal** panel focused, press Ctrl + C to stop Core Tools and disconnect the debugger.\\r\\n\\r\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription. Many of the resource creation decisions are made for you based on default behaviors. For more control over the created resources, you must instead [create your function app with advanced options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n1. In Visual Studio Code, select F1 to open the command palette. At the prompt (`>`), enter and then select **Azure Functions: Create Function App in Azure**.\\r\\n2. At the prompts, provide the following information:\\r\\n\\r\\n    | Prompt | Action |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Select the Azure subscription to use. The prompt doesn\\'t appear when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Enter a name that is valid in a URL path. The name you enter is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Select the language version you currently run locally. |\\r\\n    | **Select a location for new resources** | Select an Azure region. For better performance, select a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    In the **Azure: Activity Log** panel, the Azure extension shows the status of individual resources as they\\'re created in Azure.\\r\\n3. When the function app is created, the following related resources are created in your Azure subscription. The resources are named based on the name you entered for your function app.\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette. In the command palette, search for and select `Azure: Open in portal`.\\r\\n2. Choose your function app and press Enter. The function app page opens in the Azure portal.\\r\\n3. In the **Overview** tab, select the named link next to **Resource group**.\\r\\n4. On the **Resource group** page, review the list of included resources, and verify that they\\'re the ones you want to delete.\\r\\n5. Select **Delete resource group**, and follow the instructions.\\r\\n\\r\\n    Deletion may take a couple of minutes. When it\\'s done, a notification appears for a few seconds. You can also select the bell icon at the top of the page to view the notification.\\r\\n\\r\\nFor more information about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).',\n",
       "  'metadata': {}},\n",
       " {'_id': '13162222-7bce-8277-d7be-dd92fa4eed96',\n",
       "  'title': 'Create a Python function using Visual Studio Code - Azure Functions',\n",
       "  'text': '# Quickstart: Create a function in Azure with Python using Visual Studio Code\\r\\n\\r\\nIn this article, you use Visual Studio Code to create a Python function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nThis article uses the Python v2 programming model for Azure Functions, which provides a decorator-based approach for creating functions. To learn more about the Python v2 programming model, see the [Developer Reference Guide](functions-reference-python?pivots=python-mode-decorators)\\r\\n\\r\\nCompleting this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [CLI-based version](create-first-function-cli-python) of this article.\\r\\n\\r\\nThis video shows you how to create a Python function in Azure using Visual Studio Code.\\r\\n\\r\\nThe steps in the video are also described in the following sections.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you begin, make sure that you have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n- A Python version that is [supported by Azure Functions](supported-languages#languages-by-runtime-version). For more information, see [How to install Python](https://wiki.python.org/moin/BeginnersGuide/Download).\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python) for Visual Studio Code.\\r\\n- The [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) for Visual Studio Code, version 1.8.1 or later.\\r\\n- The [Azurite V3 extension](https://marketplace.visualstudio.com/items?itemName=Azurite.azurite) local storage emulator. While you can also use an actual Azure storage account, this article assumes you\\'re using the Azurite emulator.\\r\\n\\r\\n## Install or update Core Tools\\r\\n\\r\\nThe Azure Functions extension for Visual Studio Code integrates with Azure Functions Core Tools so that you can run and debug your functions locally in Visual Studio Code using the Azure Functions runtime. Before getting started, it\\'s a good idea to install Core Tools locally or update an existing installation to use the latest version.\\r\\n\\r\\nIn Visual Studio Code, select F1 to open the command palette, and then search for and run the command **Azure Functions: Install or Update Core Tools**.\\r\\n\\r\\nThis command tries to either start a package-based installation of the latest version of Core Tools or update an existing package-based installation. If you don\\'t have npm or Homebrew installed on your local computer, you must instead [manually install or update Core Tools](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions project in Python. Later in this article, you publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Choose the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n3. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language** | Choose `Python (Programming Model V2)`. |\\r\\n    | **Select a Python interpreter to create a virtual environment** | Choose your preferred Python interpreter. If an option isn\\'t shown, type in the full path to your Python binary. |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`. |\\r\\n    | **Name of the function you want to create** | Enter `HttpExample`. |\\r\\n    | **Authorization level** | Choose `ANONYMOUS`, which lets anyone call your function endpoint. For more information, see [Authorization level](functions-bindings-http-webhook-trigger#http-auth). |\\r\\n    | **Select how you would like to open your project** | Choose `Open in current window`. |\\r\\n4. Visual Studio Code uses the provided information and generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer. The generated `function_app.py` project file contains your functions.\\r\\n5. In the local.settings.json file, update the `AzureWebJobsStorage` setting as in the following example:\\r\\n\\r\\n    ```json\\r\\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\\r\\n    ```\\r\\n\\r\\n    This tells the local Functions host to use the storage emulator for the storage connection required by the Python v2 model. When you publish your project to Azure, this setting uses the default storage account instead. If you\\'re using an Azure Storage account during local development, set your storage account connection string here.\\r\\n\\r\\n## Start the emulator\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette. In the command palette, search for and select `Azurite: Start`.\\r\\n2. Check the bottom bar and verify that Azurite emulation services are running. If so, you can now run your function locally.\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nVisual Studio Code integrates with [Azure Functions Core tools](functions-run-local) to let you run this project on your local development computer before you publish to Azure.\\r\\n\\r\\n1. To start the function locally, press F5 or the **Run and Debug** icon in the left-hand side Activity bar. The **Terminal** panel displays the Output from Core Tools. Your app starts in the **Terminal** panel. You can see the URL endpoint of your HTTP-triggered function running locally.\\r\\n\\r\\n    If you have trouble running on Windows, make sure that the default terminal for Visual Studio Code isn\\'t set to **WSL Bash**.\\r\\n2. With Core Tools still running in **Terminal**, choose the Azure icon in the activity bar. In the **Workspace** area, expand **Local Project** &gt; **Functions**. Right-click (Windows) or Ctrl - click (macOS) the new function and choose **Execute Function Now...**.\\r\\n3. In **Enter request body** you see the request message body value of `{ \"name\": \"Azure\" }`. Press Enter to send this request message to your function.\\r\\n4. When the function executes locally and returns a response, a notification is raised in Visual Studio Code. Information about the function execution is shown in **Terminal** panel.\\r\\n5. With the **Terminal** panel focused, press Ctrl + C to stop Core Tools and disconnect the debugger.\\r\\n\\r\\nAfter you verify that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription. Many of the resource creation decisions are made for you based on default behaviors. For more control over the created resources, you must instead [create your function app with advanced options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n1. In Visual Studio Code, select F1 to open the command palette. At the prompt (`>`), enter and then select **Azure Functions: Create Function App in Azure**.\\r\\n2. At the prompts, provide the following information:\\r\\n\\r\\n    | Prompt | Action |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Select the Azure subscription to use. The prompt doesn\\'t appear when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Enter a name that is valid in a URL path. The name you enter is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Select the language version you currently run locally. |\\r\\n    | **Select a location for new resources** | Select an Azure region. For better performance, select a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    In the **Azure: Activity Log** panel, the Azure extension shows the status of individual resources as they\\'re created in Azure.\\r\\n3. When the function app is created, the following related resources are created in your Azure subscription. The resources are named based on the name you entered for your function app.\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette. In the command palette, search for and select `Azure: Open in portal`.\\r\\n2. Choose your function app and press Enter. The function app page opens in the Azure portal.\\r\\n3. In the **Overview** tab, select the named link next to **Resource group**.\\r\\n4. On the **Resource group** page, review the list of included resources, and verify that they\\'re the ones you want to delete.\\r\\n5. Select **Delete resource group**, and follow the instructions.\\r\\n\\r\\n    Deletion may take a couple of minutes. When it\\'s done, a notification appears for a few seconds. You can also select the bell icon at the top of the page to view the notification.\\r\\n\\r\\nFor more information about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).',\n",
       "  'metadata': {}},\n",
       " {'_id': '6fd9324e-dde3-8a06-12da-fc8bef66bbd6',\n",
       "  'title': 'Create a TypeScript function using Visual Studio Code - Azure Functions',\n",
       "  'text': '# Quickstart: Create a function in Azure with TypeScript using Visual Studio Code (nodejs-model-v3)\\r\\n\\r\\nIn this article, you use Visual Studio Code to create a TypeScript function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe content of this article changes based on your choice of the Node.js programming model in the selector at the top of the page. The v4 model is generally available and is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](functions-node-upgrade-v4).\\r\\n\\r\\nCompletion of this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [CLI-based version](create-first-function-cli-typescript) of this article.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you get started, make sure you have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n\\r\\n- [Node.js 18.x](https://nodejs.org/en/about/previous-releases) or [Node.js 16.x](https://nodejs.org/en/about/previous-releases). Use the `node --version` command to check your version.\\r\\n\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [Azure Functions extension v1.10.4](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) or above for Visual Studio Code.\\r\\n\\r\\n- [Azure Functions Core Tools 4.x](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions project in TypeScript. Later in this article, you publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Choose the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n\\r\\n1. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language for your function project** | Choose `TypeScript`. |\\r\\n    | **Select a TypeScript programming model** | Choose `Model V3` |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`. |\\r\\n    | **Provide a function name** | Type `HttpExample`. |\\r\\n    | **Authorization level** | Choose `Anonymous`, which enables anyone to call your function endpoint. For more information, see [Authorization level](functions-bindings-http-webhook-trigger#http-auth). |\\r\\n    | **Select how you would like to open your project** | Choose `Open in current window`. |\\r\\n\\r\\n    Using this information, Visual Studio Code generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer. To learn more about files that are created, see [Generated project files](functions-develop-vs-code?tabs=typescript#generated-project-files).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nVisual Studio Code integrates with [Azure Functions Core tools](functions-run-local) to let you run this project on your local development computer before you publish to Azure.\\r\\n\\r\\n1. To start the function locally, press F5 or the **Run and Debug** icon in the left-hand side Activity bar. The **Terminal** panel displays the Output from Core Tools. Your app starts in the **Terminal** panel. You can see the URL endpoint of your HTTP-triggered function running locally.\\r\\n\\r\\n    If you have trouble running on Windows, make sure that the default terminal for Visual Studio Code isn\\'t set to **WSL Bash**.\\r\\n2. With Core Tools still running in **Terminal**, choose the Azure icon in the activity bar. In the **Workspace** area, expand **Local Project** &gt; **Functions**. Right-click (Windows) or Ctrl - click (macOS) the new function and choose **Execute Function Now...**.\\r\\n3. In **Enter request body** you see the request message body value of `{ \"name\": \"Azure\" }`. Press Enter to send this request message to your function.\\r\\n4. When the function executes locally and returns a response, a notification is raised in Visual Studio Code. Information about the function execution is shown in **Terminal** panel.\\r\\n5. With the **Terminal** panel focused, press Ctrl + C to stop Core Tools and disconnect the debugger.\\r\\n\\r\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription. Many of the resource creation decisions are made for you based on default behaviors. For more control over the created resources, you must instead [create your function app with advanced options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n1. In Visual Studio Code, select F1 to open the command palette. At the prompt (`>`), enter and then select **Azure Functions: Create Function App in Azure**.\\r\\n2. At the prompts, provide the following information:\\r\\n\\r\\n    | Prompt | Action |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Select the Azure subscription to use. The prompt doesn\\'t appear when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Enter a name that is valid in a URL path. The name you enter is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Select the language version you currently run locally. |\\r\\n    | **Select a location for new resources** | Select an Azure region. For better performance, select a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    In the **Azure: Activity Log** panel, the Azure extension shows the status of individual resources as they\\'re created in Azure.\\r\\n3. When the function app is created, the following related resources are created in your Azure subscription. The resources are named based on the name you entered for your function app.\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette. In the command palette, search for and select `Azure: Open in portal`.\\r\\n2. Choose your function app and press Enter. The function app page opens in the Azure portal.\\r\\n3. In the **Overview** tab, select the named link next to **Resource group**.\\r\\n4. On the **Resource group** page, review the list of included resources, and verify that they\\'re the ones you want to delete.\\r\\n5. Select **Delete resource group**, and follow the instructions.\\r\\n\\r\\n    Deletion may take a couple of minutes. When it\\'s done, a notification appears for a few seconds. You can also select the bell icon at the top of the page to view the notification.\\r\\n\\r\\nFor more information about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).\\r\\n\\r\\n# Quickstart: Create a function in Azure with TypeScript using Visual Studio Code (nodejs-model-v4)\\r\\n\\r\\nIn this article, you use Visual Studio Code to create a TypeScript function that responds to HTTP requests. After testing the code locally, you deploy it to the serverless environment of Azure Functions.\\r\\n\\r\\nImportant\\r\\n\\r\\nThe content of this article changes based on your choice of the Node.js programming model in the selector at the top of the page. The v4 model is generally available and is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](functions-node-upgrade-v4).\\r\\n\\r\\nCompletion of this quickstart incurs a small cost of a few USD cents or less in your Azure account.\\r\\n\\r\\nThere\\'s also a [CLI-based version](create-first-function-cli-typescript) of this article.\\r\\n\\r\\n## Configure your environment\\r\\n\\r\\nBefore you get started, make sure you have the following requirements in place:\\r\\n\\r\\n- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio).\\r\\n\\r\\n- [Node.js 18.x](https://nodejs.org/en/about/previous-releases) or above. Use the `node --version` command to check your version.\\r\\n- [TypeScript 4.x](https://www.typescriptlang.org/). Use the `tsc -v` command to check your version.\\r\\n\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n- The [Azure Functions extension v1.10.4](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) or above for Visual Studio Code.\\r\\n\\r\\n- [Azure Functions Core Tools v4.0.5382 or above](functions-run-local#install-the-azure-functions-core-tools).\\r\\n\\r\\n## Create your local project\\r\\n\\r\\nIn this section, you use Visual Studio Code to create a local Azure Functions project in TypeScript. Later in this article, you publish your function code to Azure.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette and search for and run the command `Azure Functions: Create New Project...`.\\r\\n2. Choose the directory location for your project workspace and choose **Select**. You should either create a new folder or choose an empty folder for the project workspace. Don\\'t choose a project folder that is already part of a workspace.\\r\\n\\r\\n1. Provide the following information at the prompts:\\r\\n\\r\\n    | Prompt | Selection |\\r\\n    | --- | --- |\\r\\n    | **Select a language for your function project** | Choose `TypeScript`. |\\r\\n    | **Select a TypeScript programming model** | Choose `Model V4` |\\r\\n    | **Select a template for your project\\'s first function** | Choose `HTTP trigger`. |\\r\\n    | **Provide a function name** | Type `HttpExample`. |\\r\\n    | **Select how you would like to open your project** | Choose `Open in current window` |\\r\\n\\r\\n    Using this information, Visual Studio Code generates an Azure Functions project with an HTTP trigger. You can view the local project files in the Explorer. To learn more about files that are created, see [Azure Functions TypeScript developer guide](functions-reference-node?tabs=typescript).\\r\\n\\r\\n## Run the function locally\\r\\n\\r\\nVisual Studio Code integrates with [Azure Functions Core tools](functions-run-local) to let you run this project on your local development computer before you publish to Azure.\\r\\n\\r\\n1. To start the function locally, press F5 or the **Run and Debug** icon in the left-hand side Activity bar. The **Terminal** panel displays the Output from Core Tools. Your app starts in the **Terminal** panel. You can see the URL endpoint of your HTTP-triggered function running locally.\\r\\n\\r\\n    If you have trouble running on Windows, make sure that the default terminal for Visual Studio Code isn\\'t set to **WSL Bash**.\\r\\n2. With Core Tools still running in **Terminal**, choose the Azure icon in the activity bar. In the **Workspace** area, expand **Local Project** &gt; **Functions**. Right-click (Windows) or Ctrl - click (macOS) the new function and choose **Execute Function Now...**.\\r\\n3. In **Enter request body** you see the request message body value of `{ \"name\": \"Azure\" }`. Press Enter to send this request message to your function.\\r\\n4. When the function executes locally and returns a response, a notification is raised in Visual Studio Code. Information about the function execution is shown in **Terminal** panel.\\r\\n5. With the **Terminal** panel focused, press Ctrl + C to stop Core Tools and disconnect the debugger.\\r\\n\\r\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to use Visual Studio Code to publish the project directly to Azure.\\r\\n\\r\\n## Sign in to Azure\\r\\n\\r\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\r\\n\\r\\n1. If you aren\\'t already signed in, in the **Activity bar**, select the Azure icon. Then under **Resources**, select **Sign in to Azure**.\\r\\n\\r\\n    If you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, select **Create an Azure Account**. Students can select **Create an Azure for Students Account**.\\r\\n2. When you are prompted in the browser, select your Azure account and sign in by using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\r\\n3. After you successfully sign in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the side bar.\\r\\n\\r\\n## Create the function app in Azure\\r\\n\\r\\nIn this section, you create a function app and related resources in your Azure subscription. Many of the resource creation decisions are made for you based on default behaviors. For more control over the created resources, you must instead [create your function app with advanced options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n1. In Visual Studio Code, select F1 to open the command palette. At the prompt (`>`), enter and then select **Azure Functions: Create Function App in Azure**.\\r\\n2. At the prompts, provide the following information:\\r\\n\\r\\n    | Prompt | Action |\\r\\n    | --- | --- |\\r\\n    | **Select subscription** | Select the Azure subscription to use. The prompt doesn\\'t appear when you have only one subscription visible under **Resources**. |\\r\\n    | **Enter a globally unique name for the function app** | Enter a name that is valid in a URL path. The name you enter is validated to make sure that it\\'s unique in Azure Functions. |\\r\\n    | **Select a runtime stack** | Select the language version you currently run locally. |\\r\\n    | **Select a location for new resources** | Select an Azure region. For better performance, select a [region](https://azure.microsoft.com/regions/) near you. |\\r\\n\\r\\n    In the **Azure: Activity Log** panel, the Azure extension shows the status of individual resources as they\\'re created in Azure.\\r\\n3. When the function app is created, the following related resources are created in your Azure subscription. The resources are named based on the name you entered for your function app.\\r\\n\\r\\n    - A [resource group](../azure-resource-manager/management/overview), which is a logical container for related resources.\\r\\n    - A standard [Azure Storage account](../storage/common/storage-account-create), which maintains state and other information about your projects.\\r\\n    - A function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\r\\n    - An Azure App Service plan, which defines the underlying host for your function app.\\r\\n    - An Application Insights instance that\\'s connected to the function app, and which tracks the use of your functions in the app.\\r\\n\\r\\n    A notification is displayed after your function app is created and the deployment package is applied.\\r\\n\\r\\n    Tip\\r\\n\\r\\n    By default, the Azure resources required by your function app are created based on the name you enter for your function app. By default, the resources are created with the function app in the same, new resource group. If you want to customize the names of the associated resources or reuse existing resources, [publish the project with advanced create options](functions-develop-vs-code?tabs=advanced-options#publish-to-azure).\\r\\n\\r\\n## Deploy the project to Azure\\r\\n\\r\\nImportant\\r\\n\\r\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\r\\n\\r\\n1. In the command palette, enter and then select **Azure Functions: Deploy to Function App**.\\r\\n2. Select the function app you just created. When prompted about overwriting previous deployments, select **Deploy** to deploy your function code to the new function app resource.\\r\\n3. When deployment is completed, select **View Output** to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower-right corner to see it again.\\r\\n\\r\\n## Run the function in Azure\\r\\n\\r\\n1. Press F1 to display the command palette, then search for and run the command `Azure Functions:Execute Function Now...`. If prompted, select your subscription.\\r\\n2. Select your new function app resource and `HttpExample` as your function.\\r\\n3. In **Enter request body** type `{ \"name\": \"Azure\" }`, then press Enter to send this request message to your function.\\r\\n4. When the function executes in Azure, the response is displayed in the notification area. Expand the notification to review the full response.\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nWhen you continue to the next step and add an Azure Storage queue binding to your function, you\\'ll need to keep all your resources in place to build on what you\\'ve already done.\\r\\n\\r\\nOtherwise, you can use the following steps to delete the function app and its related resources to avoid incurring any further costs.\\r\\n\\r\\n1. In Visual Studio Code, press F1 to open the command palette. In the command palette, search for and select `Azure: Open in portal`.\\r\\n2. Choose your function app and press Enter. The function app page opens in the Azure portal.\\r\\n3. In the **Overview** tab, select the named link next to **Resource group**.\\r\\n4. On the **Resource group** page, review the list of included resources, and verify that they\\'re the ones you want to delete.\\r\\n5. Select **Delete resource group**, and follow the instructions.\\r\\n\\r\\n    Deletion may take a couple of minutes. When it\\'s done, a notification appears for a few seconds. You can also select the bell icon at the top of the page to view the notification.\\r\\n\\r\\nFor more information about Functions costs, see [Estimating Consumption plan costs](functions-consumption-costs).',\n",
       "  'metadata': {}},\n",
       " {'_id': 'f0614e62-3aa4-f45b-310f-8221decd4898',\n",
       "  'title': 'Create your first function using Visual Studio Code for the Web',\n",
       "  'text': \"Create your first function using Visual Studio Code for the Web\\nThis quickstart creates an HTTP triggered function using Visual Studio Code for the Web in the Azure portal. The function app that hosts your code runs in the Flex Consumption hosting plan in Azure Functions.\\nImportant\\nThe Flex Consumption plan is currently in preview.\\nImportant\\nVisual Studio Code for the Web in the Azure portal is currently only supported for Node.js, PowerShell, and Python apps hosted in the Flex Consumption plan, which is currently in preview. For C# and Java apps, you should instead complete Create a Flex Consumption app.\\nPrerequisites\\nIf you don't have an Azure subscription, create an Azure free account before you begin.\\nSign in to Azure\\nSign in to the Azure portal with your Azure account.\\nCreate a function app\\nYou must have a function app to host the execution of your functions in the Flex Consumption plan. A function app lets you group functions as a logical unit for easier management, deployment, scaling, and sharing of resources.\\nUse these steps to create your function app and related Azure resources.\\nIn the Azure portal, from the menu or the Home page, select Create a resource.\\nSelect Get started and then Create under Function App.\\nUnder Select a hosting option, choose Flex Consumption > Select.\\nOn the Basics page, use the function app settings as specified in the following table:\\nSetting\\nSuggested value\\nDescription\\nSubscription\\nYour subscription\\nThe subscription in which you create your new function app.\\nResource Group\\nmyResourceGroup\\nName for the new resource group in which you create your function app.\\nFunction App name\\nGlobally unique name\\nName that identifies your new function app. Valid characters are a-z (case insensitive), 0-9, and -.\\nRegion\\nPreferred region\\nSelect a region that's near you or near other services that your functions can access. Unsupported regions aren't displayed. For more information, see View currently supported regions.\\nRuntime stack\\nPreferred language\\nChoose one of the supported language runtime stacks. In-portal editing using Visual Studio Code for the Web is currently only available for Node.js, PowerShell, and Python apps. C# class library and Java functions must be developed locally.\\nVersion\\nLanguage version\\nChoose a supported version of your language runtime stack.\\nInstance size\\nDefault\\nDetermines the amount of instance memory allocated for each instance of your app. For more information, see Instance memory.\\nAccept the default options in the remaining tabs, including the default behavior of creating a new storage account on the Storage tab and a new Application Insight instance on the Monitoring tab. You can also choose to use an existing storage account or Application Insights instance.\\nSelect Review + create to review the app configuration you chose, and then select Create to provision and deploy the function app.\\nSelect the Notifications icon in the upper-right corner of the portal and watch for the Deployment succeeded message.\\nSelect Go to resource to view your new function app. You can also select Pin to dashboard. Pinning makes it easier to return to this function app resource from your dashboard.\\nNext, create a function in the new function app in the portal using Visual Studio Code for Web.\\nCreate an HTTP trigger function\\nIn the Overview page of your function app under Create functions in your preferred environment select Create with VS Code for the Web. This takes you to the Visual Studio Code for the Web editor.\\nUnder Select a template, scroll down and choose the HTTP trigger template.\\nIn Template details, use HttpExample for New Function, select Anonymous from the Authorization level drop-down list, and then select Create.\\nAzure creates the HTTP trigger function. Now, you can run the new function by sending an HTTP request.\\nTest the function\\nTip\\nThe Code + Test functionality in the portal works even for functions that are read-only and can't be edited in the portal.\\nOn the Overview page for your new function app, select your new HTTP triggered function in the Functions tab.\\nIn the left menu, expand Developer, select Code + Test, and then select Test/Run.\\nIn the Test/Run dialog, select Run.\\nAn HTTP POST request is sent to your new function with a payload that contains the name value of Azure. You can also test the function by selecting GET for HTTP method and adding a name parameter with a value of YOUR_NAME.\\nTip\\nTo test in an external browser, instead select Get function URL, copy the default (Function key) value, add the query string value &name=<YOUR_NAME> to the end of this URL, and then submit the URL in the address bar of your web browser.\\nWhen your function runs, trace information is written to the logs. To see the trace output, return to the Code + Test page in the portal and expand the Logs arrow at the bottom of the page. Call your function again to see the trace output written to the logs.\\nClean up resources\\nOther quickstarts in this collection build upon this quickstart. If you plan to work with subsequent quickstarts, tutorials, or with any of the services you've created in this quickstart, don't clean up the resources.\\nResources in Azure refer to function apps, functions, storage accounts, and so forth. They're grouped into resource groups, and you can delete everything in a group by deleting the group.\\nYou've created resources to complete these quickstarts. You might be billed for these resources, depending on your account status and service pricing. If you don't need the resources anymore, here's how to delete them:\\nIn the Azure portal, go to the Resource group page.\\nTo get to that page from the function app page, select the Overview tab, and then select the link under Resource group.\\nTo get to that page from the dashboard, select Resource groups, and then select the resource group that you used for this article.\\nIn the Resource group page, review the list of included resources, and verify that they're the ones you want to delete.\\nSelect Delete resource group and follow the instructions.\\nDeletion might take a couple of minutes. When it's done, a notification appears for a few seconds. You can also select the bell icon at the top of the page to view the notification.\\nNext steps\\nNow that you've created your first function, let's add an output binding to the function that writes a message to a Storage queue.\\nAdd messages to an Azure Storage queue using Functions\",\n",
       "  'metadata': {}},\n",
       " {'_id': 'bfc79bca-d1c8-5912-c0ab-4de04787a646',\n",
       "  'title': 'Create function app resources in Azure using PowerShell',\n",
       "  'text': '# Create function app resources in Azure using PowerShell\\r\\n\\r\\nThe Azure PowerShell example scripts in this article create function apps and other resources required to host your functions in Azure. A function app provides an execution context in which your functions are executed. All functions running in a function app share the same resources and connections, and they\\'re all scaled together.\\r\\n\\r\\nAfter the resources are created, you can deploy your project files to the new function app. To learn more, see [Deployment methods](functions-deployment-technologies#deployment-methods).\\r\\n\\r\\nEvery function app requires your PowerShell scripts to create the following resources:\\r\\n\\r\\n| Resource | cmdlet | Description |\\r\\n| --- | --- | --- |\\r\\n| Resource group | [New-AzResourceGroup](/en-us/powershell/module/az.resources/new-azresourcegroup) | Creates a [resource group](../azure-resource-manager/management/overview) in which you\\'ll create your function app. |\\r\\n| Storage account | [New-AzStorageAccount](/en-us/powershell/module/az.storage/new-azstorageaccount) | Creates a [storage account](../storage/common/storage-account-create) used by your function app. Storage account names must be between 3 and 24 characters in length and can contain numbers and lowercase letters only. You can also use an existing account, which must meet the [storage account requirements](storage-considerations#storage-account-requirements). |\\r\\n| App Service plan | [New-AzFunctionAppPlan](/en-us/powershell/module/az.functions/new-azfunctionappplan) | Explicitly creates a hosting plan, which defines how resources are allocated to your function app. Used only when hosting in a Premium or Dedicated plan. You won\\'t use this cmdlet when hosting in a serverless [Consumption plan](consumption-plan), since Consumption plans are created when you run `New-AzFunctionApp`. For more information, see [Azure Functions hosting options](functions-scale). |\\r\\n| Function app | [New-AzFunctionApp](/en-us/powershell/module/az.functions/new-azfunctionapp) | Creates the function app using the required resources. The `-Name` parameter must be a globally unique name across all of Azure App Service. Valid characters in `-Name` are `a-z` (case insensitive), `0-9`, and `-`. Most examples create a function app that supports C# functions. You can change the language by using the `-Runtime` parameter, with supported values of `DotNet`, `Java`, `Node`, `PowerShell`, and `Python`. Use the `-RuntimeVersion` to choose a [specific language version](supported-languages#languages-by-runtime-version). |\\r\\n\\r\\nThis article contains the following examples:\\r\\n\\r\\n- Create a serverless function app for C#\\r\\n- Create a serverless function app for Python\\r\\n- Create a scalable function app in a Premium plan\\r\\n- Create a function app in a Dedicated plan\\r\\n- Create a function app with a named Storage connection\\r\\n- Create a function app with an Azure Cosmos DB connection\\r\\n- Create a function app with continuous deployment\\r\\n- Create a serverless Python function app and mount file share\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\n- If you choose to use Azure PowerShell locally:\\r\\n    - [Install the latest version of the Az PowerShell module](/en-us/powershell/azure/install-azure-powershell).\\r\\n    - Connect to your Azure account using the [Connect-AzAccount](/en-us/powershell/module/az.accounts/connect-azaccount) cmdlet.\\r\\n- If you choose to use Azure Cloud Shell:\\r\\n    - See [Overview of Azure Cloud Shell](/en-us/azure/cloud-shell/overview) for more information.\\r\\n\\r\\nIf you don\\'t have an [Azure subscription](/en-us/azure/guides/developer/azure-developer-guide#understanding-accounts-subscriptions-and-billing), create an [Azure free account](https://azure.microsoft.com/free/?ref=microsoft.com&amp;utm_source=microsoft.com&amp;utm_medium=docs&amp;utm_campaign=visualstudio) before you begin.\\r\\n\\r\\n## Create a serverless function app for C#\\r\\n\\r\\nThe following script creates a serverless C# function app in the default Consumption plan:\\r\\n\\r\\n```azurepowershell\\r\\n# Function app and storage account names must be unique.\\r\\n\\r\\n# Variable block\\r\\n$randomIdentifier = Get-Random\\r\\n$location = \"eastus\"\\r\\n$resourceGroup = \"msdocs-azure-functions-rg-$randomIdentifier\"\\r\\n$tag = @{script = \"create-function-app-consumption\"}\\r\\n$storage = \"msdocsaccount$randomIdentifier\"\\r\\n$functionApp = \"msdocs-serverless-function-$randomIdentifier\"\\r\\n$skuStorage = \"Standard_LRS\"\\r\\n$functionsVersion = \"4\"\\r\\n\\r\\n# Create a resource group\\r\\nWrite-Host \"Creating $resourceGroup in $location...\"\\r\\nNew-AzResourceGroup -Name $resourceGroup -Location $location -Tag $tag\\r\\n\\r\\n# Create an Azure storage account in the resource group.\\r\\nWrite-Host \"Creating $storage\"\\r\\nNew-AzStorageAccount -Name $storage -Location $location -ResourceGroupName $resourceGroup -SkuName $skuStorage\\r\\n\\r\\n# Create a serverless function app in the resource group.\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzFunctionApp -Name $functionApp -StorageAccountName $storage -Location $location -ResourceGroupName $resourceGroup -Runtime DotNet-Isolated -FunctionsVersion $functionsVersion\\r\\n```\\r\\n\\r\\n## Create a serverless function app for Python\\r\\n\\r\\nThe following script creates a serverless Python function app in a Consumption plan:\\r\\n\\r\\n```azurepowershell\\r\\n# Function app and storage account names must be unique.\\r\\n\\r\\n# Variable block\\r\\n$randomIdentifier = Get-Random\\r\\n$location = \"eastus\"\\r\\n$resourceGroup = \"msdocs-azure-functions-rg-$randomIdentifier\"\\r\\n$tag = @{script = \"create-function-app-consumption-python\"}\\r\\n$storage = \"msdocsaccount$randomIdentifier\"\\r\\n$functionApp = \"msdocs-serverless-python-function-$randomIdentifier\"\\r\\n$skuStorage = \"Standard_LRS\"\\r\\n$functionsVersion = \"4\"\\r\\n$pythonVersion = \"3.9\" #Allowed values: 3.7, 3.8, and 3.9\\r\\n\\r\\n# Create a resource group\\r\\nWrite-Host \"Creating $resourceGroup in $location...\"\\r\\nNew-AzResourceGroup -Name $resourceGroup -Location $location -Tag $tag\\r\\n\\r\\n# Create an Azure storage account in the resource group.\\r\\nWrite-Host \"Creating $storage\"\\r\\nNew-AzStorageAccount -Name $storage -Location $location -ResourceGroupName $resourceGroup -SkuName $skuStorage\\r\\n\\r\\n# Create a serverless Python function app in the resource group.\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzFunctionApp -Name $functionApp -StorageAccountName $storage -Location $location -ResourceGroupName $resourceGroup -OSType Linux -Runtime Python -RuntimeVersion $pythonVersion -FunctionsVersion $functionsVersion\\r\\n```\\r\\n\\r\\n## Create a scalable function app in a Premium plan\\r\\n\\r\\nThe following script creates a C# function app in an Elastic Premium plan that supports [dynamic scale](event-driven-scaling):\\r\\n\\r\\n```azurepowershell\\r\\n# Function app and storage account names must be unique.\\r\\n\\r\\n# Variable block\\r\\n$randomIdentifier = Get-Random\\r\\n$location = \"eastus\"\\r\\n$resourceGroup = \"msdocs-azure-functions-rg-$randomIdentifier\"\\r\\n$tag = @{script = \"create-function-app-premium-plan\"}\\r\\n$storage = \"msdocsaccount$randomIdentifier\"\\r\\n$premiumPlan = \"msdocs-premium-plan-$randomIdentifier\"\\r\\n$functionApp = \"msdocs-function-$randomIdentifier\"\\r\\n$skuStorage = \"Standard_LRS\" # Allowed values: Standard_LRS, Standard_GRS, Standard_RAGRS, Standard_ZRS, Premium_LRS, Premium_ZRS, Standard_GZRS, Standard_RAGZRS\\r\\n$skuPlan = \"EP1\"\\r\\n$functionsVersion = \"4\"\\r\\n\\r\\n# Create a resource group\\r\\nWrite-Host \"Creating $resourceGroup in $location...\"\\r\\nNew-AzResourceGroup -Name $resourceGroup -Location $location -Tag $tag\\r\\n\\r\\n# Create an Azure storage account in the resource group.\\r\\nWrite-Host \"Creating $storage\"\\r\\nNew-AzStorageAccount -Name $storage -Location $location -ResourceGroupName $resourceGroup -SkuName $skuStorage\\r\\n\\r\\n# Create a Premium plan\\r\\nWrite-Host \"Creating $premiumPlan\"\\r\\nNew-AzFunctionAppPlan -Name $premiumPlan -ResourceGroupName $resourceGroup -Location $location -Sku $skuPlan -WorkerType Windows\\r\\n\\r\\n# Create a Function App\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzFunctionApp -Name $functionApp -StorageAccountName $storage -PlanName $premiumPlan -ResourceGroupName $resourceGroup -Runtime DotNet -FunctionsVersion $functionsVersion\\r\\n```\\r\\n\\r\\n## Create a function app in a Dedicated plan\\r\\n\\r\\nThe following script creates a function app hosted in a Dedicated plan, which isn\\'t scaled dynamically by Functions:\\r\\n\\r\\n```azurepowershell\\r\\n# Function app and storage account names must be unique.\\r\\n\\r\\n# Variable block\\r\\n$randomIdentifier = Get-Random\\r\\n$location = \"eastus\"\\r\\n$resourceGroup = \"msdocs-azure-functions-rg-$randomIdentifier\"\\r\\n$tag = @{script = \"create-function-app-app-service-plan\"}\\r\\n$storage = \"msdocsaccount$randomIdentifier\"\\r\\n$appServicePlan = \"msdocs-app-service-plan-$randomIdentifier\"\\r\\n$functionApp = \"msdocs-serverless-function-$randomIdentifier\"\\r\\n$skuStorage = \"Standard_LRS\"\\r\\n$skuPlan = \"B1\"\\r\\n$functionsVersion = \"4\"\\r\\n\\r\\n# Create a resource group\\r\\nWrite-Host \"Creating $resourceGroup in $location...\"\\r\\nNew-AzResourceGroup -Name $resourceGroup -Location $location -Tag $tag\\r\\n\\r\\n# Create an Azure storage account in the resource group.\\r\\nWrite-Host \"Creating $storage\"\\r\\nNew-AzStorageAccount -Name $storage -Location $location -ResourceGroupName $resourceGroup -SkuName $skuStorage\\r\\n\\r\\n# Create an App Service plan\\r\\nWrite-Host \"Creating $appServicePlan\"\\r\\nNew-AzFunctionAppPlan -Name $appServicePlan -ResourceGroupName $resourceGroup -Location $location -Sku $skuPlan -WorkerType Windows\\r\\n\\r\\n# Create a Function App\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzFunctionApp -Name $functionApp -StorageAccountName $storage -PlanName $appServicePlan -ResourceGroupName $resourceGroup -Runtime DotNet -FunctionsVersion $functionsVersion\\r\\n```\\r\\n\\r\\n## Create a function app with a named Storage connection\\r\\n\\r\\nThe following script creates a function app with a named Storage connection in application settings:\\r\\n\\r\\n```azurepowershell\\r\\n# Function app and storage account names must be unique.\\r\\n\\r\\n# Variable block\\r\\n$randomIdentifier = Get-Random\\r\\n$location = \"eastus\"\\r\\n$resourceGroup = \"msdocs-azure-functions-rg-$randomIdentifier\"\\r\\n$tag = @{script = \"create-function-app-connect-to-storage-account\"}\\r\\n$storage = \"msdocsaccount$randomIdentifier\"\\r\\n$functionApp = \"msdocs-serverless-function-$randomIdentifier\"\\r\\n$skuStorage = \"Standard_LRS\"\\r\\n$functionsVersion = \"4\"\\r\\n\\r\\n# Create a resource group\\r\\nWrite-Host \"Creating $resourceGroup in $location...\"\\r\\nNew-AzResourceGroup -Name $resourceGroup -Location $location -Tag $tag\\r\\n\\r\\n# Create an Azure storage account in the resource group.\\r\\nWrite-Host \"Creating $storage\"\\r\\nNew-AzStorageAccount -Name $storage -Location $location -ResourceGroupName $resourceGroup -SkuName $skuStorage\\r\\n\\r\\n# Create a serverless function app in the resource group.\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzFunctionApp -Name $functionApp -StorageAccountName $storage -Location $location -ResourceGroupName $resourceGroup -Runtime DotNet -FunctionsVersion $functionsVersion\\r\\n\\r\\n# Get the storage account connection string.\\r\\n$connstr = (Get-AzStorageAccount -StorageAccountName $storage -ResourceGroupName $resourceGroup).Context.ConnectionString\\r\\n\\r\\n# Update function app settings to connect to the storage account.\\r\\nUpdate-AzFunctionAppSetting -Name $functionApp -ResourceGroupName $resourceGroup -AppSetting @{StorageConStr = $connstr}\\r\\n```\\r\\n\\r\\n## Create a function app with an Azure Cosmos DB connection\\r\\n\\r\\nThe following script creates a function app and a connected Azure Cosmos DB account:\\r\\n\\r\\n```azurepowershell\\r\\n# Function app and storage account names must be unique.\\r\\n\\r\\n# Variable block\\r\\n$randomIdentifier = Get-Random\\r\\n$location = \"eastus\"\\r\\n$resourceGroup = \"msdocs-azure-functions-rg-$randomIdentifier\"\\r\\n$tag = @{script = \"create-function-app-connect-to-cosmos-db\"}\\r\\n$storage = \"msdocsaccount$randomIdentifier\"\\r\\n$functionApp = \"msdocs-serverless-function-$randomIdentifier\"\\r\\n$skuStorage = \"Standard_LRS\"\\r\\n$functionsVersion = \"4\"\\r\\n\\r\\n# Create a resource group\\r\\nWrite-Host \"Creating $resourceGroup in $location...\"\\r\\nNew-AzResourceGroup -Name $resourceGroup -Location $location -Tag $tag\\r\\n\\r\\n# Create an Azure storage account in the resource group.\\r\\nWrite-Host \"Creating $storage\"\\r\\nNew-AzStorageAccount -Name $storage -Location $location -ResourceGroupName $resourceGroup -SkuName $skuStorage\\r\\n\\r\\n# Create a serverless function app in the resource group.\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzFunctionApp -Name $functionApp -StorageAccountName $storage -Location $location -ResourceGroupName $resourceGroup -Runtime DotNet -FunctionsVersion $functionsVersion\\r\\n\\r\\n# Create an Azure Cosmos DB database account using the same function app name.\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzCosmosDBAccount -Name $functionApp -ResourceGroupName $resourceGroup -Location $location\\r\\n\\r\\n# Get the Azure Cosmos DB connection string.\\r\\n$endpoint = (Get-AzCosmosDBAccount -Name $functionApp -ResourceGroupName $resourceGroup).DocumentEndpoint\\r\\nWrite-Host $endpoint\\r\\n\\r\\n$key = (Get-AzCosmosDBAccountKey -Name $functionApp -ResourceGroupName $resourceGroup).PrimaryMasterKey\\r\\nWrite-Host $key\\r\\n\\r\\n# Configure function app settings to use the Azure Cosmos DB connection string.\\r\\nUpdate-AzFunctionAppSetting -Name $functionApp -ResourceGroupName $resourceGroup -AppSetting @{CosmosDB_Endpoint = $endpoint; CosmosDB_Key = $key}\\r\\n```\\r\\n\\r\\n## Create a function app with continuous deployment\\r\\n\\r\\nThe following script creates a function app that has continuous deployment configured to publish from a public GitHub repository:\\r\\n\\r\\n```azurepowershell\\r\\n# Function app and storage account names must be unique.\\r\\n\\r\\n# Variable block\\r\\n$randomIdentifier = Get-Random\\r\\n$location = \"eastus\"\\r\\n$resourceGroup = \"msdocs-azure-functions-rg-$randomIdentifier\"\\r\\n$tag = @{script = \"deploy-function-app-with-function-github\"}\\r\\n$storage = \"msdocsaccount$randomIdentifier\"\\r\\n$functionApp = \"mygithubfunc$randomIdentifier\"\\r\\n$skuStorage = \"Standard_LRS\"\\r\\n$functionsVersion = \"4\"\\r\\n$runtime = \"Node\"\\r\\n# Public GitHub repository containing an Azure Functions code project.\\r\\n$gitrepo = \"https://github.com/Azure-Samples/functions-quickstart-javascript\"\\r\\n<# Set GitHub personal access token (PAT) to enable authenticated GitHub deployment in your subscription when using a private repo. \\r\\n$token = <Replace with a GitHub access token when using a private repo.>\\r\\n$propertiesObject = @{\\r\\n    token = $token\\r\\n  }\\r\\n\\r\\nSet-AzResource -PropertyObject $propertiesObject -ResourceId /providers/Microsoft.Web/sourcecontrols/GitHub -ApiVersion 2018-02-01 -Force\\r\\n#>\\r\\n\\r\\n# Create a resource group\\r\\nWrite-Host \"Creating $resourceGroup in $location...\"\\r\\nNew-AzResourceGroup -Name $resourceGroup -Location $location -Tag $tag\\r\\n\\r\\n# Create an Azure storage account in the resource group.\\r\\nWrite-Host \"Creating $storage\"\\r\\nNew-AzStorageAccount -Name $storage -Location $location -ResourceGroupName $resourceGroup -SkuName $skuStorage\\r\\n\\r\\n# Create a function app in the resource group.\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzFunctionApp -Name $functionApp -StorageAccountName $storage -Location $location -ResourceGroupName $resourceGroup -Runtime $runtime -FunctionsVersion $functionsVersion\\r\\n\\r\\n# Configure GitHub deployment from a public GitHub repo and deploy once.\\r\\n$propertiesObject = @{\\r\\n    repoUrl = $gitrepo\\r\\n    branch = \\'main\\'\\r\\n    isManualIntegration = $True # $False when using a private repo\\r\\n  }\\r\\n  \\r\\nSet-AzResource -PropertyObject $propertiesObject -ResourceGroupName $resourceGroup -ResourceType Microsoft.Web/sites/sourcecontrols -ResourceName $functionApp/web -ApiVersion 2018-02-01 -Force\\r\\n\\r\\n# Connect to function application\\r\\nInvoke-RestMethod -Uri \"https://$functionApp.azurewebsites.net/api/httpexample?name=Azure\"\\r\\n```\\r\\n\\r\\n## Create a serverless Python function app and mount file share\\r\\n\\r\\nThe following script creates a Python function app on Linux and creates and mounts an external Azure Files share:\\r\\n\\r\\n```azurepowershell\\r\\n# Function app and storage account names must be unique.\\r\\n\\r\\n# Variable block\\r\\n$randomIdentifier = Get-Random\\r\\n$location = \"eastus\"\\r\\n$resourceGroup = \"msdocs-azure-functions-rg-$randomIdentifier\"\\r\\n$tag = @{script = \"functions-cli-mount-files-storage-linux\"}\\r\\n$storage = \"msdocsaccount$randomIdentifier\"\\r\\n$functionApp = \"msdocs-serverless-function-$randomIdentifier\"\\r\\n$skuStorage = \"Standard_LRS\"\\r\\n$functionsVersion = \"4\"\\r\\n$pythonVersion = \"3.9\" #Allowed values: 3.7, 3.8, and 3.9\\r\\n$share = \"msdocs-fileshare-$randomIdentifier\"\\r\\n$directory = \"msdocs-directory-$randomIdentifier\"\\r\\n$shareId = \"msdocs-share-$randomIdentifier\"\\r\\n$mountPath = \"/mounted-$randomIdentifier\"\\r\\n\\r\\n# Create a resource group\\r\\nWrite-Host \"Creating $resourceGroup in $location...\"\\r\\nNew-AzResourceGroup -Name $resourceGroup -Location $location -Tag $tag\\r\\n\\r\\n# Create an Azure storage account in the resource group.\\r\\nWrite-Host \"Creating $storage\"\\r\\nNew-AzStorageAccount -Name $storage -Location $location -ResourceGroupName $resourceGroup -SkuName $skuStorage\\r\\n\\r\\n# Get the storage account key. \\r\\n$keys = Get-AzStorageAccountKey -Name $storage -ResourceGroupName $resourceGroup\\r\\n$storageKey = $keys[0].Value\\r\\n\\r\\n## Create a serverless Python function app in the resource group.\\r\\nWrite-Host \"Creating $functionApp\"\\r\\nNew-AzFunctionApp -Name $functionApp -StorageAccountName $storage -Location $location -ResourceGroupName $resourceGroup -OSType Linux -Runtime Python -RuntimeVersion $pythonVersion -FunctionsVersion $functionsVersion\\r\\n\\r\\n# Create a share in Azure Files.\\r\\nWrite-Host \"Creating $share\"\\r\\n$storageContext = New-AzStorageContext -StorageAccountName $storage -StorageAccountKey $storageKey\\r\\nNew-AzStorageShare -Name $share -Context $storageContext\\r\\n\\r\\n# Create a directory in the share.\\r\\nWrite-Host \"Creating $directory in $share\"\\r\\nNew-AzStorageDirectory -ShareName $share -Path $directory -Context $storageContext\\r\\n\\r\\n# Add a storage account configuration to the function app\\r\\nWrite-Host \"Adding $storage configuration\"\\r\\n$storagePath = New-AzWebAppAzureStoragePath -Name $shareid -Type AzureFiles -ShareName $share -AccountName $storage -MountPath $mountPath -AccessKey $storageKey\\r\\nSet-AzWebApp -Name $functionApp -ResourceGroupName $resourceGroup -AzureStoragePath $storagePath \\r\\n\\r\\n# Get a function app\\'s storage account configurations.\\r\\n(Get-AzWebApp -Name $functionApp -ResourceGroupName $resourceGroup).AzureStoragePath\\r\\n```\\r\\n\\r\\nMounted file shares are only supported on Linux. For more information, see [Mount file shares](storage-considerations#mount-file-shares).\\r\\n\\r\\n## Clean up resources\\r\\n\\r\\nIn the preceding steps, you created Azure resources in a resource group. If you don\\'t expect to need these resources in the future, delete the resource group by running the following PowerShell command:\\r\\n\\r\\n```azurecli\\r\\nRemove-AzResourceGroup -Name myResourceGroup\\r\\n```\\r\\n\\r\\nThis command may take a minute to run.',\n",
       "  'metadata': {}},\n",
       " {'_id': '3cac48f1-a41f-f5c3-93f9-ce052e3e0acb',\n",
       "  'title': 'Azure Functions Dedicated hosting',\n",
       "  'text': \"# Dedicated hosting plans for Azure Functions\\r\\n\\r\\nThis article is about hosting your function app with dedicated resources in an App Service plan, including in an App Service Environment (ASE). For other hosting options, see the [hosting plan article](functions-scale).\\r\\n\\r\\nAn App Service plan defines a set of dedicated compute resources for an app to run. These dedicated compute resources are analogous to the [*server farm*](https://wikipedia.org/wiki/Server_farm) in conventional hosting. One or more function apps can be configured to run on the same computing resources (App Service plan) as other App Service apps, such as web apps. The dedicated App Service plans supported for function app hosting include Basic, Standard, Premium, and Isolated SKUs. For details about how the App Service plan works, see the [Azure App Service plans in-depth overview](../app-service/overview-hosting-plans).\\r\\n\\r\\nImportant\\r\\n\\r\\nFree and Shared tier App Service plans aren't supported by Azure Functions. For a lower-cost option hosting your function executions, you should instead consider the [Consumption plan](consumption-plan) or the [Flex Consumption plan](flex-consumption-plan), where you are billed based on function executions.\\r\\n\\r\\nConsider a dedicated App Service plan in the following situations:\\r\\n\\r\\n- You have existing, underutilized VMs that are already running other App Service instances.\\r\\n- You want to provide a custom image on which to run your functions.\\r\\n\\r\\n## Billing\\r\\n\\r\\nYou pay for function apps in an App Service Plan as you would for other App Service resources. This differs from Azure Functions [Consumption plan](consumption-plan) or [Premium plan](functions-premium-plan) hosting, which have consumption-based cost components. You are billed only for the plan, regardless of how many function apps or web apps run in the plan. To learn more, see the [App Service pricing page](https://azure.microsoft.com/pricing/details/app-service/windows/).\\r\\n\\r\\n##  Always On\\r\\n\\r\\nWhen you run your app on an App Service plan, you should enable the **Always on** setting so that your function app runs correctly. On an App Service plan, the Functions runtime goes idle after a few minutes of inactivity. The **Always on** setting is available only on an App Service plan. In other plans, the platform activates function apps automatically. If you choose not to enable **Always on**, you can reactivate an idled app in these ways:\\r\\n\\r\\n- Send a request to an HTTP trigger endpoint or any other endpoint on the app. Even a failed request should wake up your app.\\r\\n- Access your app in the [Azure portal](https://portal.azure.com).\\r\\n\\r\\nEven with **Always on** enabled, the execution timeout for individual functions is controlled by the `functionTimeout` setting in the [host.json](functions-host-json#functiontimeout) project file.\\r\\n\\r\\n## Scaling\\r\\n\\r\\nUsing an App Service plan, you can manually scale out by adding more VM instances. You can also enable autoscale, though autoscale will be slower than the elastic scale of the Premium plan. For more information, see [Scale instance count manually or automatically](/en-us/azure/azure-monitor/autoscale/autoscale-get-started?toc=%2fazure%2fapp-service%2ftoc.json). You can also scale up by choosing a different App Service plan. For more information, see [Scale up an app in Azure](../app-service/manage-scale-up).\\r\\n\\r\\nNote\\r\\n\\r\\nWhen running JavaScript (Node.js) functions on an App Service plan, you should choose a plan that has fewer vCPUs. For more information, see [Choose single-core App Service plans](functions-reference-node#choose-single-vcpu-app-service-plans).\\r\\n\\r\\n## App Service Environments\\r\\n\\r\\nRunning in an App Service Environment (ASE) lets you fully isolate your functions and take advantage of higher numbers of instances than an App Service Plan. To get started, see [Introduction to the App Service Environments](../app-service/environment/overview).\\r\\n\\r\\nIf you just want to run your function app in a virtual network, you can do this using the [Premium plan](functions-premium-plan). To learn more, see [Establish Azure Functions private site access](functions-create-private-site-access).\",\n",
       "  'metadata': {}},\n",
       " {'_id': '1e5fb7b6-7ffb-a690-cd8f-8672f7c38aca',\n",
       "  'title': 'Zip push deployment for Azure Functions',\n",
       "  'text': '# Zip deployment for Azure Functions\\r\\n\\r\\nThis article describes how to deploy your function app project files to Azure from a .zip (compressed) file. You learn how to do a push deployment, both by using Azure CLI and by using the REST APIs. [Azure Functions Core Tools](functions-run-local) also uses these deployment APIs when publishing a local project to Azure.\\r\\n\\r\\nZip deployment is also an easy way to [run your functions from a package file in Azure](run-functions-from-deployment-package). It is the default deployment technology in the [Consumption](consumption-plan), [Elastic Premium](functions-premium-plan), and [Dedicated (App Service)](dedicated-plan) hosting plans. The [Flex Consumption](flex-consumption-plan) plan does not support zip deployment.\\r\\n\\r\\nAzure Functions has the full range of continuous deployment and integration options that are provided by Azure App Service. For more information, see [Continuous deployment for Azure Functions](functions-continuous-deployment).\\r\\n\\r\\nTo speed up development, you might find it easier to deploy your function app project files directly from a .zip file. The .zip deployment API takes the contents of a .zip file and extracts the contents into the `wwwroot` folder of your function app. This .zip file deployment uses the same Kudu service that powers continuous integration-based deployments, including:\\r\\n\\r\\n- Deletion of files that were left over from earlier deployments.\\r\\n- Deployment customization, including running deployment scripts.\\r\\n- Deployment logs.\\r\\n- Syncing function triggers in a [Consumption plan](functions-scale) function app.\\r\\n\\r\\nFor more information, see the [.zip deployment reference](https://github.com/projectkudu/kudu/wiki/Deploying-from-a-zip-file).\\r\\n\\r\\nImportant\\r\\n\\r\\nWhen you use .zip deployment, any files from an existing deployment that aren\\'t found in the .zip file are deleted from your function app.\\r\\n\\r\\n## Deployment .zip file requirements\\r\\n\\r\\nThe zip archive you deploy must contain all of the files needed to run your function app. You can manually create a zip archive from the contents of a Functions project folder using built-in .zip compression functionality or third-party tools.\\r\\n\\r\\nThe archive must include the [host.json](functions-host-json) file at the root of the extracted folder. The selected language stack for the function app creates additional requirements:\\r\\n\\r\\n- [.NET (isolated worker model)](dotnet-isolated-process-guide#deployment-payload)\\r\\n- [.NET (in-process model)](functions-dotnet-class-library#functions-class-library-project)\\r\\n- [Java](functions-reference-java#folder-structure)\\r\\n- [JavaScript](functions-reference-node?tabs=javascript#folder-structure)\\r\\n- [TypeScript](functions-reference-node?tabs=typescript#folder-structure)\\r\\n- [PowerShell](functions-reference-powershell#folder-structure)\\r\\n- [Python](functions-reference-python#folder-structure)\\r\\n\\r\\nImportant\\r\\n\\r\\nFor languages that generate compiled output for deployment, make sure to compress the contents of the output folder you plan to publish and not the entire project folder. When Functions extracts the contents of the zip archive, the `host.json` file must exist in the root of the package.\\r\\n\\r\\nA zip deployment process extracts the zip archive\\'s files and folders in the `wwwroot` directory. If you include the parent directory when creating the archive, the system will not find the files it expects to see in `wwwroot`.\\r\\n\\r\\n## Deploy by using Azure CLI\\r\\n\\r\\nYou can use Azure CLI to trigger a push deployment. Push deploy a .zip file to your function app by using the [az functionapp deployment source config-zip](/en-us/cli/azure/functionapp/deployment/source#az-functionapp-deployment-source-config-zip) command. To use this command, you must use Azure CLI version 2.0.21 or later. To see what Azure CLI version you are using, use the `az --version` command.\\r\\n\\r\\nIn the following command, replace the `<zip_file_path>` placeholder with the path to the location of your .zip file. Also, replace `<app_name>` with the unique name of your function app and replace `<resource_group>` with the name of your resource group.\\r\\n\\r\\n```azurecli\\r\\naz functionapp deployment source config-zip -g <resource_group> -n \\\\\\r\\n<app_name> --src <zip_file_path>\\r\\n```\\r\\n\\r\\nThis command deploys project files from the downloaded .zip file to your function app in Azure. It then restarts the app. To view the list of deployments for this function app, you must use the REST APIs.\\r\\n\\r\\nWhen you\\'re using Azure CLI on your local computer, `<zip_file_path>` is the path to the .zip file on your computer. You can also run Azure CLI in [Azure Cloud Shell](../cloud-shell/overview). When you use Cloud Shell, you must first upload your deployment .zip file to the Azure Files account that\\'s associated with your Cloud Shell. In that case, `<zip_file_path>` is the storage location that your Cloud Shell account uses. For more information, see [Persist files in Azure Cloud Shell](../cloud-shell/persisting-shell-storage).\\r\\n\\r\\n## Deploy ZIP file with REST APIs\\r\\n\\r\\nYou can use the [deployment service REST APIs](https://github.com/projectkudu/kudu/wiki/REST-API) to deploy the .zip file to your app in Azure. To deploy, send a POST request to `https://<app_name>.scm.azurewebsites.net/api/zipdeploy`. The POST request must contain the .zip file in the message body. The deployment credentials for your app are provided in the request by using HTTP BASIC authentication. For more information, see the [.zip push deployment reference](https://github.com/projectkudu/kudu/wiki/Deploying-from-a-zip-file).\\r\\n\\r\\nFor the HTTP BASIC authentication, you need your App Service deployment credentials. To see how to set your deployment credentials, see [Set and reset user-level credentials](../app-service/deploy-configure-credentials#userscope).\\r\\n\\r\\n### With cURL\\r\\n\\r\\nThe following example uses the cURL tool to deploy a .zip file. Replace the placeholders `<deployment_user>`, `<zip_file_path>`, and `<app_name>`. When prompted by cURL, type in the password.\\r\\n\\r\\n```bash\\r\\ncurl -X POST -u <deployment_user> --data-binary \"@<zip_file_path>\" https://<app_name>.scm.azurewebsites.net/api/zipdeploy\\r\\n```\\r\\n\\r\\nThis request triggers push deployment from the uploaded .zip file. You can review the current and past deployments by using the `https://<app_name>.scm.azurewebsites.net/api/deployments` endpoint, as shown in the following cURL example. Again, replace `<app_name>` with the name of your app and `<deployment_user>` with the username of your deployment credentials.\\r\\n\\r\\n```bash\\r\\ncurl -u <deployment_user> https://<app_name>.scm.azurewebsites.net/api/deployments\\r\\n```\\r\\n\\r\\n#### Asynchronous zip deployment\\r\\n\\r\\nWhile deploying synchronously you may receive errors related to connection timeouts. Add `?isAsync=true` to the URL to deploy asynchronously. You will receive a response as soon as the zip file is uploaded with a `Location` header pointing to the pollable deployment status URL. When polling the URL provided in the `Location` header, you will receive a HTTP 202 (Accepted) response while the process is ongoing and a HTTP 200 (OK) response once the archive has been expanded and the deployment has completed successfully.\\r\\n\\r\\n#### Microsoft Entra authentication\\r\\n\\r\\nAn alternative to using HTTP BASIC authentication for the zip deployment is to use a Microsoft Entra identity. Microsoft Entra identity may be needed if [HTTP BASIC authentication is disabled for the SCM site](../app-service/deploy-configure-credentials#disable-basic-authentication).\\r\\n\\r\\nA valid Microsoft Entra access token for the user or service principal performing the deployment will be required. An access token can be retrieved using the Azure CLI\\'s `az account get-access-token` command. The access token will be used in the Authentication header of the HTTP POST request.\\r\\n\\r\\n```bash\\r\\ncurl -X POST \\\\\\r\\n    --data-binary \"@<zip_file_path>\" \\\\\\r\\n    -H \"Authorization: Bearer <access_token>\" \\\\\\r\\n    \"https://<app_name>.scm.azurewebsites.net/api/zipdeploy\"\\r\\n```\\r\\n\\r\\n### With PowerShell\\r\\n\\r\\nThe following example uses [Publish-AzWebapp](/en-us/powershell/module/az.websites/publish-azwebapp) upload the .zip file. Replace the placeholders `<group-name>`, `<app-name>`, and `<zip-file-path>`.\\r\\n\\r\\n```powershell\\r\\nPublish-AzWebapp -ResourceGroupName <group-name> -Name <app-name> -ArchivePath <zip-file-path>\\r\\n```\\r\\n\\r\\nThis request triggers push deployment from the uploaded .zip file.\\r\\n\\r\\nTo review the current and past deployments, run the following commands. Again, replace the `<deployment-user>`, `<deployment-password>`, and `<app-name>` placeholders.\\r\\n\\r\\n```bash\\r\\n$username = \"<deployment-user>\"\\r\\n$password = \"<deployment-password>\"\\r\\n$apiUrl = \"https://<app-name>.scm.azurewebsites.net/api/deployments\"\\r\\n$base64AuthInfo = [Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes((\"{0}:{1}\" -f $username, $password)))\\r\\n$userAgent = \"powershell/1.0\"\\r\\nInvoke-RestMethod -Uri $apiUrl -Headers @{Authorization=(\"Basic {0}\" -f $base64AuthInfo)} -UserAgent $userAgent -Method GET\\r\\n```\\r\\n\\r\\n## Deploy by using ARM Template\\r\\n\\r\\nYou can use [ZipDeploy ARM template extension](https://github.com/projectkudu/kudu/wiki/MSDeploy-VS.-ZipDeploy#zipdeploy) to push your .zip file to your function app.\\r\\n\\r\\n### Example ZipDeploy ARM Template\\r\\n\\r\\nThis template includes both a production and staging slot and deploys to one or the other. Typically, you would use this template to deploy to the staging slot and then swap to get your new zip package running on the production slot.\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\\r\\n  \"contentVersion\": \"1.0.0.0\",\\r\\n  \"parameters\": {\\r\\n    \"appServiceName\": {\\r\\n      \"type\": \"string\"\\r\\n    },\\r\\n    \"deployToProduction\": {\\r\\n      \"type\": \"bool\",\\r\\n      \"defaultValue\": false\\r\\n    },\\r\\n    \"slot\": {\\r\\n      \"type\": \"string\",\\r\\n      \"defaultValue\": \"staging\"\\r\\n    },\\r\\n    \"packageUri\": {\\r\\n      \"type\": \"secureString\"\\r\\n    }\\r\\n  },\\r\\n  \"resources\": [\\r\\n    {\\r\\n      \"condition\": \"[parameters(\\'deployToProduction\\')]\",\\r\\n      \"type\": \"Microsoft.Web/sites/extensions\",\\r\\n      \"apiVersion\": \"2021-02-01\",\\r\\n      \"name\": \"[format(\\'{0}/ZipDeploy\\', parameters(\\'appServiceName\\'))]\",\\r\\n      \"properties\": {\\r\\n        \"packageUri\": \"[parameters(\\'packageUri\\')]\",\\r\\n        \"appOffline\": true\\r\\n      }\\r\\n    },\\r\\n    {\\r\\n      \"condition\": \"[not(parameters(\\'deployToProduction\\'))]\",\\r\\n      \"type\": \"Microsoft.Web/sites/slots/extensions\",\\r\\n      \"apiVersion\": \"2021-02-01\",\\r\\n      \"name\": \"[format(\\'{0}/{1}/ZipDeploy\\', parameters(\\'appServiceName\\'), parameters(\\'slot\\'))]\",\\r\\n      \"properties\": {\\r\\n        \"packageUri\": \"[parameters(\\'packageUri\\')]\",\\r\\n        \"appOffline\": true\\r\\n      }\\r\\n    }\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\nFor the initial deployment, you would deploy directly to the production slot. For more information, see [Slot deployments](functions-infrastructure-as-code#slot-deployments).\\r\\n\\r\\n## Run functions from the deployment package\\r\\n\\r\\nYou can also choose to run your functions directly from the deployment package file. This method skips the deployment step of copying files from the package to the `wwwroot` directory of your function app. Instead, the package file is mounted by the Functions runtime, and the contents of the `wwwroot` directory become read-only.\\r\\n\\r\\nZip deployment integrates with this feature, which you can enable by setting the function app setting `WEBSITE_RUN_FROM_PACKAGE` to a value of `1`. For more information, see [Run your functions from a deployment package file](run-functions-from-deployment-package).\\r\\n\\r\\n## Deployment customization\\r\\n\\r\\nThe deployment process assumes that the .zip file that you push contains a ready-to-run app. By default, no customizations are run. To enable the same build processes that you get with continuous integration, add the following to your application settings:\\r\\n\\r\\n`SCM_DO_BUILD_DURING_DEPLOYMENT=true`\\r\\n\\r\\nWhen you use .zip push deployment, this setting is **false** by default. The default is **true** for continuous integration deployments. When set to **true**, your deployment-related settings are used during deployment. You can configure these settings either as app settings or in a .deployment configuration file that\\'s located in the root of your .zip file. For more information, see [Repository and deployment-related settings](https://github.com/projectkudu/kudu/wiki/Configurable-settings#repository-and-deployment-related-settings) in the deployment reference.\\r\\n\\r\\n## Download your function app files\\r\\n\\r\\nIf you created your functions by using the editor in the Azure portal, you can download your existing function app project as a .zip file in one of these ways:\\r\\n\\r\\n- **From the Azure portal:**\\r\\n\\r\\n    1. Sign in to the [Azure portal](https://portal.azure.com), and then go to your function app.\\r\\n    2. On the **Overview** tab, select **Download app content**. Select your download options, and then select **Download**.\\r\\n\\r\\n        The downloaded .zip file is in the correct format to be republished to your function app by using .zip push deployment. The portal download can also add the files needed to open your function app directly in Visual Studio.\\r\\n- **Using REST APIs:**\\r\\n\\r\\n    Use the following deployment GET API to download the files from your `<function_app>` project:\\r\\n\\r\\n    ```http\\r\\n    https://<function_app>.scm.azurewebsites.net/api/zip/site/wwwroot/\\r\\n    ```\\r\\n\\r\\n    Including `/site/wwwroot/` makes sure your zip file includes only the function app project files and not the entire site. If you are not already signed in to Azure, you will be asked to do so.\\r\\n\\r\\nYou can also download a .zip file from a GitHub repository. When you download a GitHub repository as a .zip file, GitHub adds an extra folder level for the branch. This extra folder level means that you can\\'t deploy the .zip file directly as you downloaded it from GitHub. If you\\'re using a GitHub repository to maintain your function app, you should use [continuous integration](functions-continuous-deployment) to deploy your app.',\n",
       "  'metadata': {}},\n",
       " {'_id': 'dc385d8e-f6f2-817e-7896-51d27e78111e',\n",
       "  'title': 'Develop Python worker extensions for Azure Functions',\n",
       "  'text': '# Develop Python worker extensions for Azure Functions\\r\\n\\r\\nAzure Functions lets you integrate custom behaviors as part of Python function execution. This feature enables you to create business logic that customers can easily use in their own function apps. To learn more, see the [Python developer reference](functions-reference-python#python-worker-extensions). Worker extensions are supported in both the v1 and v2 Python programming models.\\r\\n\\r\\nIn this tutorial, you\\'ll learn how to:\\r\\n\\r\\n- Create an application-level Python worker extension for Azure Functions.\\r\\n- Consume your extension in an app the way your customers do.\\r\\n- Package and publish an extension for consumption.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nBefore you start, you must meet these requirements:\\r\\n\\r\\n- [Python 3.7 or above](https://www.python.org/downloads). To check the full list of supported Python versions in Azure Functions, see the [Python developer guide](functions-reference-python#python-version).\\r\\n- The [Azure Functions Core Tools](functions-run-local#v2), version 4.0.5095 or later, which supports using the extension with the [v2 Python programming model](functions-reference-python). Check your version with `func --version`.\\r\\n- [Visual Studio Code](https://code.visualstudio.com/) installed on one of the [supported platforms](https://code.visualstudio.com/docs/supporting/requirements#_platforms).\\r\\n\\r\\n## Create the Python Worker extension\\r\\n\\r\\nThe extension you create reports the elapsed time of an HTTP trigger invocation in the console logs and in the HTTP response body.\\r\\n\\r\\n### Folder structure\\r\\n\\r\\nThe folder for your extension project should be like the following structure:\\r\\n\\r\\n```\\r\\n<python_worker_extension_root>/\\r\\n | - .venv/\\r\\n | - python_worker_extension_timer/\\r\\n | | - __init__.py\\r\\n | - setup.py\\r\\n | - readme.md\\r\\n```\\r\\n\\r\\n| Folder/file | Description |\\r\\n| --- | --- |\\r\\n| **.venv/** | (Optional) Contains a Python virtual environment used for local development. |\\r\\n| **python\\\\_worker\\\\_extension/** | Contains the source code of the Python worker extension. This folder contains the main Python module to be published into PyPI. |\\r\\n| **setup.py** | Contains the metadata of the Python worker extension package. |\\r\\n| **readme.md** | Contains the instruction and usage of your extension. This content is displayed as the description in the home page in your PyPI project. |\\r\\n\\r\\n### Configure project metadata\\r\\n\\r\\nFirst you create `setup.py`, which provides essential information about your package. To make sure that your extension is distributed and integrated into your customer\\'s function apps properly, confirm that `\\'azure-functions >= 1.7.0, < 2.0.0\\'` is in the `install_requires` section.\\r\\n\\r\\nIn the following template, you should change `author`, `author_email`, `install_requires`, `license`, `packages`, and `url` fields as needed.\\r\\n\\r\\n```python\\r\\nfrom setuptools import find_packages, setup\\r\\nsetup(\\r\\n    name=\\'python-worker-extension-timer\\',\\r\\n    version=\\'1.0.0\\',\\r\\n    author=\\'Your Name Here\\',\\r\\n    author_email=\\'your@email.here\\',\\r\\n    classifiers=[\\r\\n        \\'Intended Audience :: End Users/Desktop\\',\\r\\n        \\'Development Status :: 5 - Production/Stable\\',\\r\\n        \\'Intended Audience :: End Users/Desktop\\',\\r\\n        \\'License :: OSI Approved :: Apache Software License\\',\\r\\n        \\'Programming Language :: Python\\',\\r\\n        \\'Programming Language :: Python :: 3.7\\',\\r\\n        \\'Programming Language :: Python :: 3.8\\',\\r\\n        \\'Programming Language :: Python :: 3.9\\',\\r\\n        \\'Programming Language :: Python :: 3.10\\',\\r\\n    ],\\r\\n    description=\\'Python Worker Extension Demo\\',\\r\\n    include_package_data=True,\\r\\n    long_description=open(\\'readme.md\\').read(),\\r\\n    install_requires=[\\r\\n        \\'azure-functions >= 1.7.0, < 2.0.0\\',\\r\\n        # Any additional packages that will be used in your extension\\r\\n    ],\\r\\n    extras_require={},\\r\\n    license=\\'MIT\\',\\r\\n    packages=find_packages(where=\\'.\\'),\\r\\n    url=\\'https://your-github-or-pypi-link\\',\\r\\n    zip_safe=False,\\r\\n)\\r\\n```\\r\\n\\r\\nNext, you\\'ll implement your extension code in the application-level scope.\\r\\n\\r\\n### Implement the timer extension\\r\\n\\r\\nAdd the following code in `python_worker_extension_timer/__init__.py` to implement the application-level extension:\\r\\n\\r\\n```python\\r\\nimport typing\\r\\nfrom logging import Logger\\r\\nfrom time import time\\r\\nfrom azure.functions import AppExtensionBase, Context, HttpResponse\\r\\nclass TimerExtension(AppExtensionBase):\\r\\n    \"\"\"A Python worker extension to record elapsed time in a function invocation\\r\\n    \"\"\"\\r\\n\\r\\n    @classmethod\\r\\n    def init(cls):\\r\\n        # This records the starttime of each function\\r\\n        cls.start_timestamps: typing.Dict[str, float] = {}\\r\\n\\r\\n    @classmethod\\r\\n    def configure(cls, *args, append_to_http_response:bool=False, **kwargs):\\r\\n        # Customer can use TimerExtension.configure(append_to_http_response=)\\r\\n        # to decide whether the elapsed time should be shown in HTTP response\\r\\n        cls.append_to_http_response = append_to_http_response\\r\\n\\r\\n    @classmethod\\r\\n    def pre_invocation_app_level(\\r\\n        cls, logger: Logger, context: Context,\\r\\n        func_args: typing.Dict[str, object],\\r\\n        *args, **kwargs\\r\\n    ) -> None:\\r\\n        logger.info(f\\'Recording start time of {context.function_name}\\')\\r\\n        cls.start_timestamps[context.invocation_id] = time()\\r\\n\\r\\n    @classmethod\\r\\n    def post_invocation_app_level(\\r\\n        cls, logger: Logger, context: Context,\\r\\n        func_args: typing.Dict[str, object],\\r\\n        func_ret: typing.Optional[object],\\r\\n        *args, **kwargs\\r\\n    ) -> None:\\r\\n        if context.invocation_id in cls.start_timestamps:\\r\\n            # Get the start_time of the invocation\\r\\n            start_time: float = cls.start_timestamps.pop(context.invocation_id)\\r\\n            end_time: float = time()\\r\\n            # Calculate the elapsed time\\r\\n            elapsed_time = end_time - start_time\\r\\n            logger.info(f\\'Time taken to execute {context.function_name} is {elapsed_time} sec\\')\\r\\n            # Append the elapsed time to the end of HTTP response\\r\\n            # if the append_to_http_response is set to True\\r\\n            if cls.append_to_http_response and isinstance(func_ret, HttpResponse):\\r\\n                func_ret._HttpResponse__body += f\\' (TimeElapsed: {elapsed_time} sec)\\'.encode()\\r\\n```\\r\\n\\r\\nThis code inherits from [AppExtensionBase](https://github.com/Azure/azure-functions-python-library/blob/dev/azure/functions/extension/app_extension_base.py) so that the extension applies to every function in the app. You could have also implemented the extension on a function-level scope by inheriting from [FuncExtensionBase](https://github.com/Azure/azure-functions-python-library/blob/dev/azure/functions/extension/func_extension_base.py).\\r\\n\\r\\nThe `init` method is a class method that\\'s called by the worker when the extension class is imported. You can do initialization actions here for the extension. In this case, a hash map is initialized for recording the invocation start time for each function.\\r\\n\\r\\nThe `configure` method is customer-facing. In your readme file, you can tell your customers when they need to call `Extension.configure()`. The readme should also document the extension capabilities, possible configuration, and usage of your extension. In this example, customers can choose whether the elapsed time is reported in the `HttpResponse`.\\r\\n\\r\\nThe `pre_invocation_app_level` method is called by the Python worker before the function runs. It provides the information from the function, such as function context and arguments. In this example, the extension logs a message and records the start time of an invocation based on its invocation\\\\_id.\\r\\n\\r\\nSimilarly, the `post_invocation_app_level` is called after function execution. This example calculates the elapsed time based on the start time and current time. It also overwrites the return value of the HTTP response.\\r\\n\\r\\n### Create a readme.md\\r\\n\\r\\nCreate a readme.md file in the root of your extension project. This file contains the instructions and usage of your extension. The readme.md content is displayed as the description in the home page in your PyPI project.\\r\\n\\r\\n```markdown\\r\\n# Python Worker Extension Timer\\r\\n\\r\\nIn this file, tell your customers when they need to call `Extension.configure()`.\\r\\n\\r\\nThe readme should also document the extension capabilities, possible configuration,\\r\\nand usage of your extension.\\r\\n```\\r\\n\\r\\n## Consume your extension locally\\r\\n\\r\\nNow that you\\'ve created an extension, you can use it in an app project to verify it works as intended.\\r\\n\\r\\n### Create an HTTP trigger function\\r\\n\\r\\n1. Create a new folder for your app project and navigate to it.\\r\\n2. From the appropriate shell, such as Bash, run the following command to initialize the project:\\r\\n\\r\\n    ```bash\\r\\n    func init --python\\r\\n    ```\\r\\n3. Use the following command to create a new HTTP trigger function that allows anonymous access:\\r\\n\\r\\n    ```bash\\r\\n    func new -t HttpTrigger -n HttpTrigger -a anonymous\\r\\n    ```\\r\\n\\r\\n### Activate a virtual environment\\r\\n\\r\\n1. Create a Python virtual environment, based on OS as follows:\\r\\n\\r\\n**Linux**\\r\\n\\r\\n    ```bash\\r\\n    python3 -m venv .venv\\r\\n    ```\\r\\n\\r\\n**Windows**\\r\\n\\r\\n    ```console\\r\\n    py -m venv .venv\\r\\n    ```\\r\\n2. Activate the Python virtual environment, based on OS as follows:\\r\\n\\r\\n**Linux**\\r\\n\\r\\n    ```bash\\r\\n    source .venv/bin/activate\\r\\n    ```\\r\\n\\r\\n**Windows**\\r\\n\\r\\n    ```console\\r\\n    .venv\\\\Scripts\\\\Activate.ps1\\r\\n    ```\\r\\n\\r\\n### Configure the extension\\r\\n\\r\\n1. Install remote packages for your function app project using the following command:\\r\\n\\r\\n    ```bash\\r\\n    pip install -r requirements.txt\\r\\n    ```\\r\\n2. Install the extension from your local file path, in editable mode as follows:\\r\\n\\r\\n    ```bash\\r\\n    pip install -e <PYTHON_WORKER_EXTENSION_ROOT>\\r\\n    ```\\r\\n\\r\\n    In this example, replace `<PYTHON_WORKER_EXTENSION_ROOT>` with the root file location of your extension project.\\r\\n\\r\\n    When a customer uses your extension, they\\'ll instead add your extension package location to the requirements.txt file, as in the following examples:\\r\\n\\r\\n**PyPI**\\r\\n\\r\\n    ```python\\r\\n    # requirements.txt\\r\\n    python_worker_extension_timer==1.0.0\\r\\n    ```\\r\\n\\r\\n**GitHub**\\r\\n\\r\\n    ```python\\r\\n    # requirements.txt\\r\\n    git+https://github.com/Azure-Samples/python-worker-extension-timer@main\\r\\n    ```\\r\\n3. Open the local.settings.json project file and add the following field to `Values`:\\r\\n\\r\\n    ```json\\r\\n    \"PYTHON_ENABLE_WORKER_EXTENSIONS\": \"1\" \\r\\n    ```\\r\\n\\r\\n    When running in Azure, you instead add `PYTHON_ENABLE_WORKER_EXTENSIONS=1` to the [app settings in the function app](functions-how-to-use-azure-function-app-settings#settings).\\r\\n4. Add following two lines before the `main` function in *\\\\_\\\\_init.py\\\\_\\\\_* file for the v1 programming model, or in the *function\\\\_app.py* file for the v2 programming model:\\r\\n\\r\\n    ```python\\r\\n    from python_worker_extension_timer import TimerExtension\\r\\n    TimerExtension.configure(append_to_http_response=True)\\r\\n    ```\\r\\n\\r\\n    This code imports the `TimerExtension` module and sets the `append_to_http_response` configuration value.\\r\\n\\r\\n### Verify the extension\\r\\n\\r\\n1. From your app project root folder, start the function host using `func host start --verbose`. You should see the local endpoint of your function in the output as `https://localhost:7071/api/HttpTrigger`.\\r\\n2. In the browser, send a GET request to `https://localhost:7071/api/HttpTrigger`. You should see a response like the following, with the **TimeElapsed** data for the request appended.\\r\\n\\r\\n    ```\\r\\n    This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response. (TimeElapsed: 0.0009996891021728516 sec)\\r\\n    ```\\r\\n\\r\\n## Publish your extension\\r\\n\\r\\nAfter you\\'ve created and verified your extension, you still need to complete these remaining publishing tasks:\\r\\n\\r\\n- Choose a license.\\r\\n- Create a readme.md and other documentation.\\r\\n- Publish the extension library to a Python package registry or a version control system (VCS).\\r\\n\\r\\n**PyPI**\\r\\nTo publish your extension to PyPI:\\r\\n\\r\\n1. Run the following command to install `twine` and `wheel` in your default Python environment or a virtual environment:\\r\\n\\r\\n    ```bash\\r\\n    pip install twine wheel\\r\\n    ```\\r\\n2. Remove the old `dist/` folder from your extension repository.\\r\\n3. Run the following command to generate a new package inside `dist/`:\\r\\n\\r\\n    ```bash\\r\\n    python setup.py sdist bdist_wheel\\r\\n    ```\\r\\n4. Run the following command to upload the package to PyPI:\\r\\n\\r\\n    ```bash\\r\\n    twine upload dist/*\\r\\n    ```\\r\\n\\r\\n    You may need to provide your PyPI account credentials during upload. You can also test your package upload with `twine upload -r testpypi dist/*`. For more information, see the [Twine documentation](https://twine.readthedocs.io/en/stable/).\\r\\n\\r\\nAfter these steps, customers can use your extension by including your package name in their requirements.txt.\\r\\n\\r\\nFor more information, see the [official Python packaging tutorial](https://packaging.python.org/tutorials/packaging-projects/).\\r\\n\\r\\n**GitHub**\\r\\nYou can also publish the extension source code with the setup.py file to a GitHub repository, as shown in [this sample repository](https://github.com/Azure-Samples/python-worker-extension-timer).\\r\\n\\r\\nFor more information about VCS support in pip, see the [official pip VCS support documentation](https://pip.pypa.io/en/stable/cli/pip_install/#vcs-support).\\r\\n\\r\\n## Examples\\r\\n\\r\\n- You can view completed sample extension project from this article in the [python_worker_extension_timer](https://github.com/Azure-Samples/python-worker-extension-timer) sample repository.\\r\\n- OpenCensus integration is an open-source project that uses the extension interface to integrate telemetry tracing in Azure Functions Python apps. See the [opencensus-python-extensions-azure](https://github.com/census-ecosystem/opencensus-python-extensions-azure/tree/main/extensions/functions) repository to review the implementation of this Python worker extension.',\n",
       "  'metadata': {}},\n",
       " {'_id': '0d5cd213-f2d1-e5c8-f485-7b13a251e27f',\n",
       "  'title': 'How to disable functions in Azure Functions',\n",
       "  'text': '# How to disable functions in Azure Functions\\r\\n\\r\\nThis article explains how to disable a function in Azure Functions. To *disable* a function means to make the runtime ignore the event intended to trigger the function. This ability lets you prevent a specific function from running without having to modify and republish the entire function app.\\r\\n\\r\\nYou can disable a function in place by creating an app setting in the format `AzureWebJobs.<FUNCTION_NAME>.Disabled` set to `true`. You can create and modify this application setting in several ways, including by using the [Azure CLI](/en-us/cli/azure/), [Azure PowerShell](/en-us/powershell/azure/), and from your function\\'s **Overview** tab in the [Azure portal](https://portal.azure.com).\\r\\n\\r\\nChanges to application settings cause your function app to restart. For more information, see [App settings reference for Azure Functions](functions-app-settings).\\r\\n\\r\\n## Disable a function\\r\\n\\r\\nUse one of these modes to create an app setting that disables an example function named `QueueTrigger`:\\r\\n\\r\\n**Portal**\\r\\nUse the **Enable** and **Disable** buttons on the function\\'s **Overview** page. These buttons work by changing the value of the `AzureWebJobs.QueueTrigger.Disabled` app setting. The function-specific app setting is created the first time a function is disabled.\\r\\n\\r\\nEven when you publish to your function app from a local project, you can still use the portal to disable functions in the function app.\\r\\n\\r\\nNote\\r\\n\\r\\nDisabled functions can still be run by calling the REST endpoint using a master key. To learn more, see Run a disabled function. This means that a disabled function still runs when started from the **Test/Run** window in the portal using the **master (Host key)**.\\r\\n\\r\\n**Azure CLI**\\r\\nIn the Azure CLI, you use the [`az functionapp config appsettings set`](/en-us/cli/azure/functionapp/config/appsettings#az-functionapp-config-appsettings-set) command to create and modify the app setting. The following command disables a function named `QueueTrigger` by creating an app setting named `AzureWebJobs.QueueTrigger.Disabled` and setting it to `true`.\\r\\n\\r\\n```azurecli\\r\\naz functionapp config appsettings set --name <FUNCTION_APP_NAME> \\\\\\r\\n--resource-group <RESOURCE_GROUP_NAME> \\\\\\r\\n--settings AzureWebJobs.QueueTrigger.Disabled=true\\r\\n```\\r\\n\\r\\nTo re-enable the function, rerun the same command with a value of `false`.\\r\\n\\r\\n```azurecli\\r\\naz functionapp config appsettings set --name <myFunctionApp> \\\\\\r\\n--resource-group <myResourceGroup> \\\\\\r\\n--settings AzureWebJobs.QueueTrigger.Disabled=false\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\nThe [`Update-AzFunctionAppSetting`](/en-us/powershell/module/az.functions/update-azfunctionappsetting) command adds or updates an application setting. The following command disables a function named `QueueTrigger` by creating an app setting named `AzureWebJobs.QueueTrigger.Disabled` and setting it to `true`.\\r\\n\\r\\n```azurepowershell\\r\\nUpdate-AzFunctionAppSetting -Name <FUNCTION_APP_NAME> -ResourceGroupName <RESOURCE_GROUP_NAME> -AppSetting @{\"AzureWebJobs.QueueTrigger.Disabled\" = \"true\"}\\r\\n```\\r\\n\\r\\nTo re-enable the function, rerun the same command with a value of `false`.\\r\\n\\r\\n```azurepowershell\\r\\nUpdate-AzFunctionAppSetting -Name <FUNCTION_APP_NAME> -ResourceGroupName <RESOURCE_GROUP_NAME> -AppSetting @{\"AzureWebJobs.QueueTrigger.Disabled\" = \"false\"}\\r\\n```\\r\\n\\r\\n## Disable functions in a slot\\r\\n\\r\\nBy default, app settings also apply to apps running in deployment slots. You can, however, override the app setting used by the slot by setting a slot-specific app setting. For example, you might want a function to be active in production but not during deployment testing. It\\'s common to disable timer triggered functions in slots to prevent simultaneous executions.\\r\\n\\r\\nTo disable a function only in the staging slot:\\r\\n\\r\\n**Portal**\\r\\nNavigate to the slot instance of your function app by selecting **Deployment slots** under **Deployment**, choosing your slot, and selecting **Functions** in the slot instance. Choose your function, then use the **Enable** and **Disable** buttons on the function\\'s **Overview** page. These buttons work by changing the value of the `AzureWebJobs.<FUNCTION_NAME>.Disabled` app setting. This function-specific setting is created the first time you disable the function.\\r\\n\\r\\nYou can also directly add the app setting named `AzureWebJobs.<FUNCTION_NAME>.Disabled` with value of `true` in the **Configuration** for the slot instance. When you add a slot-specific app setting, make sure to check the **Deployment slot setting** box. This option maintains the setting value with the slot during swaps.\\r\\n\\r\\n**Azure CLI**\\r\\n\\r\\n```azurecli\\r\\naz functionapp config appsettings set --name <FUNCTION_APP_NAME> \\\\\\r\\n--resource-group <RESOURCE_GROUP_NAME> --slot <SLOT_NAME> \\\\\\r\\n--slot-settings AzureWebJobs.QueueTrigger.Disabled=true\\r\\n```\\r\\n\\r\\nTo re-enable the function, rerun the same command with a value of `false`.\\r\\n\\r\\n```azurecli\\r\\naz functionapp config appsettings set --name <myFunctionApp> \\\\\\r\\n--resource-group <myResourceGroup> --slot <SLOT_NAME> \\\\\\r\\n--slot-settings AzureWebJobs.QueueTrigger.Disabled=false\\r\\n```\\r\\n\\r\\n**Azure PowerShell**\\r\\nAzure PowerShell currently doesn\\'t support this functionality.\\r\\n\\r\\nTo learn more, see [Azure Functions Deployment slots](functions-deployment-slots).\\r\\n\\r\\n## Run a disabled function\\r\\n\\r\\nYou can still cause a disabled function to run by supplying the master access key (`_master`) in a REST request to the endpoint URL of the disabled function. In this way, you can develop and validate functions in Azure in a disabled state while preventing them from being accessed by others. Using any other type of key in the request returns an HTTP 404 response.\\r\\n\\r\\nCaution\\r\\n\\r\\nDue to the elevated permissions in your function app granted by the master key, you shouldn\\'t share this key with third parties or distribute it in native client applications. Use caution when choosing the admin HTTP access level for your function endpoints.\\r\\n\\r\\nTo learn more about the master key, see [Understand keys](function-keys-how-to#understand-keys). To learn more about calling non-HTTP triggered functions, see [Manually run a non HTTP-triggered function](functions-manually-run-non-http).\\r\\n\\r\\n## Disable functions locally\\r\\n\\r\\nFunctions can be disabled in the same way when running locally. To disable a function named `QueueTrigger`, add an entry to the Values collection in the local.settings.json file, as follows:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"IsEncrypted\": false,\\r\\n  \"Values\": {\\r\\n    \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\\r\\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\", \\r\\n    \"AzureWebJobs.QueueTrigger.Disabled\": true\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n## Considerations\\r\\n\\r\\nKeep the following considerations in mind when you disable functions:\\r\\n\\r\\n- When you disable an HTTP triggered function by using the methods described in this article, the endpoint can still be accessed when running on your local computer and in the portal.\\r\\n- At this time, function names that contain a hyphen (`-`) can\\'t be disabled when running on Linux. If you plan to disable your functions when running on Linux, don\\'t use hyphens in your function names.',\n",
       "  'metadata': {}},\n",
       " {'_id': 'a73dc11f-a9bc-a240-6220-c86d525721d2',\n",
       "  'title': 'Differences between in-process and isolate worker process .NET Azure Functions',\n",
       "  'text': \"# Differences between the isolated worker model and the in-process model for .NET on Azure Functions\\r\\n\\r\\nThere are two execution models for .NET functions:\\r\\n\\r\\n| Execution model | Description |\\r\\n| --- | --- |\\r\\n| **Isolated worker model** | Your function code runs in a separate .NET worker process. Use with [supported versions of .NET and .NET Framework](dotnet-isolated-process-guide#supported-versions). To learn more, see [Develop .NET isolated worker process functions](dotnet-isolated-process-guide). |\\r\\n| **In-process model** | Your function code runs in the same process as the Functions host process. Supports only [Long Term Support (LTS) versions of .NET](functions-dotnet-class-library#supported-versions). To learn more, see [Develop .NET class library functions](functions-dotnet-class-library). |\\r\\n\\r\\nImportant\\r\\n\\r\\n[Support will end for the in-process model on November 10, 2026](https://aka.ms/azure-functions-retirements/in-process-model). We highly recommend that you [migrate your apps to the isolated worker model](/en-us/azure/azure-functions/migrate-dotnet-to-isolated-model) for full support.\\r\\n\\r\\nThis article describes the current state of the functional and behavioral differences between the two models. To migrate from the in-process model to the isolated worker model, see [Migrate .NET apps from the in-process model to the isolated worker model](migrate-dotnet-to-isolated-model).\\r\\n\\r\\n## Execution model comparison table\\r\\n\\r\\nUse the following table to compare feature and functional differences between the two models:\\r\\n\\r\\n| Feature/behavior | Isolated worker model | In-process model^3^ |\\r\\n| --- | --- | --- |\\r\\n| Supported .NET versions | Long Term Support (LTS) versions,Standard Term Support (STS) versions,.NET Framework | Long Term Support (LTS) versions, ending with .NET 8 |\\r\\n| Core packages | [Microsoft.Azure.Functions.Worker](https://www.nuget.org/packages/Microsoft.Azure.Functions.Worker/)[Microsoft.Azure.Functions.Worker.Sdk](https://www.nuget.org/packages/Microsoft.Azure.Functions.Worker.Sdk) | [Microsoft.NET.Sdk.Functions](https://www.nuget.org/packages/Microsoft.NET.Sdk.Functions/) |\\r\\n| Binding extension packages | [Microsoft.Azure.Functions.Worker.Extensions.*](https://www.nuget.org/packages?q=Microsoft.Azure.Functions.Worker.Extensions) | [Microsoft.Azure.WebJobs.Extensions.*](https://www.nuget.org/packages?q=Microsoft.Azure.WebJobs.Extensions) |\\r\\n| Durable Functions | [Supported](durable/durable-functions-dotnet-isolated-overview) | [Supported](durable/durable-functions-overview) |\\r\\n| Model types exposed by bindings | Simple typesJSON serializable typesArrays/enumerations[Service SDK types](dotnet-isolated-process-guide#sdk-types)^4^ | Simple types[JSON serializable](/en-us/dotnet/api/system.text.json.jsonserializeroptions) typesArrays/enumerationsService SDK types^4^ |\\r\\n| HTTP trigger model types | [HttpRequestData](/en-us/dotnet/api/microsoft.azure.functions.worker.http.httprequestdata?view=azure-dotnet&amp;preserve-view=true) / [HttpResponseData](/en-us/dotnet/api/microsoft.azure.functions.worker.http.httpresponsedata?view=azure-dotnet&amp;preserve-view=true)[HttpRequest](/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest) / [IActionResult](/en-us/dotnet/api/microsoft.aspnetcore.mvc.iactionresult) (using [ASP.NET Core integration](dotnet-isolated-process-guide#aspnet-core-integration))^5^ | [HttpRequest](/en-us/dotnet/api/microsoft.aspnetcore.http.httprequest) / [IActionResult](/en-us/dotnet/api/microsoft.aspnetcore.mvc.iactionresult)^5^[HttpRequestMessage](/en-us/dotnet/api/system.net.http.httprequestmessage) / [HttpResponseMessage](/en-us/dotnet/api/system.net.http.httpresponsemessage) |\\r\\n| Output binding interactions | Return values in an expanded model with: - single or [multiple outputs](dotnet-isolated-process-guide#multiple-output-bindings) - arrays of outputs | Return values (single output only),`out` parameters,`IAsyncCollector` |\\r\\n| Imperative bindings^1^ | Not supported - instead [work with SDK types directly](dotnet-isolated-process-guide#register-azure-clients) | [Supported](functions-dotnet-class-library#binding-at-runtime) |\\r\\n| Dependency injection | [Supported](dotnet-isolated-process-guide#dependency-injection) (improved model consistent with .NET ecosystem) | [Supported](functions-dotnet-dependency-injection) |\\r\\n| Middleware | [Supported](dotnet-isolated-process-guide#middleware) | Not supported |\\r\\n| Logging | [`ILogger<T>`](/en-us/dotnet/api/microsoft.extensions.logging.logger-1)/[`ILogger`](/en-us/dotnet/api/microsoft.extensions.logging.ilogger) obtained from [FunctionContext](/en-us/dotnet/api/microsoft.azure.functions.worker.functioncontext) or via [dependency injection](dotnet-isolated-process-guide#dependency-injection) | [`ILogger`](/en-us/dotnet/api/microsoft.extensions.logging.ilogger) passed to the function[`ILogger<T>`](/en-us/dotnet/api/microsoft.extensions.logging.logger-1) via [dependency injection](functions-dotnet-dependency-injection) |\\r\\n| Application Insights dependencies | [Supported](dotnet-isolated-process-guide#application-insights) | [Supported](functions-monitoring#dependencies) |\\r\\n| Cancellation tokens | [Supported](dotnet-isolated-process-guide#cancellation-tokens) | [Supported](functions-dotnet-class-library#cancellation-tokens) |\\r\\n| Cold start times^2^ | [Configurable optimizations](dotnet-isolated-process-guide#performance-optimizations) | Optimized |\\r\\n| ReadyToRun | [Supported](dotnet-isolated-process-guide#readytorun) | [Supported](functions-dotnet-class-library#readytorun) |\\r\\n| [Flex Consumption] | [Supported](flex-consumption-plan#supported-language-stack-versions) | Not supported |\\r\\n| .NET Aspire | [Preview](dotnet-isolated-process-guide#net-aspire-preview) | Not supported |\\r\\n\\r\\n^1^ When you need to interact with a service using parameters determined at runtime, using the corresponding service SDKs directly is recommended over using imperative bindings. The SDKs are less verbose, cover more scenarios, and have advantages for error handling and debugging purposes. This recommendation applies to both models.\\r\\n\\r\\n^2^ Cold start times could be additionally impacted on Windows when using some preview versions of .NET due to just-in-time loading of preview frameworks. This impact applies to both the in-process and out-of-process models but can be noticeable when comparing across different versions. This delay for preview versions isn't present on Linux plans.\\r\\n\\r\\n^3^ C# Script functions also run in-process and use the same libraries as in-process class library functions. For more information, see the [Azure Functions C# script (.csx) developer reference](functions-reference-csharp).\\r\\n\\r\\n^4^ Service SDK types include types from the [Azure SDK for .NET](/en-us/dotnet/azure/sdk/azure-sdk-for-dotnet) such as [BlobClient](/en-us/dotnet/api/azure.storage.blobs.blobclient).\\r\\n\\r\\n^5^ ASP.NET Core types are not supported for .NET Framework.\\r\\n\\r\\n## Supported versions\\r\\n\\r\\nVersions of the Functions runtime support specific versions of .NET. To learn more about Functions versions, see [Azure Functions runtime versions overview](functions-versions). Version support also depends on whether your functions run in-process or isolated worker process.\\r\\n\\r\\nNote\\r\\n\\r\\nTo learn how to change the Functions runtime version used by your function app, see [view and update the current runtime version](set-runtime-version#view-the-current-runtime-version).\\r\\n\\r\\nThe following table shows the highest level of .NET or .NET Framework that can be used with a specific version of Functions.\\r\\n\\r\\n| Functions runtime version | [Isolated worker model](dotnet-isolated-process-guide) | [In-process model](functions-dotnet-class-library)^4^ |\\r\\n| --- | --- | --- |\\r\\n| Functions 4.x^1^ | .NET 9.0.NET 8.0.NET Framework 4.8^2^ | .NET 8.0 |\\r\\n| Functions 1.x^3^ | n/a | .NET Framework 4.8 |\\r\\n\\r\\n^1^ .NET 6 was previously supported on both models but reached the [end of official support](https://dotnet.microsoft.com/platform/support/policy) on November 12, 2024. .NET 7 was previously supported on the isolated worker model but reached the [end of official support](https://dotnet.microsoft.com/platform/support/policy) on May 14, 2024.\\r\\n\\r\\n^2^ The build process also requires the [.NET SDK](https://dotnet.microsoft.com/download).\\r\\n\\r\\n^3^ Support ends for version 1.x of the Azure Functions runtime on September 14, 2026. For more information, see [this support announcement](https://aka.ms/azure-functions-retirements/hostv1). For continued full support, you should [migrate your apps to version 4.x](migrate-version-1-version-4).\\r\\n\\r\\n^4^ Support ends for the in-process model on November 10, 2026. For more information, see [this support announcement](https://aka.ms/azure-functions-retirements/in-process-model). For continued full support, you should [migrate your apps to the isolated worker model](migrate-dotnet-to-isolated-model).\\r\\n\\r\\nFor the latest news about Azure Functions releases, including the removal of specific older minor versions, monitor [Azure App Service announcements](https://github.com/Azure/app-service-announcements/issues).\",\n",
       "  'metadata': {}},\n",
       " {'_id': '2bad2db5-5cd7-ebd7-0d05-03ed117b6330',\n",
       "  'title': 'Guide for running C# Azure Functions in an isolated worker process',\n",
       "  'text': '# Guide for running C# Azure Functions in the isolated worker model\\r\\n\\r\\nThis article is an introduction to working with Azure Functions in .NET, using the isolated worker model. This model allows your project to target versions of .NET independently of other runtime components. For information about specific .NET versions supported, see supported version.\\r\\n\\r\\nUse the following links to get started right away building .NET isolated worker model functions.\\r\\n\\r\\n| Getting started | Concepts | Samples |\\r\\n| --- | --- | --- |\\r\\n| - [Using Visual Studio Code](create-first-function-vs-code-csharp?tabs=isolated-process)<br>- [Using command line tools](create-first-function-cli-csharp?tabs=isolated-process)<br>- [Using Visual Studio](functions-create-your-first-function-visual-studio?tabs=isolated-process) | - [Hosting options](functions-scale)<br>- [Monitoring](functions-monitoring) | - [Reference samples](https://github.com/Azure/azure-functions-dotnet-worker/tree/main/samples) |\\r\\n\\r\\nTo learn just about deploying an isolated worker model project to Azure, see Deploy to Azure Functions.\\r\\n\\r\\n## Benefits of the isolated worker model\\r\\n\\r\\nThere are two modes in which you can run your .NET class library functions: either [in the same process](functions-dotnet-class-library) as the Functions host runtime (*in-process*) or in an isolated worker process. When your .NET functions run in an isolated worker process, you can take advantage of the following benefits:\\r\\n\\r\\n- **Fewer conflicts:** Because your functions run in a separate process, assemblies used in your app don\\'t conflict with different versions of the same assemblies used by the host process.\\r\\n- **Full control of the process**: You control the start-up of the app, which means that you can manage the configurations used and the middleware started.\\r\\n- **Standard dependency injection:** Because you have full control of the process, you can use current .NET behaviors for dependency injection and incorporating middleware into your function app.\\r\\n- **.NET version flexibility:** Running outside of the host process means that your functions can run on versions of .NET not natively supported by the Functions runtime, including the .NET Framework.\\r\\n\\r\\nIf you have an existing C# function app that runs in-process, you need to migrate your app to take advantage of these benefits. For more information, see [Migrate .NET apps from the in-process model to the isolated worker model](migrate-dotnet-to-isolated-model).\\r\\n\\r\\nFor a comprehensive comparison between the two modes, see [Differences between in-process and isolate worker process .NET Azure Functions](dotnet-isolated-in-process-differences).\\r\\n\\r\\n## Supported versions\\r\\n\\r\\nVersions of the Functions runtime support specific versions of .NET. To learn more about Functions versions, see [Azure Functions runtime versions overview](functions-versions). Version support also depends on whether your functions run in-process or isolated worker process.\\r\\n\\r\\nNote\\r\\n\\r\\nTo learn how to change the Functions runtime version used by your function app, see [view and update the current runtime version](set-runtime-version#view-the-current-runtime-version).\\r\\n\\r\\nThe following table shows the highest level of .NET or .NET Framework that can be used with a specific version of Functions.\\r\\n\\r\\n| Functions runtime version | [Isolated worker model](dotnet-isolated-process-guide) | [In-process model](functions-dotnet-class-library)^4^ |\\r\\n| --- | --- | --- |\\r\\n| Functions 4.x^1^ | .NET 9.0.NET 8.0.NET Framework 4.8^2^ | .NET 8.0 |\\r\\n| Functions 1.x^3^ | n/a | .NET Framework 4.8 |\\r\\n\\r\\n^1^ .NET 6 was previously supported on both models but reached the [end of official support](https://dotnet.microsoft.com/platform/support/policy) on November 12, 2024. .NET 7 was previously supported on the isolated worker model but reached the [end of official support](https://dotnet.microsoft.com/platform/support/policy) on May 14, 2024.\\r\\n\\r\\n^2^ The build process also requires the [.NET SDK](https://dotnet.microsoft.com/download).\\r\\n\\r\\n^3^ Support ends for version 1.x of the Azure Functions runtime on September 14, 2026. For more information, see [this support announcement](https://aka.ms/azure-functions-retirements/hostv1). For continued full support, you should [migrate your apps to version 4.x](migrate-version-1-version-4).\\r\\n\\r\\n^4^ Support ends for the in-process model on November 10, 2026. For more information, see [this support announcement](https://aka.ms/azure-functions-retirements/in-process-model). For continued full support, you should [migrate your apps to the isolated worker model](migrate-dotnet-to-isolated-model).\\r\\n\\r\\nFor the latest news about Azure Functions releases, including the removal of specific older minor versions, monitor [Azure App Service announcements](https://github.com/Azure/app-service-announcements/issues).\\r\\n\\r\\n## Project structure\\r\\n\\r\\nA .NET project for Azure Functions using the isolated worker model is basically a .NET console app project that targets a supported .NET runtime. The following are the basic files required in any .NET isolated project:\\r\\n\\r\\n- C# project file (.csproj) that defines the project and dependencies.\\r\\n- Program.cs file that\\'s the entry point for the app.\\r\\n- Any code files defining your functions.\\r\\n- [host.json](functions-host-json) file that defines configuration shared by functions in your project.\\r\\n- [local.settings.json](functions-develop-local#local-settings-file) file that defines environment variables used by your project when run locally on your machine.\\r\\n\\r\\nFor complete examples, see the [.NET 8 sample project](https://github.com/Azure/azure-functions-dotnet-worker/tree/main/samples/FunctionApp) and the [.NET Framework 4.8 sample project](https://github.com/Azure/azure-functions-dotnet-worker/tree/main/samples/NetFxWorker).\\r\\n\\r\\n## Package references\\r\\n\\r\\nA .NET project for Azure Functions using the isolated worker model uses a unique set of packages, for both core functionality and binding extensions.\\r\\n\\r\\n### Core packages\\r\\n\\r\\nThe following packages are required to run your .NET functions in an isolated worker process:\\r\\n\\r\\n- [Microsoft.Azure.Functions.Worker](https://www.nuget.org/packages/Microsoft.Azure.Functions.Worker/)\\r\\n- [Microsoft.Azure.Functions.Worker.Sdk](https://www.nuget.org/packages/Microsoft.Azure.Functions.Worker.Sdk/)\\r\\n\\r\\n#### Version 2.x\\r\\n\\r\\nThe 2.x versions of the core packages change the supported frameworks and bring in support for new .NET APIs from these later versions. When you target .NET 9 or later, your app needs to reference version 2.0.0 or later of both packages.\\r\\n\\r\\nWhen updating to the 2.x versions, note the following changes:\\r\\n\\r\\n- Starting with version 2.0.0 of [Microsoft.Azure.Functions.Worker.Sdk](https://www.nuget.org/packages/Microsoft.Azure.Functions.Worker.Sdk/):\\r\\n    - The SDK includes default configurations for [SDK container builds](/en-us/dotnet/core/docker/publish-as-container).\\r\\n    - The SDK includes support for [`dotnet run`](/en-us/dotnet/core/tools/dotnet-run) when the [Azure Functions Core Tools](functions-develop-local) is installed. On Windows, the Core Tools needs to be installed through a mechanism other than NPM.\\r\\n- Starting with version 2.0.0 of [Microsoft.Azure.Functions.Worker](https://www.nuget.org/packages/Microsoft.Azure.Functions.Worker/):\\r\\n    - This version adds support for `IHostApplicationBuilder`. Some examples in this guide include tabs to show alternatives using `IHostApplicationBuilder`. These examples require the 2.x versions.\\r\\n    - Service provider scope validation is included by default if run in a development environment. This behavior matches ASP.NET Core.\\r\\n    - The `EnableUserCodeException` option is enabled by default. The property is now marked as obsolete.\\r\\n    - The `IncludeEmptyEntriesInMessagePayload` option is enabled by default. With this option enabled, trigger payloads that represent collections always include empty entries. For example, if a message is sent without a body, an empty entry would still be present in `string[]` for the trigger data. The inclusion of empty entries facilitates cross-referencing with metadata arrays which the function may also reference. You can disable this behavior by setting `IncludeEmptyEntriesInMessagePayload` to `false` in the `WorkerOptions` service configuration.\\r\\n    - The `ILoggerExtensions` class is renamed to `FunctionsLoggerExtensions`. The rename prevents an ambiguous call error when using `LogMetric()` on an `ILogger` instance.\\r\\n    - For apps that use `HttpResponseData`, the `WriteAsJsonAsync()` method no longer will set the status code to `200 OK`. In 1.x, this overrode other error codes that had been set.\\r\\n- The 2.x versions drop .NET 5 TFM support.\\r\\n\\r\\n### Extension packages\\r\\n\\r\\nBecause .NET isolated worker process functions use different binding types, they require a unique set of binding extension packages.\\r\\n\\r\\nYou find these extension packages under [Microsoft.Azure.Functions.Worker.Extensions](https://www.nuget.org/packages?q=Microsoft.Azure.Functions.Worker.Extensions).\\r\\n\\r\\n## Start-up and configuration\\r\\n\\r\\nWhen you use the isolated worker model, you have access to the start-up of your function app, which is usually in `Program.cs`. You\\'re responsible for creating and starting your own host instance. As such, you also have direct access to the configuration pipeline for your app. With .NET Functions isolated worker process, you can much more easily add configurations, inject dependencies, and run your own middleware.\\r\\n\\r\\n**IHostBuilder**\\r\\nThe following code shows an example of a [HostBuilder](/en-us/dotnet/api/microsoft.extensions.hosting.hostbuilder) pipeline:\\r\\n\\r\\n```csharp\\r\\nvar host = new HostBuilder()\\r\\n    .ConfigureFunctionsWorkerDefaults()\\r\\n    .ConfigureServices(s =>\\r\\n    {\\r\\n        s.AddApplicationInsightsTelemetryWorkerService();\\r\\n        s.ConfigureFunctionsApplicationInsights();\\r\\n        s.AddSingleton<IHttpResponderService, DefaultHttpResponderService>();\\r\\n        s.Configure<LoggerFilterOptions>(options =>\\r\\n        {\\r\\n            // The Application Insights SDK adds a default logging filter that instructs ILogger to capture only Warning and more severe logs. Application Insights requires an explicit override.\\r\\n            // Log levels can also be configured using appsettings.json. For more information, see https://learn.microsoft.com/en-us/azure/azure-monitor/app/worker-service#ilogger-logs\\r\\n            LoggerFilterRule? toRemove = options.Rules.FirstOrDefault(rule => rule.ProviderName\\r\\n                == \"Microsoft.Extensions.Logging.ApplicationInsights.ApplicationInsightsLoggerProvider\");\\r\\n\\r\\n            if (toRemove is not null)\\r\\n            {\\r\\n                options.Rules.Remove(toRemove);\\r\\n            }\\r\\n        });\\r\\n    })\\r\\n    .Build();\\r\\n```\\r\\n\\r\\nThis code requires `using Microsoft.Extensions.DependencyInjection;`.\\r\\n\\r\\nBefore calling `Build()` on the `IHostBuilder`, you should:\\r\\n\\r\\n- Call either `ConfigureFunctionsWebApplication()` if using ASP.NET Core integration or `ConfigureFunctionsWorkerDefaults()` otherwise. See HTTP trigger for details on these options. If you\\'re writing your application using F#, some trigger and binding extensions require extra configuration. See the setup documentation for the [Blobs extension](functions-bindings-storage-blob#install-extension), the [Tables extension](functions-bindings-storage-table#install-extension), and the [Cosmos DB extension](functions-bindings-cosmosdb-v2#install-extension) when you plan to use these extensions in an F# app.\\r\\n- Configure any services or app configuration your project requires. See Configuration for details. If you\\'re planning to use Application Insights, you need to call `AddApplicationInsightsTelemetryWorkerService()` and `ConfigureFunctionsApplicationInsights()` in the `ConfigureServices()` delegate. See Application Insights for details.\\r\\n\\r\\nIf your project targets .NET Framework 4.8, you also need to add `FunctionsDebugger.Enable();` before creating the HostBuilder. It should be the first line of your `Main()` method. For more information, see Debugging when targeting .NET Framework.\\r\\n\\r\\nThe [HostBuilder](/en-us/dotnet/api/microsoft.extensions.hosting.hostbuilder) is used to build and return a fully initialized [`IHost`](/en-us/dotnet/api/microsoft.extensions.hosting.ihost) instance, which you run asynchronously to start your function app.\\r\\n\\r\\n```csharp\\r\\nawait host.RunAsync();\\r\\n```\\r\\n\\r\\n**IHostApplicationBuilder**\\r\\n*To use `IHostApplicationBuilder`, your app must use version 2.x or later of the core packages.*\\r\\n\\r\\nThe following code shows an example of an [IHostApplicationBuilder](/en-us/dotnet/api/microsoft.extensions.hosting.ihostapplicationbuilder) pipeline:\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Azure.Functions.Worker.Builder;\\r\\nusing Microsoft.Extensions.DependencyInjection;\\r\\nusing Microsoft.Extensions.Hosting;\\r\\n\\r\\nvar builder = FunctionsApplication.CreateBuilder(args);\\r\\n\\r\\nbuilder.Services\\r\\n    .AddApplicationInsightsTelemetryWorkerService()\\r\\n    .ConfigureFunctionsApplicationInsights()\\r\\n    .AddSingleton<IHttpResponderService, DefaultHttpResponderService>();\\r\\n\\r\\nbuilder.Logging.Services.Configure<LoggerFilterOptions>(options =>\\r\\n    {\\r\\n        // The Application Insights SDK adds a default logging filter that instructs ILogger to capture only Warning and more severe logs. Application Insights requires an explicit override.\\r\\n        // Log levels can also be configured using appsettings.json. For more information, see https://learn.microsoft.com/azure/azure-monitor/app/worker-service#ilogger-logs\\r\\n        LoggerFilterRule defaultRule = options.Rules.FirstOrDefault(rule => rule.ProviderName\\r\\n            == \"Microsoft.Extensions.Logging.ApplicationInsights.ApplicationInsightsLoggerProvider\");\\r\\n        if (defaultRule is not null)\\r\\n        {\\r\\n            options.Rules.Remove(defaultRule);\\r\\n        }\\r\\n    });\\r\\n\\r\\nvar host = builder.Build();\\r\\n```\\r\\n\\r\\nBefore calling `Build()` on the `IHostApplicationBuilder`, you should:\\r\\n\\r\\n- If you want to use ASP.NET Core integration, call `builder.ConfigureFunctionsWebApplication()`.\\r\\n- If you\\'re writing your application using F#, you might need to register some binding extensions. See the setup documentation for the [Blobs extension](functions-bindings-storage-blob#install-extension), the [Tables extension](functions-bindings-storage-table#install-extension), and the [Cosmos DB extension](functions-bindings-cosmosdb-v2#install-extension) when you plan to use these extensions in an F# app.\\r\\n- Configure any services or app configuration your project requires. See Configuration for details.\\r\\n- If you\\'re planning to use Application Insights, you need to call `AddApplicationInsightsTelemetryWorkerService()` and `ConfigureFunctionsApplicationInsights()` against the builder\\'s `Services` property. See Application Insights for details.\\r\\n\\r\\nIf your project targets .NET Framework 4.8, you also need to add `FunctionsDebugger.Enable();` before creating the HostBuilder. It should be the first line of your `Main()` method. For more information, see Debugging when targeting .NET Framework.\\r\\n\\r\\nThe [IHostApplicationBuilder](/en-us/dotnet/api/microsoft.extensions.hosting.ihostapplicationbuilder) is used to build and return a fully initialized [`IHost`](/en-us/dotnet/api/microsoft.extensions.hosting.ihost) instance, which you run asynchronously to start your function app.\\r\\n\\r\\n```csharp\\r\\nawait host.RunAsync();\\r\\n```\\r\\n\\r\\n### Configuration\\r\\n\\r\\nThe type of builder you use determines how you can configure the application.\\r\\n\\r\\n**IHostBuilder**\\r\\nThe [ConfigureFunctionsWorkerDefaults](/en-us/dotnet/api/microsoft.extensions.hosting.workerhostbuilderextensions.configurefunctionsworkerdefaults?view=azure-dotnet&amp;preserve-view=true#Microsoft_Extensions_Hosting_WorkerHostBuilderExtensions_ConfigureFunctionsWorkerDefaults_Microsoft_Extensions_Hosting_IHostBuilder_) method is used to add the settings required for the function app to run. The method includes the following functionality:\\r\\n\\r\\n- Default set of converters.\\r\\n- Set the default [JsonSerializerOptions](/en-us/dotnet/api/system.text.json.jsonserializeroptions) to ignore casing on property names.\\r\\n- Integrate with Azure Functions logging.\\r\\n- Output binding middleware and features.\\r\\n- Function execution middleware.\\r\\n- Default gRPC support.\\r\\n\\r\\n```csharp\\r\\n.ConfigureFunctionsWorkerDefaults()\\r\\n```\\r\\n\\r\\nHaving access to the host builder pipeline means that you can also set any app-specific configurations during initialization. You can call the [ConfigureAppConfiguration](/en-us/dotnet/api/microsoft.extensions.hosting.hostbuilder.configureappconfiguration) method on [HostBuilder](/en-us/dotnet/api/microsoft.extensions.hosting.hostbuilder) one or more times to add any configuration sources required by your code. To learn more about app configuration, see [Configuration in ASP.NET Core](/en-us/aspnet/core/fundamentals/configuration).\\r\\n\\r\\n**IHostApplicationBuilder**\\r\\nThe `FunctionsApplication.CreateBuilder()` method is used to add the settings required for the function app to run. The method includes the following functionality:\\r\\n\\r\\n- Default set of converters.\\r\\n- Set the default [JsonSerializerOptions](/en-us/dotnet/api/system.text.json.jsonserializeroptions) to ignore casing on property names.\\r\\n- Integrate with Azure Functions logging.\\r\\n- Output binding middleware and features.\\r\\n- Function execution middleware.\\r\\n- Default gRPC support.\\r\\n- Apply other defaults from [Host.CreateDefaultBuilder()](/en-us/dotnet/api/microsoft.extensions.hosting.host.createdefaultbuilder).\\r\\n\\r\\nHaving access to the builder pipeline means that you can also set any app-specific configurations during initialization. You can call extension methods on the builder\\'s `Configuration` property to add any configuration sources required by your code. To learn more about app configuration, see [Configuration in ASP.NET Core](/en-us/aspnet/core/fundamentals/configuration).\\r\\n\\r\\nThese configurations only apply to the worker code you author, and they don\\'t directly influence the configuration of the Functions host or triggers and bindings. To make changes to the functions host or trigger and binding configuration, you still need to use the [host.json file](functions-host-json).\\r\\n\\r\\nNote\\r\\n\\r\\nCustom configuration sources cannot be used for configuration of triggers and bindings. Trigger and binding configuration must be available to the Functions platform, and not just your application code. You can provide this configuration through the [application settings](../app-service/configure-common#configure-app-settings), [Key Vault references](../app-service/app-service-key-vault-references?toc=/azure/azure-functions/toc.json), or [App Configuration references](../app-service/app-service-configuration-references?toc=/azure/azure-functions/toc.json) features.\\r\\n\\r\\n### Dependency injection\\r\\n\\r\\nThe isolated worker model uses standard .NET mechanisms for injecting services.\\r\\n\\r\\n**IHostBuilder**\\r\\nWhen you use a `HostBuilder`, call [ConfigureServices](/en-us/dotnet/api/microsoft.extensions.hosting.hostbuilder.configureservices) on the host builder and use the extension methods on [IServiceCollection](/en-us/dotnet/api/microsoft.extensions.dependencyinjection.iservicecollection) to inject specific services. The following example injects a singleton service dependency:\\r\\n\\r\\n```csharp\\r\\n.ConfigureServices(services =>\\r\\n{\\r\\n    services.AddSingleton<IHttpResponderService, DefaultHttpResponderService>();\\r\\n})\\r\\n```\\r\\n\\r\\n**IHostApplicationBuilder**\\r\\nWhen you use an `IHostApplicationBuilder`, you can use its `Services` property to access the [IServiceCollection](/en-us/dotnet/api/microsoft.extensions.dependencyinjection.iservicecollection). The following example injects a singleton service dependency:\\r\\n\\r\\n```csharp\\r\\nbuilder.Services.AddSingleton<IHttpResponderService, DefaultHttpResponderService>();\\r\\n```\\r\\n\\r\\nThis code requires `using Microsoft.Extensions.DependencyInjection;`. To learn more, see [Dependency injection in ASP.NET Core](/en-us/aspnet/core/fundamentals/dependency-injection?view=aspnetcore-5.0&amp;preserve-view=true).\\r\\n\\r\\n#### Register Azure clients\\r\\n\\r\\nDependency injection can be used to interact with other Azure services. You can inject clients from the [Azure SDK for .NET](/en-us/dotnet/azure/sdk/azure-sdk-for-dotnet) using the [Microsoft.Extensions.Azure](https://www.nuget.org/packages/Microsoft.Extensions.Azure) package. After installing the package, [register the clients](/en-us/dotnet/azure/sdk/dependency-injection#register-clients) by calling `AddAzureClients()` on the service collection in `Program.cs`. The following example configures a [named client](/en-us/dotnet/azure/sdk/dependency-injection#configure-multiple-service-clients-with-different-names) for Azure Blobs:\\r\\n\\r\\n**IHostBuilder**\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Extensions.Azure;\\r\\nusing Microsoft.Extensions.DependencyInjection;\\r\\nusing Microsoft.Extensions.Hosting;\\r\\n\\r\\nvar host = new HostBuilder()\\r\\n    .ConfigureFunctionsWorkerDefaults()\\r\\n    .ConfigureServices((hostContext, services) =>\\r\\n    {\\r\\n        services.AddAzureClients(clientBuilder =>\\r\\n        {\\r\\n            clientBuilder.AddBlobServiceClient(hostContext.Configuration.GetSection(\"MyStorageConnection\"))\\r\\n                .WithName(\"copierOutputBlob\");\\r\\n        });\\r\\n    })\\r\\n    .Build();\\r\\n\\r\\nhost.Run();\\r\\n```\\r\\n\\r\\n**IHostApplicationBuilder**\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Azure.Functions.Worker.Builder;\\r\\nusing Microsoft.Extensions.Azure;\\r\\nusing Microsoft.Extensions.Hosting;\\r\\n\\r\\nvar builder = FunctionsApplication.CreateBuilder(args);\\r\\n\\r\\nbuilder.Services\\r\\n    .AddAzureClients(clientBuilder =>\\r\\n        {\\r\\n            clientBuilder.AddBlobServiceClient(builder.Configuration.GetSection(\"MyStorageConnection\"))\\r\\n                .WithName(\"copierOutputBlob\");\\r\\n        });\\r\\n\\r\\nbuilder.Build().Run();\\r\\n```\\r\\n\\r\\nThe following example shows how we can use this registration and SDK types to copy blob contents as a stream from one container to another using an injected client:\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Extensions.Azure;\\r\\nusing Microsoft.Extensions.Logging;\\r\\n\\r\\nnamespace MyFunctionApp\\r\\n{\\r\\n    public class BlobCopier\\r\\n    {\\r\\n        private readonly ILogger<BlobCopier> _logger;\\r\\n        private readonly BlobContainerClient _copyContainerClient;\\r\\n\\r\\n        public BlobCopier(ILogger<BlobCopier> logger, IAzureClientFactory<BlobServiceClient> blobClientFactory)\\r\\n        {\\r\\n            _logger = logger;\\r\\n            _copyContainerClient = blobClientFactory.CreateClient(\"copierOutputBlob\").GetBlobContainerClient(\"samples-workitems-copy\");\\r\\n            _copyContainerClient.CreateIfNotExists();\\r\\n        }\\r\\n\\r\\n        [Function(\"BlobCopier\")]\\r\\n        public async Task Run([BlobTrigger(\"samples-workitems/{name}\", Connection = \"MyStorageConnection\")] Stream myBlob, string name)\\r\\n        {\\r\\n            await _copyContainerClient.UploadBlobAsync(name, myBlob);\\r\\n            _logger.LogInformation($\"Blob {name} copied!\");\\r\\n        }\\r\\n\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe [`ILogger<T>`](/en-us/dotnet/api/microsoft.extensions.logging.ilogger-1) in this example was also obtained through dependency injection, so it\\'s registered automatically. To learn more about configuration options for logging, see Logging.\\r\\n\\r\\nTip\\r\\n\\r\\nThe example used a literal string for the name of the client in both `Program.cs` and the function. Consider instead using a shared constant string defined on the function class. For example, you could add `public const string CopyStorageClientName = nameof(_copyContainerClient);` and then reference `BlobCopier.CopyStorageClientName` in both locations. You could similarly define the configuration section name with the function rather than in `Program.cs`.\\r\\n\\r\\n### Middleware\\r\\n\\r\\nThe isolated worker model also supports middleware registration, again by using a model similar to what exists in ASP.NET. This model gives you the ability to inject logic into the invocation pipeline, and before and after functions execute.\\r\\n\\r\\nThe [ConfigureFunctionsWorkerDefaults](/en-us/dotnet/api/microsoft.extensions.hosting.workerhostbuilderextensions.configurefunctionsworkerdefaults?view=azure-dotnet&amp;preserve-view=true#Microsoft_Extensions_Hosting_WorkerHostBuilderExtensions_ConfigureFunctionsWorkerDefaults_Microsoft_Extensions_Hosting_IHostBuilder_) extension method has an overload that lets you register your own middleware, as you can see in the following example.\\r\\n\\r\\n**IHostBuilder**\\r\\n\\r\\n```csharp\\r\\nvar host = new HostBuilder()\\r\\n    .ConfigureFunctionsWorkerDefaults(workerApplication =>\\r\\n    {\\r\\n        // Register our custom middlewares with the worker\\r\\n\\r\\n        workerApplication.UseMiddleware<ExceptionHandlingMiddleware>();\\r\\n\\r\\n        workerApplication.UseMiddleware<MyCustomMiddleware>();\\r\\n\\r\\n        workerApplication.UseWhen<StampHttpHeaderMiddleware>((context) =>\\r\\n        {\\r\\n            // We want to use this middleware only for http trigger invocations.\\r\\n            return context.FunctionDefinition.InputBindings.Values\\r\\n                          .First(a => a.Type.EndsWith(\"Trigger\")).Type == \"httpTrigger\";\\r\\n        });\\r\\n    })\\r\\n    .Build();\\r\\n```\\r\\n\\r\\n**IHostApplicationBuilder**\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Azure.Functions.Worker.Builder;\\r\\nusing Microsoft.Extensions.DependencyInjection;\\r\\nusing Microsoft.Extensions.Hosting;\\r\\n\\r\\nvar builder = FunctionsApplication.CreateBuilder(args);\\r\\n\\r\\n// Register our custom middlewares with the worker\\r\\nbuilder\\r\\n    .UseMiddleware<ExceptionHandlingMiddleware>()\\r\\n    .UseMiddleware<MyCustomMiddleware>()\\r\\n    .UseWhen<StampHttpHeaderMiddleware>((context) =>\\r\\n    {\\r\\n        // We want to use this middleware only for http trigger invocations.\\r\\n        return context.FunctionDefinition.InputBindings.Values\\r\\n                        .First(a => a.Type.EndsWith(\"Trigger\")).Type == \"httpTrigger\";\\r\\n    });\\r\\n\\r\\nbuilder.Build().Run();\\r\\n```\\r\\n\\r\\nThe `UseWhen` extension method can be used to register a middleware that gets executed conditionally. You must pass to this method a predicate that returns a boolean value, and the middleware participates in the invocation processing pipeline when the return value of the predicate is `true`.\\r\\n\\r\\nThe following extension methods on [FunctionContext](/en-us/dotnet/api/microsoft.azure.functions.worker.functioncontext?view=azure-dotnet&amp;preserve-view=true) make it easier to work with middleware in the isolated model.\\r\\n\\r\\n| Method | Description |\\r\\n| --- | --- |\\r\\n| **`GetHttpRequestDataAsync`** | Gets the `HttpRequestData` instance when called by an HTTP trigger. This method returns an instance of `ValueTask<HttpRequestData?>`, which is useful when you want to read message data, such as request headers and cookies. |\\r\\n| **`GetHttpResponseData`** | Gets the `HttpResponseData` instance when called by an HTTP trigger. |\\r\\n| **`GetInvocationResult`** | Gets an instance of `InvocationResult`, which represents the result of the current function execution. Use the `Value` property to get or set the value as needed. |\\r\\n| **`GetOutputBindings`** | Gets the output binding entries for the current function execution. Each entry in the result of this method is of type `OutputBindingData`. You can use the `Value` property to get or set the value as needed. |\\r\\n| **`BindInputAsync`** | Binds an input binding item for the requested `BindingMetadata` instance. For example, you can use this method when you have a function with a `BlobInput` input binding that needs to be used by your middleware. |\\r\\n\\r\\nThis is an example of a middleware implementation that reads the `HttpRequestData` instance and updates the `HttpResponseData` instance during function execution:\\r\\n\\r\\n```csharp\\r\\ninternal sealed class StampHttpHeaderMiddleware : IFunctionsWorkerMiddleware\\r\\n{\\r\\n    public async Task Invoke(FunctionContext context, FunctionExecutionDelegate next)\\r\\n    {\\r\\n        var requestData = await context.GetHttpRequestDataAsync();\\r\\n\\r\\n        string correlationId;\\r\\n        if (requestData!.Headers.TryGetValues(\"x-correlationId\", out var values))\\r\\n        {\\r\\n            correlationId = values.First();\\r\\n        }\\r\\n        else\\r\\n        {\\r\\n            correlationId = Guid.NewGuid().ToString();\\r\\n        }\\r\\n\\r\\n        await next(context);\\r\\n\\r\\n        context.GetHttpResponseData()?.Headers.Add(\"x-correlationId\", correlationId);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThis middleware checks for the presence of a specific request header(x-correlationId), and when present uses the header value to stamp a response header. Otherwise, it generates a new GUID value and uses that for stamping the response header. For a more complete example of using custom middleware in your function app, see the [custom middleware reference sample](https://github.com/Azure/azure-functions-dotnet-worker/blob/main/samples/CustomMiddleware).\\r\\n\\r\\n### Customizing JSON serialization\\r\\n\\r\\nThe isolated worker model uses `System.Text.Json` by default. You can customize the behavior of the serializer by configuring services as part of your `Program.cs` file. This section covers general-purpose serialization and won\\'t influence HTTP trigger JSON serialization with ASP.NET Core integration, which must be configured separately.\\r\\n\\r\\n**IHostBuilder**\\r\\nThe following example shows this using `ConfigureFunctionsWebApplication`, but it will also work for `ConfigureFunctionsWorkerDefaults`:\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Extensions.DependencyInjection;\\r\\nusing Microsoft.Extensions.Hosting;\\r\\n\\r\\nvar host = new HostBuilder()\\r\\n    .ConfigureFunctionsWebApplication((IFunctionsWorkerApplicationBuilder builder) =>\\r\\n    {\\r\\n        builder.Services.Configure<JsonSerializerOptions>(jsonSerializerOptions =>\\r\\n        {\\r\\n            jsonSerializerOptions.PropertyNamingPolicy = JsonNamingPolicy.CamelCase;\\r\\n            jsonSerializerOptions.DefaultIgnoreCondition = JsonIgnoreCondition.WhenWritingNull;\\r\\n            jsonSerializerOptions.ReferenceHandler = ReferenceHandler.Preserve;\\r\\n\\r\\n            // override the default value\\r\\n            jsonSerializerOptions.PropertyNameCaseInsensitive = false;\\r\\n        });\\r\\n    })\\r\\n    .Build();\\r\\n\\r\\nhost.Run();\\r\\n```\\r\\n\\r\\n**IHostApplicationBuilder**\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Azure.Functions.Worker.Builder;\\r\\nusing Microsoft.Extensions.DependencyInjection;\\r\\nusing Microsoft.Extensions.Hosting;\\r\\n\\r\\nvar builder = FunctionsApplication.CreateBuilder(args);\\r\\n\\r\\nbuilder.ConfigureFunctionsWebApplication();\\r\\n\\r\\nbuilder.Services.Configure<JsonSerializerOptions>(jsonSerializerOptions =>\\r\\n    {\\r\\n        jsonSerializerOptions.PropertyNamingPolicy = JsonNamingPolicy.CamelCase;\\r\\n        jsonSerializerOptions.DefaultIgnoreCondition = JsonIgnoreCondition.WhenWritingNull;\\r\\n        jsonSerializerOptions.ReferenceHandler = ReferenceHandler.Preserve;\\r\\n\\r\\n        // override the default value\\r\\n        jsonSerializerOptions.PropertyNameCaseInsensitive = false;\\r\\n    });\\r\\n\\r\\nbuilder.Build().Run();\\r\\n```\\r\\n\\r\\nYou might want to instead use JSON.NET (`Newtonsoft.Json`) for serialization. To do this, you would install the [`Microsoft.Azure.Core.NewtonsoftJson`](https://www.nuget.org/packages/Microsoft.Azure.Core.NewtonsoftJson) package. Then, in your service registration, you would reassign the `Serializer` property on the `WorkerOptions` configuration. The following example shows this using `ConfigureFunctionsWebApplication`, but it will also work for `ConfigureFunctionsWorkerDefaults`:\\r\\n\\r\\n**IHostBuilder**\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Extensions.DependencyInjection;\\r\\nusing Microsoft.Extensions.Hosting;\\r\\n\\r\\nvar host = new HostBuilder()\\r\\n    .ConfigureFunctionsWebApplication((IFunctionsWorkerApplicationBuilder builder) =>\\r\\n    {\\r\\n        builder.Services.Configure<WorkerOptions>(workerOptions =>\\r\\n        {\\r\\n            var settings = NewtonsoftJsonObjectSerializer.CreateJsonSerializerSettings();\\r\\n            settings.ContractResolver = new CamelCasePropertyNamesContractResolver();\\r\\n            settings.NullValueHandling = NullValueHandling.Ignore;\\r\\n\\r\\n            workerOptions.Serializer = new NewtonsoftJsonObjectSerializer(settings);\\r\\n        });\\r\\n    })\\r\\n    .Build();\\r\\n\\r\\nhost.Run();\\r\\n```\\r\\n\\r\\n**IHostApplicationBuilder**\\r\\n\\r\\n```csharp\\r\\nusing Microsoft.Azure.Functions.Worker;\\r\\nusing Microsoft.Azure.Functions.Worker.Builder;\\r\\nusing Microsoft.Extensions.DependencyInjection;\\r\\nusing Microsoft.Extensions.Hosting;\\r\\n\\r\\nvar builder = FunctionsApplication.CreateBuilder(args);\\r\\n\\r\\nbuilder.ConfigureFunctionsWebApplication();\\r\\n\\r\\nbuilder.Services.Configure<WorkerOptions>(workerOptions =>\\r\\n    {\\r\\n        var settings = NewtonsoftJsonObjectSerializer.CreateJsonSerializerSettings();\\r\\n        settings.ContractResolver = new CamelCasePropertyNamesContractResolver();\\r\\n        settings.NullValueHandling = NullValueHandling.Ignore;\\r\\n\\r\\n        workerOptions.Serializer = new NewtonsoftJsonObjectSerializer(settings);\\r\\n    });\\r\\n\\r\\nbuilder.Build().Run();\\r\\n```\\r\\n\\r\\n## Methods recognized as functions\\r\\n\\r\\nA function method is a public method of a public class with a `Function` attribute applied to the method and a trigger attribute applied to an input parameter, as shown in the following example:\\r\\n\\r\\n```csharp\\r\\n[Function(nameof(QueueFunction))]\\r\\n[QueueOutput(\"output-queue\")]\\r\\npublic string[] Run([QueueTrigger(\"input-queue\")] Album myQueueItem, FunctionContext context)\\r\\n```\\r\\n\\r\\nThe trigger attribute specifies the trigger type and binds input data to a method parameter. The previous example function is triggered by a queue message, and the queue message is passed to the method in the `myQueueItem` parameter.\\r\\n\\r\\nThe `Function` attribute marks the method as a function entry point. The name must be unique within a project, start with a letter and only contain letters, numbers, `_`, and `-`, up to 127 characters in length. Project templates often create a method named `Run`, but the method name can be any valid C# method name. The method must be a public member of a public class. It should generally be an instance method so that services can be passed in via dependency injection.\\r\\n\\r\\n## Function parameters\\r\\n\\r\\nHere are some of the parameters that you can include as part of a function method signature:\\r\\n\\r\\n- Bindings, which are marked as such by decorating the parameters as attributes. The function must contain exactly one trigger parameter.\\r\\n- An execution context object, which provides information about the current invocation.\\r\\n- A cancellation token, used for graceful shutdown.\\r\\n\\r\\n### Execution context\\r\\n\\r\\n.NET isolated passes a [FunctionContext](/en-us/dotnet/api/microsoft.azure.functions.worker.functioncontext?view=azure-dotnet&amp;preserve-view=true) object to your function methods. This object lets you get an [`ILogger`](/en-us/dotnet/api/microsoft.extensions.logging.ilogger) instance to write to the logs by calling the [GetLogger](/en-us/dotnet/api/microsoft.azure.functions.worker.functioncontextloggerextensions.getlogger) method and supplying a `categoryName` string. You can use this context to obtain an [`ILogger`](/en-us/dotnet/api/microsoft.extensions.logging.ilogger) without having to use dependency injection. To learn more, see Logging.\\r\\n\\r\\n### Cancellation tokens\\r\\n\\r\\nA function can accept a [CancellationToken](/en-us/dotnet/api/system.threading.cancellationtoken) parameter, which enables the operating system to notify your code when the function is about to be terminated. You can use this notification to make sure the function doesn\\'t terminate unexpectedly in a way that leaves data in an inconsistent state.\\r\\n\\r\\nCancellation tokens are supported in .NET functions when running in an isolated worker process. The following example raises an exception when a cancellation request is received:\\r\\n\\r\\n```csharp\\r\\n[Function(nameof(ThrowOnCancellation))]\\r\\npublic async Task ThrowOnCancellation(\\r\\n    [EventHubTrigger(\"sample-workitem-1\", Connection = \"EventHubConnection\")] string[] messages,\\r\\n    FunctionContext context,\\r\\n    CancellationToken cancellationToken)\\r\\n{\\r\\n    _logger.LogInformation(\"C# EventHub {functionName} trigger function processing a request.\", nameof(ThrowOnCancellation));\\r\\n\\r\\n    foreach (var message in messages)\\r\\n    {\\r\\n        cancellationToken.ThrowIfCancellationRequested();\\r\\n        await Task.Delay(6000); // task delay to simulate message processing\\r\\n        _logger.LogInformation(\"Message \\'{msg}\\' was processed.\", message);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe following example performs clean-up actions when a cancellation request is received:\\r\\n\\r\\n```csharp\\r\\n[Function(nameof(HandleCancellationCleanup))]\\r\\npublic async Task HandleCancellationCleanup(\\r\\n    [EventHubTrigger(\"sample-workitem-2\", Connection = \"EventHubConnection\")] string[] messages,\\r\\n    FunctionContext context,\\r\\n    CancellationToken cancellationToken)\\r\\n{\\r\\n    _logger.LogInformation(\"C# EventHub {functionName} trigger function processing a request.\", nameof(HandleCancellationCleanup));\\r\\n\\r\\n    foreach (var message in messages)\\r\\n    {\\r\\n        if (cancellationToken.IsCancellationRequested)\\r\\n        {\\r\\n            _logger.LogInformation(\"A cancellation token was received, taking precautionary actions.\");\\r\\n            // Take precautions like noting how far along you are with processing the batch\\r\\n            _logger.LogInformation(\"Precautionary activities complete.\");\\r\\n            break;\\r\\n        }\\r\\n\\r\\n        await Task.Delay(6000); // task delay to simulate message processing\\r\\n        _logger.LogInformation(\"Message \\'{msg}\\' was processed.\", message);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n## Bindings\\r\\n\\r\\nBindings are defined by using attributes on methods, parameters, and return types. Bindings can provide data as strings, arrays, and serializable types, such as plain old class objects (POCOs). For some binding extensions, you can also bind to service-specific types defined in service SDKs.\\r\\n\\r\\nFor HTTP triggers, see the HTTP trigger section.\\r\\n\\r\\nFor a complete set of reference samples using triggers and bindings with isolated worker process functions, see the [binding extensions reference sample](https://github.com/Azure/azure-functions-dotnet-worker/blob/main/samples',\n",
       "  'metadata': {}},\n",
       " {'_id': 'f7076227-1be1-bf72-0297-c2aef74c01cc',\n",
       "  'title': 'Azure Storage provider for Durable Functions',\n",
       "  'text': '# Azure Storage provider (Azure Functions)\\r\\n\\r\\nThis document describes the characteristics of the Durable Functions Azure Storage provider, with a focus on performance and scalability aspects. The Azure Storage provider is the default provider. It stores instance states and queues in an Azure Storage (classic) account.\\r\\n\\r\\nNote\\r\\n\\r\\nFor more information on the supported storage providers for Durable Functions and how they compare, see the [Durable Functions storage providers](durable-functions-storage-providers) documentation.\\r\\n\\r\\nIn the Azure Storage provider, all function execution is driven by Azure Storage queues. Orchestration and entity status and history are stored in Azure Tables. Azure Blobs and blob leases are used to distribute orchestration instances and entities across multiple app instances (also known as *workers* or simply *VMs*). This section goes into more detail on the various Azure Storage artifacts and how they impact performance and scalability.\\r\\n\\r\\n## Storage representation\\r\\n\\r\\nA [task hub](durable-functions-task-hubs) durably persists all instance states and all messages. For a quick overview of how these are used to track the progress of an orchestration, see the [task hub execution example](durable-functions-task-hubs#execution-example).\\r\\n\\r\\nThe Azure Storage provider represents the task hub in storage using the following components:\\r\\n\\r\\n- Between two and three Azure Tables. Two tables are used to represent histories and instance states. If the Table Partition Manager is enabled, then a third table is introduced to store partition information.\\r\\n- One Azure Queue stores the activity messages.\\r\\n- One or more Azure Queues store the instance messages. Each of these so-called *control queues* represents a [partition](durable-functions-perf-and-scale#partition-count) that is assigned a subset of all instance messages, based on the hash of the instance ID.\\r\\n- A few extra blob containers used for lease blobs and/or large messages.\\r\\n\\r\\nFor example, a task hub named `xyz` with `PartitionCount = 4` contains the following queues and tables:\\r\\n\\r\\nNext, we describe these components and the role they play in more detail.\\r\\n\\r\\n### History table\\r\\n\\r\\nThe **History** table is an Azure Storage table that contains the history events for all orchestration instances within a task hub. The name of this table is in the form *TaskHubName*History. As instances run, new rows are added to this table. The partition key of this table is derived from the instance ID of the orchestration. Instance IDs are random by default, ensuring optimal distribution of internal partitions in Azure Storage. The row key for this table is a sequence number used for ordering the history events.\\r\\n\\r\\nWhen an orchestration instance needs to run, the corresponding rows of the History table are loaded into memory using a range query within a single table partition. These *history events* are then replayed into the orchestrator function code to get it back into its previously checkpointed state. The use of execution history to rebuild state in this way is influenced by the [Event Sourcing pattern](/en-us/azure/architecture/patterns/event-sourcing).\\r\\n\\r\\nTip\\r\\n\\r\\nOrchestration data stored in the History table includes output payloads from activity and sub-orchestrator functions. Payloads from external events are also stored in the History table. Because the full history is loaded into memory every time an orchestrator needs to execute, a large enough history can result in significant memory pressure on a given VM. The length and size of the orchestration history can be reduced by splitting large orchestrations into multiple sub-orchestrations or by reducing the size of outputs returned by the activity and sub-orchestrator functions it calls. Alternatively, you can reduce memory usage by lowering per-VM [concurrency throttles](durable-functions-perf-and-scale#concurrency-throttles) to limit how many orchestrations are loaded into memory concurrently.\\r\\n\\r\\n### Instances table\\r\\n\\r\\nThe **Instances** table contains the statuses of all orchestration and entity instances within a task hub. As instances are created, new rows are added to this table. The partition key of this table is the orchestration instance ID or entity key and the row key is an empty string. There is one row per orchestration or entity instance.\\r\\n\\r\\nThis table is used to satisfy [instance query requests from code](durable-functions-instance-management#query-instances) as well as [status query HTTP API](durable-functions-http-api#get-instance-status) calls. It is kept eventually consistent with the contents of the **History** table mentioned previously. The use of a separate Azure Storage table to efficiently satisfy instance query operations in this way is influenced by the [Command and Query Responsibility Segregation (CQRS) pattern](/en-us/azure/architecture/patterns/cqrs).\\r\\n\\r\\nTip\\r\\n\\r\\nThe partitioning of the *Instances* table allows it to store millions of orchestration instances without any noticeable impact on runtime performance or scale. However, the number of instances can have a significant impact on [multi-instance query](durable-functions-instance-management#query-all-instances) performance. To control the amount of data stored in these tables, consider periodically [purging old instance data](durable-functions-instance-management#purge-instance-history).\\r\\n\\r\\n### Partitions table\\r\\n\\r\\nNote\\r\\n\\r\\nThis table is shown in the task hub only when `Table Partition Manager` is enabled. To apply it, configure `useTablePartitionManagement` setting in your app\\'s [host.json](durable-functions-bindings?tabs=2x-durable-functions#host-json).\\r\\n\\r\\nThe **Partitions** table stores the status of partitions for the Durable Functions app and is used to distribute partitions across your app\\'s workers. There is one row per partition.\\r\\n\\r\\n### Queues\\r\\n\\r\\nOrchestrator, entity, and activity functions are all triggered by internal queues in the function app\\'s task hub. Using queues in this way provides reliable \"at-least-once\" message delivery guarantees. There are two types of queues in Durable Functions: the **control queue** and the **work-item queue**.\\r\\n\\r\\n#### The work-item queue\\r\\n\\r\\nThere is one work-item queue per task hub in Durable Functions. It\\'s a basic queue and behaves similarly to any other `queueTrigger` queue in Azure Functions. This queue is used to trigger stateless *activity functions* by dequeueing a single message at a time. Each of these messages contains activity function inputs and additional metadata, such as which function to execute. When a Durable Functions application scales out to multiple VMs, these VMs all compete to acquire tasks from the work-item queue.\\r\\n\\r\\n#### Control queue(s)\\r\\n\\r\\nThere are multiple *control queues* per task hub in Durable Functions. A *control queue* is more sophisticated than the simpler work-item queue. Control queues are used to trigger the stateful orchestrator and entity functions. Because the orchestrator and entity function instances are stateful singletons, it\\'s important that each orchestration or entity is only processed by one worker at a time. To achieve this constraint, each orchestration instance or entity is assigned to a single control queue. These control queues are load balanced across workers to ensure that each queue is only processed by one worker at a time. More details on this behavior can be found in subsequent sections.\\r\\n\\r\\nControl queues contain a variety of orchestration lifecycle message types. Examples include [orchestrator control messages](durable-functions-instance-management), activity function *response* messages, and timer messages. As many as 32 messages will be dequeued from a control queue in a single poll. These messages contain payload data as well as metadata including which orchestration instance it is intended for. If multiple dequeued messages are intended for the same orchestration instance, they will be processed as a batch.\\r\\n\\r\\nControl queue messages are constantly polled using a background thread. The batch size of each queue poll is controlled by the `controlQueueBatchSize` setting in host.json and has a default of 32 (the maximum value supported by Azure Queues). The maximum number of prefetched control-queue messages that are buffered in memory is controlled by the `controlQueueBufferThreshold` setting in host.json. The default value for `controlQueueBufferThreshold` varies depending on a variety of factors, including the type of hosting plan. For more information on these settings, see the [host.json schema](../functions-host-json#durabletask) documentation.\\r\\n\\r\\nTip\\r\\n\\r\\nIncreasing the value for `controlQueueBufferThreshold` allows a single orchestration or entity to process events faster. However, increasing this value can also result in higher memory usage. The higher memory usage is partly due to pulling more messages off the queue and partly due to fetching more orchestration histories into memory. Reducing the value for `controlQueueBufferThreshold` can therefore be an effective way to reduce memory usage.\\r\\n\\r\\n#### Queue polling\\r\\n\\r\\nThe durable task extension implements a random exponential back-off algorithm to reduce the effect of idle-queue polling on storage transaction costs. When a message is found, the runtime immediately checks for another message. When no message is found, it waits for a period of time before trying again. After subsequent failed attempts to get a queue message, the wait time continues to increase until it reaches the maximum wait time, which defaults to 30 seconds.\\r\\n\\r\\nThe maximum polling delay is configurable via the `maxQueuePollingInterval` property in the [host.json file](../functions-host-json#durabletask). Setting this property to a higher value could result in higher message processing latencies. Higher latencies would be expected only after periods of inactivity. Setting this property to a lower value could result in [higher storage costs](durable-functions-billing#azure-storage-transactions) due to increased storage transactions.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen running in the Azure Functions Consumption and Premium plans, the [Azure Functions Scale Controller](../event-driven-scaling) will poll each control and work-item queue once every 10 seconds. This additional polling is necessary to determine when to activate function app instances and to make scale decisions. At the time of writing, this 10 second interval is constant and cannot be configured.\\r\\n\\r\\n#### Orchestration start delays\\r\\n\\r\\nOrchestrations instances are started by putting an `ExecutionStarted` message in one of the task hub\\'s control queues. Under certain conditions, you may observe multi-second delays between when an orchestration is scheduled to run and when it actually starts running. During this time interval, the orchestration instance remains in the `Pending` state. There are two potential causes of this delay:\\r\\n\\r\\n- **Backlogged control queues**: If the control queue for this instance contains a large number of messages, it may take time before the `ExecutionStarted` message is received and processed by the runtime. Message backlogs can happen when orchestrations are processing lots of events concurrently. Events that go into the control queue include orchestration start events, activity completions, durable timers, termination, and external events. If this delay happens under normal circumstances, consider creating a new task hub with a larger number of partitions. Configuring more partitions will cause the runtime to create more control queues for load distribution. Each partition corresponds to 1:1 with a control queue, with a maximum of 16 partitions.\\r\\n- **Back off polling delays**: Another common cause of orchestration delays is the previously described back-off polling behavior for control queues. However, this delay is only expected when an app is scaled out to two or more instances. If there is only one app instance or if the app instance that starts the orchestration is also the same instance that is polling the target control queue, then there will not be a queue polling delay. Back off polling delays can be reduced by updating the **host.json** settings, as described previously.\\r\\n\\r\\n### Blobs\\r\\n\\r\\nIn most cases, Durable Functions doesn\\'t use Azure Storage Blobs to persist data. However, queues and tables have [size limits](../../azure-resource-manager/management/azure-subscription-service-limits#azure-queue-storage-limits) that can prevent Durable Functions from persisting all of the required data into a storage row or queue message. For example, when a piece of data that needs to be persisted to a queue is greater than 45 KB when serialized, Durable Functions will compress the data and store it in a blob instead. When persisting data to blob storage in this way, Durable Function stores a reference to that blob in the table row or queue message. When Durable Functions needs to retrieve the data it will automatically fetch it from the blob. These blobs are stored in the blob container `<taskhub>-largemessages`.\\r\\n\\r\\n#### Performance considerations\\r\\n\\r\\nThe extra compression and blob operation steps for large messages can be expensive in terms of CPU and I/O latency costs. Additionally, Durable Functions needs to load persisted data in memory, and may do so for many different function executions at the same time. As a result, persisting large data payloads can cause high memory usage as well. To minimize memory overhead, consider persisting large data payloads manually (for example, in blob storage) and instead pass around references to this data. This way your code can load the data only when needed to avoid redundant loads during [orchestrator function replays](durable-functions-orchestrations#reliability). However, storing payloads to local disks is *not* recommended since on-disk state is not guaranteed to be available since functions may execute on different VMs throughout their lifetimes.\\r\\n\\r\\n### Storage account selection\\r\\n\\r\\nThe queues, tables, and blobs used by Durable Functions are created in a configured Azure Storage account. The account to use can be specified using the `durableTask/storageProvider/connectionStringName` setting (or `durableTask/azureStorageConnectionStringName` setting in Durable Functions 1.x) in the **host.json** file.\\r\\n\\r\\n#### Durable Functions 2.x\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"extensions\": {\\r\\n    \"durableTask\": {\\r\\n      \"storageProvider\": {\\r\\n        \"connectionStringName\": \"MyStorageAccountAppSetting\"\\r\\n      }\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n#### Durable Functions 1.x\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"extensions\": {\\r\\n    \"durableTask\": {\\r\\n      \"azureStorageConnectionStringName\": \"MyStorageAccountAppSetting\"\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nIf not specified, the default `AzureWebJobsStorage` storage account is used. For performance-sensitive workloads, however, configuring a non-default storage account is recommended. Durable Functions uses Azure Storage heavily, and using a dedicated storage account isolates Durable Functions storage usage from the internal usage by the Azure Functions host.\\r\\n\\r\\nNote\\r\\n\\r\\nStandard general purpose Azure Storage accounts are required when using the Azure Storage provider. All other storage account types are not supported. We highly recommend using legacy v1 general purpose storage accounts for Durable Functions. The newer v2 storage accounts can be significantly more expensive for Durable Functions workloads. For more information on Azure Storage account types, see the [Storage account overview](../../storage/common/storage-account-overview) documentation.\\r\\n\\r\\n### Orchestrator scale-out\\r\\n\\r\\nWhile activity functions can be scaled out infinitely by adding more VMs elastically, individual orchestrator instances and entities are constrained to inhabit a single partition and the maximum number of partitions is bounded by the `partitionCount` setting in your `host.json`.\\r\\n\\r\\nNote\\r\\n\\r\\nGenerally speaking, orchestrator functions are intended to be lightweight and should not require large amounts of computing power. It is therefore not necessary to create a large number of control-queue partitions to get great throughput for orchestrations. Most of the heavy work should be done in stateless activity functions, which can be scaled out infinitely.\\r\\n\\r\\nThe number of control queues is defined in the **host.json** file. The following example host.json snippet sets the `durableTask/storageProvider/partitionCount` property (or `durableTask/partitionCount` in Durable Functions 1.x) to `3`. Note that there are as many control queues as there are partitions.\\r\\n\\r\\n#### Durable Functions 2.x\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"extensions\": {\\r\\n    \"durableTask\": {\\r\\n      \"storageProvider\": {\\r\\n        \"partitionCount\": 3\\r\\n      }\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n#### Durable Functions 1.x\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"extensions\": {\\r\\n    \"durableTask\": {\\r\\n      \"partitionCount\": 3\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nA task hub can be configured with between 1 and 16 partitions. If not specified, the default partition count is **4**.\\r\\n\\r\\nDuring low traffic scenarios, your application will be scaled-in, so partitions will be managed by a small number of workers. As an example, consider the diagram below.\\r\\n\\r\\nIn the previous diagram, we see that orchestrators 1 through 6 are load balanced across partitions. Similarly, partitions, like activities, are load balanced across workers. Partitions are load-balanced across workers regardless of the number of orchestrators that get started.\\r\\n\\r\\nIf you\\'re running on the Azure Functions Consumption or Elastic Premium plans, or if you have load-based auto-scaling configured, more workers will get allocated as traffic increases and partitions will eventually load balance across all workers. If we continue to scale out, eventually each partition will eventually be managed by a single worker. Activities, on the other hand, will continue to be load-balanced across all workers. This is shown in the image below.\\r\\n\\r\\nThe upper-bound of the maximum number of concurrent *active* orchestrations at *any given time* is equal to the number of workers allocated to your application *times* your value for `maxConcurrentOrchestratorFunctions`. This upper-bound can be made more precise when your partitions are fully scaled-out across workers. When fully scaled-out, and since each worker will have only a single Functions host instance, the maximum number of *active* concurrent orchestrator instances will be equal to your number of partitions *times* your value for `maxConcurrentOrchestratorFunctions`.\\r\\n\\r\\nNote\\r\\n\\r\\nIn this context, *active* means that an orchestration or entity is loaded into memory and processing *new events*. If the orchestration or entity is waiting for more events, such as the return value of an activity function, it gets unloaded from memory and is no longer considered *active*. Orchestrations and entities will be subsequently reloaded into memory only when there are new events to process. There\\'s no practical maximum number of *total* orchestrations or entities that can run on a single VM, even if they\\'re all in the \"Running\" state. The only limitation is the number of *concurrently active* orchestration or entity instances.\\r\\n\\r\\nThe image below illustrates a fully scaled-out scenario where more orchestrators are added but some are inactive, shown in grey.\\r\\n\\r\\nDuring scale-out, control queue leases may be redistributed across Functions host instances to ensure that partitions are evenly distributed. These leases are internally implemented as Azure Blob storage leases and ensure that any individual orchestration instance or entity only runs on a single host instance at a time. If a task hub is configured with three partitions (and therefore three control queues), orchestration instances and entities can be load-balanced across all three lease-holding host instances. Additional VMs can be added to increase capacity for activity function execution.\\r\\n\\r\\nThe following diagram illustrates how the Azure Functions host interacts with the storage entities in a scaled out environment.\\r\\n\\r\\nAs shown in the previous diagram, all VMs compete for messages on the work-item queue. However, only three VMs can acquire messages from control queues, and each VM locks a single control queue.\\r\\n\\r\\nOrchestration instances and entities are distributed across all control queue instances. The distribution is done by hashing the instance ID of the orchestration or the entity name and key pair. Orchestration instance IDs by default are random GUIDs, ensuring that instances are equally distributed across all control queues.\\r\\n\\r\\nGenerally speaking, orchestrator functions are intended to be lightweight and should not require large amounts of computing power. It is therefore not necessary to create a large number of control queue partitions to get great throughput for orchestrations. Most of the heavy work should be done in stateless activity functions, which can be scaled out infinitely.\\r\\n\\r\\n## Extended sessions\\r\\n\\r\\nExtended sessions is a [caching mechanism](durable-functions-perf-and-scale#instance-caching) that keeps orchestrations and entities in memory even after they finish processing messages. The typical effect of enabling extended sessions is reduced I/O against the underlying durable store and overall improved throughput.\\r\\n\\r\\nYou can enable extended sessions by setting `durableTask/extendedSessionsEnabled` to `true` in the **host.json** file. The `durableTask/extendedSessionIdleTimeoutInSeconds` setting can be used to control how long an idle session will be held in memory:\\r\\n\\r\\n**Functions 2.0**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"extensions\": {\\r\\n    \"durableTask\": {\\r\\n      \"extendedSessionsEnabled\": true,\\r\\n      \"extendedSessionIdleTimeoutInSeconds\": 30\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n**Functions 1.0**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"durableTask\": {\\r\\n    \"extendedSessionsEnabled\": true,\\r\\n    \"extendedSessionIdleTimeoutInSeconds\": 30\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nThere are two potential downsides of this setting to be aware of:\\r\\n\\r\\n1. There\\'s an overall increase in function app memory usage because idle instances are not unloaded from memory as quickly.\\r\\n2. There can be an overall decrease in throughput if there are many concurrent, distinct, short-lived orchestrator or entity function executions.\\r\\n\\r\\nAs an example, if `durableTask/extendedSessionIdleTimeoutInSeconds` is set to 30 seconds, then a short-lived orchestrator or entity function episode that executes in less than 1 second still occupies memory for 30 seconds. It also counts against the `durableTask/maxConcurrentOrchestratorFunctions` quota mentioned previously, potentially preventing other orchestrator or entity functions from running.\\r\\n\\r\\nThe specific effects of extended sessions on orchestrator and entity functions are described in the next sections.\\r\\n\\r\\nNote\\r\\n\\r\\nExtended sessions are currently only supported in .NET languages, like C# (in-process model only) or F#. Setting `extendedSessionsEnabled` to `true` for other platforms can lead to runtime issues, such as silently failing to execute activity and orchestration-triggered functions.\\r\\n\\r\\n### Orchestrator function replay\\r\\n\\r\\nAs mentioned previously, orchestrator functions are replayed using the contents of the **History** table. By default, the orchestrator function code is replayed every time a batch of messages are dequeued from a control queue. Even if you are using the fan-out, fan-in pattern and are awaiting for all tasks to complete (for example, using `Task.WhenAll()` in .NET, `context.df.Task.all()` in JavaScript, or `context.task_all()` in Python), there will be replays that occur as batches of task responses are processed over time. When extended sessions are enabled, orchestrator function instances are held in memory longer and new messages can be processed without a full history replay.\\r\\n\\r\\nThe performance improvement of extended sessions is most often observed in the following situations:\\r\\n\\r\\n- When there are a limited number of orchestration instances running concurrently.\\r\\n- When orchestrations have large number of sequential actions (for example, hundreds of activity function calls) that complete quickly.\\r\\n- When orchestrations fan-out and fan-in a large number of actions that complete around the same time.\\r\\n- When orchestrator functions need to process large messages or do any CPU-intensive data processing.\\r\\n\\r\\nIn all other situations, there is typically no observable performance improvement for orchestrator functions.\\r\\n\\r\\nNote\\r\\n\\r\\nThese settings should only be used after an orchestrator function has been fully developed and tested. The default aggressive replay behavior can useful for detecting [orchestrator function code constraints](durable-functions-code-constraints) violations at development time, and is therefore disabled by default.\\r\\n\\r\\n### Performance targets\\r\\n\\r\\nThe following table shows the expected *maximum* throughput numbers for the scenarios described in the [Performance Targets](durable-functions-perf-and-scale#performance-targets) section of the [Performance and Scale](durable-functions-perf-and-scale) article.\\r\\n\\r\\n\"Instance\" refers to a single instance of an orchestrator function running on a single small ([A1](/en-us/azure/virtual-machines/sizes-previous-gen)) VM in Azure App Service. In all cases, it is assumed that extended sessions are enabled. Actual results may vary depending on the CPU or I/O work performed by the function code.\\r\\n\\r\\n| Scenario | Maximum throughput |\\r\\n| --- | --- |\\r\\n| Sequential activity execution | 5 activities per second, per instance |\\r\\n| Parallel activity execution (fan-out) | 100 activities per second, per instance |\\r\\n| Parallel response processing (fan-in) | 150 responses per second, per instance |\\r\\n| External event processing | 50 events per second, per instance |\\r\\n| Entity operation processing | 64 operations per second |\\r\\n\\r\\nIf you are not seeing the throughput numbers you expect and your CPU and memory usage appears healthy, check to see whether the cause is related to [the health of your storage account](../../storage/common/storage-monitoring-diagnosing-troubleshooting#troubleshooting-guidance). The Durable Functions extension can put significant load on an Azure Storage account and sufficiently high loads may result in storage account throttling.\\r\\n\\r\\nTip\\r\\n\\r\\nIn some cases you can significantly increase the throughput of external events, activity fan-in, and entity operations by increasing the value of the `controlQueueBufferThreshold` setting in **host.json**. Increasing this value beyond its default causes the Durable Task Framework storage provider to use more memory to prefetch these events more aggressively, reducing delays associated with dequeueing messages from the Azure Storage control queues. For more information, see the [host.json](durable-functions-bindings#host-json) reference documentation.\\r\\n\\r\\n### Flex Consumption Plan\\r\\n\\r\\nThe [Flex Consumption plan](../flex-consumption-plan) is an Azure Functions hosting plan that provides many of the benefits of the Consumption plan, including a serverless billing model, while also adding useful features, such as private networking, instance memory size selection, and full support for managed identity authentication.\\r\\n\\r\\nAzure Storage is currently the only supported [storage provider](durable-functions-storage-providers) for Durable Functions when hosted in the Flex Consumption plan.\\r\\n\\r\\nYou should follow these performance recommendations when hosting Durable Functions in the Flex Consumption plan:\\r\\n\\r\\n- Set the [always ready instance count](../flex-consumption-how-to#set-always-ready-instance-counts) for the `durable` group to `1`. This ensures that there is always one instance ready to handle Durable Functions related requests, thus reducing the application\\'s cold start.\\r\\n- Reduce the [queue polling interval](durable-functions-azure-storage-provider#queue-polling) to 10 seconds or less. Since this plan type is more sensitive to queue polling delays, lowering the polling interval will help increase the frequency of polling operations, thus ensuring requests are handled faster. However, more frequent polling operations will lead to a higher Azure Storage account cost.\\r\\n\\r\\n### High throughput processing\\r\\n\\r\\nThe architecture of the Azure Storage backend puts certain limitations on the maximum theoretical performance and scalability of Durable Functions. If your testing shows that Durable Functions on Azure Storage won\\'t meet your throughput requirements, you should consider instead using the [Netherite storage provider for Durable Functions](durable-functions-storage-providers#netherite).\\r\\n\\r\\nTo compare the achievable throughput for various basic scenarios, see the section [Basic Scenarios](https://microsoft.github.io/durabletask-netherite/#/scenarios) of the Netherite storage provider documentation.\\r\\n\\r\\nThe Netherite storage backend was designed and developed by [Microsoft Research](https://www.microsoft.com/research). It uses [Azure Event Hubs](../../event-hubs/event-hubs-about) and the [FASTER](https://www.microsoft.com/research/project/faster/) database technology on top of [Azure Page Blobs](../../storage/blobs/storage-blob-pageblob-overview). The design of Netherite enables significantly higher-throughput processing of orchestrations and entities compared to other providers. In some benchmark scenarios, throughput was shown to increase by more than an order of magnitude when compared to the default Azure Storage provider.\\r\\n\\r\\nFor more information on the supported storage providers for Durable Functions and how they compare, see the [Durable Functions storage providers](durable-functions-storage-providers) documentation.',\n",
       "  'metadata': {}},\n",
       " {'_id': 'd34f87c1-47a7-207e-c2fe-4a22b0a41353',\n",
       "  'title': 'Durable Functions best practices and diagnostic tools',\n",
       "  'text': '# Durable Functions best practices and diagnostic tools\\r\\n\\r\\nThis article details some best practices when using Durable Functions. It also describes various tools to help diagnose problems during development, testing, and production use.\\r\\n\\r\\n## Best practices\\r\\n\\r\\n### Use the latest version of the Durable Functions extension and SDK\\r\\n\\r\\nThere are two components that a function app uses to execute Durable Functions. One is the *Durable Functions SDK* that allows you to write orchestrator, activity, and entity functions using your target programming language. The other is the *Durable extension*, which is the runtime component that actually executes the code. With the exception of .NET in-process apps, the SDK and the extension are versioned independently.\\r\\n\\r\\nStaying up to date with the latest extension and SDK ensures your application benefits from the latest performance improvements, features, and bug fixes. Upgrading to the latest versions also ensures that Microsoft can collect the latest diagnostic telemetry to help accelerate the investigation process when you open a support case with Azure.\\r\\n\\r\\n- See [Upgrade durable functions extension version](durable-functions-extension-upgrade) for instructions on getting the latest extension version.\\r\\n- To ensure you\\'re using the latest version of the SDK, check the package manager of the language you\\'re using.\\r\\n\\r\\n### Adhere to Durable Functions [code constraints](durable-functions-code-constraints)\\r\\n\\r\\nThe [replay](durable-functions-orchestrations#reliability) behavior of orchestrator code creates constraints on the type of code that you can write in an orchestrator function. An example of a constraint is that your orchestrator function must use deterministic APIs so that each time it’s replayed, it produces the same result.\\r\\n\\r\\nNote\\r\\n\\r\\nThe Durable Functions Roslyn Analyzer is a live code analyzer that guides C# users to adhere to Durable Functions specific code constraints. See [Durable Functions Roslyn Analyzer](durable-functions-roslyn-analyzer) for instructions on how to enable it on Visual Studio and Visual Studio Code.\\r\\n\\r\\n### Familiarize yourself with your programming language\\'s Azure Functions performance settings\\r\\n\\r\\n*Using default settings*, the language runtime you select may impose strict concurrency restrictions on your functions. For example: only allowing 1 function to execute at a time on a given VM. These restrictions can usually be relaxed by *fine tuning* the concurrency and performance settings of your language. If you\\'re looking to optimize the performance of your Durable Functions application, you will need to familiarize yourself with these settings.\\r\\n\\r\\nBelow is a non-exhaustive list of some of the languages that often benefit from fine tuning their performance and concurrency settings, and their guidelines for doing so.\\r\\n\\r\\n- [JavaScript](../functions-reference-node#scaling-and-concurrency)\\r\\n- [PowerShell](../functions-reference-powershell#concurrency)\\r\\n- [Python](../python-scale-performance-reference)\\r\\n\\r\\n### Guarantee unique Task Hub names per app\\r\\n\\r\\nMultiple Durable Function apps can share the same storage account. By default, the name of the app is used as the task hub name, which ensures that accidental sharing of task hubs won\\'t happen. If you need to explicitly configure task hub names for your apps in host.json, you must ensure that the names are [*unique*](durable-functions-task-hubs#multiple-function-apps). Otherwise, the multiple apps will compete for messages, which could result in undefined behavior, including orchestrations getting unexpectedly \"stuck\" in the Pending or Running state.\\r\\n\\r\\nThe only exception is if you deploy *copies* of the same app in [multiple regions](durable-functions-disaster-recovery-geo-distribution); in this case, you can use the same task hub for the copies.\\r\\n\\r\\n### Follow guidance when deploying code changes to running orchestrators\\r\\n\\r\\nIt\\'s inevitable that functions will be added, removed, and changed over the lifetime of an application. Examples of [common breaking changes](durable-functions-versioning) include changing activity or entity function signatures and changing orchestrator logic. These changes are a problem when they affect orchestrations that are still running. If deployed incorrectly, code changes could lead to orchestrations failing with a non-deterministic error, getting stuck indefinitely, performance degradation, etc. Refer to recommended [mitigation strategies](durable-functions-versioning#mitigation-strategies) when making code changes that may impact running orchestrations.\\r\\n\\r\\n### Keep function inputs and outputs as small as possible\\r\\n\\r\\nYou can run into memory issues if you provide large inputs and outputs to and from Durable Functions APIs.\\r\\n\\r\\nInputs and outputs to Durable Functions APIs are serialized into the orchestration history. This means that large inputs and outputs can, over time, greatly contribute to an orchestrator history growing unbounded, which risks causing memory exceptions during [replay](durable-functions-orchestrations#reliability).\\r\\n\\r\\nTo mitigate the impact of large inputs and outputs to APIs, you may choose to delegate some work to sub-orchestrators. This helps load balance the history memory burden from a single orchestrator to multiple ones, therefore keeping the memory footprint of individual histories small.\\r\\n\\r\\nThat said the best practice for dealing with *large* data is to keep it in external storage and to only materialize that data inside Activities, when needed. When taking this approach, instead of communicating the data itself as inputs and/or outputs of Durable Functions APIs, you can pass in some lightweight identifier that allows you to retrieve that data from external storage when needed in your Activities.\\r\\n\\r\\n### Keep Entity data small\\r\\n\\r\\nJust like for inputs and outputs to Durable Functions APIs, if an entity\\'s explicit state is too large, you may run into memory issues. In particular, an Entity state needs to be serialized and de-serialized from storage on any request, so large states add serialization latency to each invocation. Therefore, if an Entity needs to track large data, it\\'s recommended to offload the data to external storage and track some lightweight identifier in the entity that allows you to materialize the data from storage when needed.\\r\\n\\r\\n### Fine tune your Durable Functions concurrency settings\\r\\n\\r\\nA single worker instance can execute multiple work items concurrently to increase efficiency. However, processing too many work items concurrently risks exhausting resources like CPU capacity, network connections, etc. In many cases, this shouldn’t be a concern because scaling and limiting work items are handled automatically for you. That said, if you’re experiencing performance issues (such as orchestrators taking too long to finish, are stuck in pending, etc.) or are doing performance testing, you could [configure concurrency limits](durable-functions-perf-and-scale#configuration-of-throttles) in the host.json file.\\r\\n\\r\\nNote\\r\\n\\r\\nThis is not a replacement for fine-tuning the performance and concurrency settings of your language runtime in Azure Functions. The Durable Functions concurrency settings only determine how much work can be assigned to a given VM at a time, but it does not determine the degree of parallelism in processing that work inside the VM. The latter requires fine-tuning the language runtime performance settings.\\r\\n\\r\\n### Use unique names for your external events\\r\\n\\r\\nAs with activity functions, external events have an *at-least-once* delivery guarantee. This means that, under certain *rare* conditions (which may occur during restarts, scaling, crashes, etc.), your application may receive duplicates of the same external event. Therefore, we recommend that external events contain an ID that allows them to be manually de-duplicated in orchestrators.\\r\\n\\r\\nNote\\r\\n\\r\\nThe [MSSQL](durable-functions-storage-providers#mssql) storage provider consumes external events and updates orchestrator state transactionally, so in that backend there should be no risk of duplicate events, unlike with the default [Azure Storage storage provider](durable-functions-storage-providers). That said, it is still recommended that external events have unique names so that code is portable across backends.\\r\\n\\r\\n### Invest in stress testing\\r\\n\\r\\nAs with anything performance related, the ideal concurrency settings and architechture of your app ultimately depends on your application\\'s workload. Therefore, it\\'s recommended that users to invest in a performance testing harness that simulates their expected workload and to use it to run performance and reliability experiments for their app.\\r\\n\\r\\n### Avoid sensitive data in inputs, outputs, and exceptions\\r\\n\\r\\nInputs and outputs (including exceptions) to and from Durable Functions APIs are [durably persisted](durable-functions-serialization-and-persistence) in your [storage provider of choice](durable-functions-storage-providers). If those inputs, outputs, or exceptions contain sensitive data (such as secrets, connection strings, personally identifiable information, etc.) then anyone with read access to your storage provider\\'s resources would be able to obtain them. To safely deal with sensitive data, it is recommended for users to fetch that data *within activity functions* from either Azure Key Vault or environment variables, and to never communicate that data directly to orchestrators or entities. That should help prevent sensitive data from leaking into your storage resources.\\r\\n\\r\\nNote\\r\\n\\r\\nThis guidance also applies to the `CallHttp` orchestrator API, which also persists its request and response payloads in storage. If your target HTTP endpoints require authentication, which may be sensitive, it is recommended that users implement the HTTP Call themselves inside of an activity, or to use the [built-in managed identity support offered by `CallHttp`](durable-functions-http-features#managed-identities), which does not persist any credentials to storage.\\r\\n\\r\\nTip\\r\\n\\r\\nSimilarly, avoid logging data containing secrets as anyone with read access to your logs (for example in Application Insights), would be able to obtain those secrets.\\r\\n\\r\\n## Diagnostic tools\\r\\n\\r\\nThere are several tools available to help you diagnose problems.\\r\\n\\r\\n### Durable Functions and Durable Task Framework Logs\\r\\n\\r\\n#### Durable Functions Extension\\r\\n\\r\\nThe Durable extension emits tracking events that allow you to trace the end-to-end execution of an orchestration. These tracking events can be found and queried using the [Application Insights Analytics](/en-us/azure/azure-monitor/logs/log-query-overview) tool in the Azure portal. The verbosity of tracking data emitted can be configured in the `logger` (Functions 1.x) or `logging` (Functions 2.0) section of the host.json file. See [configuration details](durable-functions-diagnostics#functions-10).\\r\\n\\r\\n#### Durable Task Framework\\r\\n\\r\\nStarting in v2.3.0 of the Durable extension, logs emitted by the underlying Durable Task Framework (DTFx) are also available for collection. See [details on how to enable these logs](durable-functions-diagnostics#durable-task-framework-logging).\\r\\n\\r\\n### Azure portal\\r\\n\\r\\n#### Diagnose and solve problems\\r\\n\\r\\nAzure Function App Diagnostics is a useful resource on Azure portal for monitoring and diagnosing potential issues in your application. It also provides suggestions to help resolve problems based on the diagnosis. See [Azure Function App Diagnostics](function-app-diagnostics).\\r\\n\\r\\n#### Durable Functions Orchestration traces\\r\\n\\r\\nAzure portal provides orchestration trace details to help you understand the status of each orchestration instance and trace the end-to-end execution. When you look at the list of functions inside your Azure Functions app, you\\'ll see a **Monitor** column that contains links to the traces. You need to have Applications Insights enabled for your app to get this information.\\r\\n\\r\\n### Durable Functions Monitor Extension\\r\\n\\r\\nThis is a [Visual Studio Code extension](https://github.com/microsoft/DurableFunctionsMonitor) that provides a UI for monitoring, managing, and debugging your orchestration instances.\\r\\n\\r\\n### Roslyn Analyzer\\r\\n\\r\\nThe Durable Functions Roslyn Analyzer is a live code analyzer that guides C# users to adhere to Durable Functions specific [code constraints](durable-functions-code-constraints). See [Durable Functions Roslyn Analyzer](durable-functions-roslyn-analyzer) for instructions on how to enable it on Visual Studio and Visual Studio Code.\\r\\n\\r\\n## Support\\r\\n\\r\\nFor questions and support, you may open an issue in one of the GitHub repos below. When reporting a bug in Azure, including information such as affected instance IDs, time ranges in UTC showing the problem, the application name (if possible) and deployment region will greatly speed up investigations.\\r\\n\\r\\n- [Durable Functions extension and .NET in-process SDK](https://github.com/Azure/azure-functions-durable-extension/issues)\\r\\n- [.NET isolated SDK](https://github.com/microsoft/durabletask-dotnet/issues)\\r\\n- [Durable Functions for Java](https://github.com/microsoft/durabletask-java/issues)\\r\\n- [Durable Functions for JavaScript](https://github.com/Azure/azure-functions-durable-js/issues)\\r\\n- [Durable Functions for Python](https://github.com/Azure/azure-functions-durable-python/issues)',\n",
       "  'metadata': {}},\n",
       " {'_id': 'e99d0994-271a-8227-0b07-b5d3cb5f104d',\n",
       "  'title': 'Durable functions billing - Azure Functions',\n",
       "  'text': '# Durable Functions billing\\r\\n\\r\\n[Durable Functions](durable-functions-overview) is billed the same way as Azure Functions. For more information, see [Azure Functions pricing](https://azure.microsoft.com/pricing/details/functions/).\\r\\n\\r\\nWhen executing orchestrator functions in Azure Functions [Consumption plan](../consumption-plan), you need to be aware of some billing behaviors. The following sections describe these behaviors and their effect in more detail.\\r\\n\\r\\n## Orchestrator function replay billing\\r\\n\\r\\n[Orchestrator functions](durable-functions-orchestrations) might replay several times throughout the lifetime of an orchestration. Each replay is viewed by the Azure Functions runtime as a distinct function invocation. For this reason, in the Azure Functions Consumption plan you\\'re billed for each replay of an orchestrator function. Other plan types don\\'t charge for orchestrator function replay.\\r\\n\\r\\n## Awaiting and yielding in orchestrator functions\\r\\n\\r\\nWhen an orchestrator function waits for an asynchronous task to complete, the runtime considers that particular function invocation to be finished. The billing for the orchestrator function stops at that point. It doesn\\'t resume until the next orchestrator function replay. You aren\\'t billed for any time spent awaiting or yielding in an orchestrator function.\\r\\n\\r\\nNote\\r\\n\\r\\nFunctions calling other functions is considered by some to be a Serverless anti-pattern. This is because of a problem known as *double billing*. When a function calls another function directly, both run at the same time. The called function is actively running code while the calling function is waiting for a response. In this case, you must pay for the time the calling function spends waiting for the called function to run.\\r\\n\\r\\nThere is no double billing in orchestrator functions. An orchestrator function\\'s billing stops while it waits for the result of an activity function or sub-orchestration.\\r\\n\\r\\n## Durable HTTP polling\\r\\n\\r\\nOrchestrator functions can make long-running HTTP calls to external endpoints as described in the [HTTP features article](durable-functions-http-features). The *\"call HTTP\"* APIs might internally poll an HTTP endpoint while following the [asynchronous 202 pattern](durable-functions-http-features#http-202-handling).\\r\\n\\r\\nThere currently isn\\'t direct billing for internal HTTP polling operations. However, internal polling might cause the orchestrator function to periodically replay. You\\'ll be billed standard charges for these internal function replays.\\r\\n\\r\\n## Azure Storage transactions\\r\\n\\r\\nDurable Functions uses Azure Storage by default to keep state persistent, process messages, and manage partitions via blob leases. Because you own this storage account, any transaction costs are billed to your Azure subscription. For more information about the Azure Storage artifacts used by Durable Functions, see the [Task hubs article](durable-functions-task-hubs).\\r\\n\\r\\nSeveral factors contribute to the actual Azure Storage costs incurred by your Durable Functions app:\\r\\n\\r\\n- A single function app is associated with a single task hub, which shares a set of Azure Storage resources. These resources are used by all durable functions in a function app. The actual number of functions in the function app has no effect on Azure Storage transaction costs.\\r\\n- Each function app instance internally polls multiple queues in the storage account by using an exponential-backoff polling algorithm. An idle app instance polls the queues less often than does an active app, which results in fewer transaction costs. For more information about Durable Functions queue-polling behavior when using the Azure Storage provider, see the [queue-polling section](durable-functions-azure-storage-provider#queue-polling) of the Azure Storage provider documentation.\\r\\n- When running in the Azure Functions Consumption or Premium plans, the [Azure Functions scale controller](../event-driven-scaling) regularly polls all task-hub queues in the background. If a function app is under light to moderate scale, only a single scale controller instance will poll these queues. If the function app scales out to a large number of instances, more scale controller instances might be added. These additional scale controller instances can increase the total queue-transaction costs.\\r\\n- Each function app instance competes for a set of blob leases. These instances will periodically make calls to the Azure Blob service either to renew held leases or to attempt to acquire new leases. The task hub\\'s configured partition count determines the number of blob leases. Scaling out to a larger number of function app instances likely increases the Azure Storage transaction costs associated with these lease operations.\\r\\n\\r\\nYou can find more information on Azure Storage pricing in the [Azure Storage pricing](https://azure.microsoft.com/pricing/details/storage/) documentation.',\n",
       "  'metadata': {}},\n",
       " {'_id': '5fb958fe-a192-31ae-6e4d-39987049b887',\n",
       "  'title': 'Bindings for Durable Functions - Azure',\n",
       "  'text': '# Bindings for Durable Functions (Azure Functions) (programming-language-python)\\r\\n\\r\\nThe [Durable Functions](durable-functions-overview) extension introduces three trigger bindings that control the execution of orchestrator, entity, and activity functions. It also introduces an output binding that acts as a client for the Durable Functions runtime.\\r\\n\\r\\nMake sure to choose your Durable Functions development language at the top of the article.\\r\\n\\r\\nImportant\\r\\n\\r\\nThis article supports both Python v1 and Python v2 programming models for Durable Functions.\\r\\n\\r\\n## Python v2 programming model\\r\\n\\r\\nDurable Functions is supported in the new [Python v2 programming model](../functions-reference-python?pivots=python-mode-decorators). To use the v2 model, you must install the Durable Functions SDK, which is the PyPI package `azure-functions-durable`, version `1.2.2` or a later version. You must also check `host.json` to make sure your app is referencing [Extension Bundles](../functions-bindings-register#extension-bundles) version 4.x to use the v2 model with Durable Functions.\\r\\n\\r\\nYou can provide feedback and suggestions in the [Durable Functions SDK for Python repo](https://github.com/Azure/azure-functions-durable-python/issues).\\r\\n\\r\\n## Orchestration trigger\\r\\n\\r\\nThe orchestration trigger enables you to author [durable orchestrator functions](durable-functions-types-features-overview#orchestrator-functions). This trigger executes when a new orchestration instance is scheduled and when an existing orchestration instance receives an event. Examples of events that can trigger orchestrator functions include durable timer expirations, activity function responses, and events raised by external clients.\\r\\n\\r\\nAzure Functions supports two programming models for Python. The way that you define an orchestration trigger depends on your chosen programming model.\\r\\n\\r\\n**v2**\\r\\nThe Python v2 programming model lets you define an orchestration trigger using the `orchestration_trigger` decorator directly in your Python function code.\\r\\n\\r\\nIn the v2 model, the Durable Functions triggers and bindings are accessed from an instance of `DFApp`, which is a subclass of `FunctionApp` that additionally exports Durable Functions-specific decorators.\\r\\n\\r\\n**v1**\\r\\nWhen you write orchestrator functions in the Python v1 programming model, the orchestration trigger is defined by the following JSON object in the `bindings` array of the *function.json* file:\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"name\": \"<Name of input parameter in function signature>\",\\r\\n    \"orchestration\": \"<Optional - name of the orchestration>\",\\r\\n    \"type\": \"orchestrationTrigger\",\\r\\n    \"direction\": \"in\"\\r\\n}\\r\\n```\\r\\n\\r\\n- `orchestration` is the name of the orchestration that clients must use when they want to start new instances of this orchestrator function. This property is optional. If not specified, the name of the function is used.\\r\\n\\r\\nInternally, this trigger binding polls the configured durable store for new orchestration events, such as orchestration start events, durable timer expiration events, activity function response events, and external events raised by other functions.\\r\\n\\r\\n### Trigger behavior\\r\\n\\r\\nHere are some notes about the orchestration trigger:\\r\\n\\r\\n- **Single-threading** - A single dispatcher thread is used for all orchestrator function execution on a single host instance. For this reason, it\\'s important to ensure that orchestrator function code is efficient and doesn\\'t perform any I/O. It is also important to ensure that this thread does not do any async work except when awaiting on Durable Functions-specific task types.\\r\\n- **Poison-message handling** - There\\'s no poison message support in orchestration triggers.\\r\\n- **Message visibility** - Orchestration trigger messages are dequeued and kept invisible for a configurable duration. The visibility of these messages is renewed automatically as long as the function app is running and healthy.\\r\\n- **Return values** - Return values are serialized to JSON and persisted to the orchestration history table in Azure Table storage. These return values can be queried by the orchestration client binding, described later.\\r\\n\\r\\nWarning\\r\\n\\r\\nOrchestrator functions should never use any input or output bindings other than the orchestration trigger binding. Doing so has the potential to cause problems with the Durable Task extension because those bindings may not obey the single-threading and I/O rules. If you\\'d like to use other bindings, add them to an activity function called from your orchestrator function. For more information about coding constraints for orchestrator functions, see the [Orchestrator function code constraints](durable-functions-code-constraints) documentation.\\r\\n\\r\\nWarning\\r\\n\\r\\nOrchestrator functions should never be declared `async`.\\r\\n\\r\\n### Trigger usage\\r\\n\\r\\nThe orchestration trigger binding supports both inputs and outputs. Here are some things to know about input and output handling:\\r\\n\\r\\n- **inputs** - Orchestration triggers can be invoked with inputs, which are accessed through the context input object. All inputs must be JSON-serializable.\\r\\n- **outputs** - Orchestration triggers support output values as well as inputs. The return value of the function is used to assign the output value and must be JSON-serializable.\\r\\n\\r\\n### Trigger sample\\r\\n\\r\\nThe following example code shows what the simplest \"Hello World\" orchestrator function might look like. Note that this example orchestrator doesn\\'t actually schedule any tasks.\\r\\n\\r\\n**v2**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\nmyApp = df.DFApp(http_auth_level=func.AuthLevel.ANONYMOUS)\\r\\n\\r\\n@myApp.orchestration_trigger(context_name=\"context\")\\r\\ndef my_orchestrator(context):\\r\\n    result = yield context.call_activity(\"Hello\", \"Tokyo\")\\r\\n    return result\\r\\n```\\r\\n\\r\\n**v1**\\r\\n\\r\\n```python\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    input = context.get_input()\\r\\n    return f\"Hello {input[\\'name\\']}!\"\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n## Activity trigger\\r\\n\\r\\nThe activity trigger enables you to author functions that are called by orchestrator functions, known as [activity functions](durable-functions-types-features-overview#activity-functions).\\r\\n\\r\\nThe way that you define an activity trigger depends on your chosen programming model.\\r\\n\\r\\n**v2**\\r\\nUsing the `activity_trigger` decorator directly in your Python function code.\\r\\n\\r\\n**v1**\\r\\nThe activity trigger is defined by the following JSON object in the `bindings` array of *function.json*:\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"name\": \"<Name of input parameter in function signature>\",\\r\\n    \"activity\": \"<Optional - name of the activity>\",\\r\\n    \"type\": \"activityTrigger\",\\r\\n    \"direction\": \"in\"\\r\\n}\\r\\n```\\r\\n\\r\\n- `activity` is the name of the activity. This value is the name that orchestrator functions use to invoke this activity function. This property is optional. If not specified, the name of the function is used.\\r\\n\\r\\nInternally, this trigger binding polls the configured durable store for new activity execution events.\\r\\n\\r\\n### Trigger behavior\\r\\n\\r\\nHere are some notes about the activity trigger:\\r\\n\\r\\n- **Threading** - Unlike the orchestration trigger, activity triggers don\\'t have any restrictions around threading or I/O. They can be treated like regular functions.\\r\\n- **Poison-message handling** - There\\'s no poison message support in activity triggers.\\r\\n- **Message visibility** - Activity trigger messages are dequeued and kept invisible for a configurable duration. The visibility of these messages is renewed automatically as long as the function app is running and healthy.\\r\\n- **Return values** - Return values are serialized to JSON and persisted to the configured durable store.\\r\\n\\r\\n### Trigger usage\\r\\n\\r\\nThe activity trigger binding supports both inputs and outputs, just like the orchestration trigger. Here are some things to know about input and output handling:\\r\\n\\r\\n- **inputs** - Activity triggers can be invoked with inputs from an orchestrator function. All inputs must be JSON-serializable.\\r\\n- **outputs** - Activity functions support output values as well as inputs. The return value of the function is used to assign the output value and must be JSON-serializable.\\r\\n- **metadata** - .NET activity functions can bind to a `string instanceId` parameter to get the instance ID of the calling orchestration.\\r\\n\\r\\n### Trigger sample\\r\\n\\r\\nThe following example code shows what a simple `SayHello` activity function might look like.\\r\\n\\r\\n**v2**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\nmyApp = df.DFApp(http_auth_level=func.AuthLevel.ANONYMOUS)\\r\\n\\r\\n@myApp.activity_trigger(input_name=\"myInput\")\\r\\ndef my_activity(myInput: str):\\r\\n    return \"Hello \" + myInput\\r\\n```\\r\\n\\r\\n**v1**\\r\\n\\r\\n```python\\r\\ndef main(name: str) -> str:\\r\\n    return f\"Hello {name}!\"\\r\\n```\\r\\n\\r\\n### Using input and output bindings\\r\\n\\r\\nYou can use regular input and output bindings in addition to the activity trigger binding.\\r\\n\\r\\n## Orchestration client\\r\\n\\r\\nThe orchestration client binding enables you to write functions that interact with orchestrator functions. These functions are often referred to as [client functions](durable-functions-types-features-overview#client-functions). For example, you can act on orchestration instances in the following ways:\\r\\n\\r\\n- Start them.\\r\\n- Query their status.\\r\\n- Terminate them.\\r\\n- Send events to them while they\\'re running.\\r\\n- Purge instance history.\\r\\n\\r\\nThe way that you define a durable client trigger depends on your chosen programming model.\\r\\n\\r\\n**v2**\\r\\nUsing the `durable_client_input` decorator directly in your Python function code.\\r\\n\\r\\n**v1**\\r\\nThe durable client trigger is defined by the following JSON object in the `bindings` array of *function.json*:\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"name\": \"<Name of input parameter in function signature>\",\\r\\n    \"taskHub\": \"<Optional - name of the task hub>\",\\r\\n    \"connectionName\": \"<Optional - name of the connection string app setting>\",\\r\\n    \"type\": \"orchestrationClient\",\\r\\n    \"direction\": \"in\"\\r\\n}\\r\\n```\\r\\n\\r\\n- `taskHub` - Used in scenarios where multiple function apps share the same storage account but need to be isolated from each other. If not specified, the default value from `host.json` is used. This value must match the value used by the target orchestrator functions.\\r\\n- `connectionName` - The name of an app setting that contains a storage account connection string. The storage account represented by this connection string must be the same one used by the target orchestrator functions. If not specified, the default storage account connection string for the function app is used.\\r\\n\\r\\nNote\\r\\n\\r\\nIn most cases, we recommend that you omit these properties and rely on the default behavior.\\r\\n\\r\\n### Client usage\\r\\n\\r\\nYou must use the language-specific SDK to get access to a client object.\\r\\n\\r\\nHere\\'s an example queue-triggered function that starts a \"HelloWorld\" orchestration.\\r\\n\\r\\n**v2**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\nmyApp = df.DFApp(http_auth_level=func.AuthLevel.ANONYMOUS)\\r\\n\\r\\n@myApp.route(route=\"orchestrators/{functionName}\")\\r\\n@myApp.durable_client_input(client_name=\"client\")\\r\\nasync def durable_trigger(req: func.HttpRequest, client):\\r\\n    function_name = req.route_params.get(\\'functionName\\')\\r\\n    instance_id = await client.start_new(function_name)\\r\\n    response = client.create_check_status_response(req, instance_id)\\r\\n    return response\\r\\n```\\r\\n\\r\\n**v1**\\r\\n**`function.json`**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"name\": \"input\",\\r\\n      \"type\": \"queueTrigger\",\\r\\n      \"queueName\": \"durable-function-trigger\",\\r\\n      \"direction\": \"in\"\\r\\n    },\\r\\n    {\\r\\n      \"name\": \"starter\",\\r\\n      \"type\": \"durableClient\",\\r\\n      \"direction\": \"in\"\\r\\n    }\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\n**`__init__.py`**\\r\\n\\r\\n```python\\r\\nimport json\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\nasync def main(msg: func.QueueMessage, starter: str) -> None:\\r\\n    client = df.DurableOrchestrationClient(starter)\\r\\n    payload = msg.get_body().decode(\\'utf-8\\')\\r\\n    instance_id = await client.start_new(\"HelloWorld\", client_input=payload)\\r\\n```\\r\\n\\r\\nMore details on starting instances can be found in [Instance management](durable-functions-instance-management).\\r\\n\\r\\n## Entity trigger\\r\\n\\r\\nEntity triggers allow you to author [entity functions](durable-functions-entities). This trigger supports processing events for a specific entity instance.\\r\\n\\r\\nNote\\r\\n\\r\\nEntity triggers are available starting in Durable Functions 2.x.\\r\\n\\r\\nInternally, this trigger binding polls the configured durable store for new entity operations that need to be executed.\\r\\n\\r\\nThe way that you define a entity trigger depends on your chosen programming model.\\r\\n\\r\\n**v2**\\r\\nUsing the `entity_trigger` decorator directly in your Python function code.\\r\\n\\r\\n**v1**\\r\\nThe entity trigger is defined by the following JSON object in the `bindings` array of *function.json*:\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"name\": \"<Name of input parameter in function signature>\",\\r\\n    \"entityName\": \"<Optional - name of the entity>\",\\r\\n    \"type\": \"entityTrigger\",\\r\\n    \"direction\": \"in\"\\r\\n}\\r\\n```\\r\\n\\r\\nBy default, the name of an entity is the name of the function.\\r\\n\\r\\n### Trigger behavior\\r\\n\\r\\nHere are some notes about the entity trigger:\\r\\n\\r\\n- **Single-threaded**: A single dispatcher thread is used to process operations for a particular entity. If multiple messages are sent to a single entity concurrently, the operations will be processed one-at-a-time.\\r\\n- **Poison-message handling** - There\\'s no poison message support in entity triggers.\\r\\n- **Message visibility** - Entity trigger messages are dequeued and kept invisible for a configurable duration. The visibility of these messages is renewed automatically as long as the function app is running and healthy.\\r\\n- **Return values** - Entity functions don\\'t support return values. There are specific APIs that can be used to save state or pass values back to orchestrations.\\r\\n\\r\\nAny state changes made to an entity during its execution will be automatically persisted after execution has completed.\\r\\n\\r\\nFor more information and examples on defining and interacting with entity triggers, see the [Durable Entities](durable-functions-entities) documentation.\\r\\n\\r\\n## Entity client\\r\\n\\r\\nThe entity client binding enables you to asynchronously trigger entity functions. These functions are sometimes referred to as [client functions](durable-functions-types-features-overview#client-functions).\\r\\n\\r\\nThe way that you define a entity client depends on your chosen programming model.\\r\\n\\r\\n**v2**\\r\\nUsing the `durable_client_input` decorator directly in your Python function code.\\r\\n\\r\\n**v1**\\r\\nThe entity client is defined by the following JSON object in the `bindings` array of *function.json*:\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"name\": \"<Name of input parameter in function signature>\",\\r\\n    \"taskHub\": \"<Optional - name of the task hub>\",\\r\\n    \"connectionName\": \"<Optional - name of the connection string app setting>\",\\r\\n    \"type\": \"durableClient\",\\r\\n    \"direction\": \"in\"\\r\\n}\\r\\n```\\r\\n\\r\\n- `taskHub` - Used in scenarios where multiple function apps share the same storage account but need to be isolated from each other. If not specified, the default value from `host.json` is used. This value must match the value used by the target entity functions.\\r\\n- `connectionName` - The name of an app setting that contains a storage account connection string. The storage account represented by this connection string must be the same one used by the target entity functions. If not specified, the default storage account connection string for the function app is used.\\r\\n\\r\\nNote\\r\\n\\r\\nIn most cases, we recommend that you omit the optional properties and rely on the default behavior.\\r\\n\\r\\nFor more information and examples on interacting with entities as a client, see the [Durable Entities](durable-functions-entities#access-entities) documentation.\\r\\n\\r\\n## host.json settings\\r\\n\\r\\nConfiguration settings for [Durable Functions](durable-functions-overview).\\r\\n\\r\\nNote\\r\\n\\r\\nAll major versions of Durable Functions are supported on all versions of the Azure Functions runtime. However, the schema of the host.json configuration is slightly different depending on the version of the Azure Functions runtime and the Durable Functions extension version you use. The following examples are for use with Azure Functions 2.0 and 3.0. In both examples, if you\\'re using Azure Functions 1.0, the available settings are the same, but the \"durableTask\" section of the host.json should go in the root of the host.json configuration instead of as a field under \"extensions\".\\r\\n\\r\\n**Durable Functions 2.x**\\r\\n\\r\\n```json\\r\\n{\\r\\n \"extensions\": {\\r\\n  \"durableTask\": {\\r\\n    \"hubName\": \"MyTaskHub\",\\r\\n    \"storageProvider\": {\\r\\n      \"connectionStringName\": \"AzureWebJobsStorage\",\\r\\n      \"controlQueueBatchSize\": 32,\\r\\n      \"controlQueueBufferThreshold\": 256,\\r\\n      \"controlQueueVisibilityTimeout\": \"00:05:00\",\\r\\n      \"maxQueuePollingInterval\": \"00:00:30\",\\r\\n      \"partitionCount\": 4,\\r\\n      \"trackingStoreConnectionStringName\": \"TrackingStorage\",\\r\\n      \"trackingStoreNamePrefix\": \"DurableTask\",\\r\\n      \"useLegacyPartitionManagement\": true,\\r\\n      \"useTablePartitionManagement\": false,\\r\\n      \"workItemQueueVisibilityTimeout\": \"00:05:00\",\\r\\n    },\\r\\n    \"tracing\": {\\r\\n      \"traceInputsAndOutputs\": false,\\r\\n      \"traceReplayEvents\": false,\\r\\n    },\\r\\n    \"notifications\": {\\r\\n      \"eventGrid\": {\\r\\n        \"topicEndpoint\": \"https://topic_name.westus2-1.eventgrid.azure.net/api/events\",\\r\\n        \"keySettingName\": \"EventGridKey\",\\r\\n        \"publishRetryCount\": 3,\\r\\n        \"publishRetryInterval\": \"00:00:30\",\\r\\n        \"publishEventTypes\": [\\r\\n          \"Started\",\\r\\n          \"Completed\",\\r\\n          \"Failed\",\\r\\n          \"Terminated\"\\r\\n        ]\\r\\n      }\\r\\n    },\\r\\n    \"maxConcurrentActivityFunctions\": 10,\\r\\n    \"maxConcurrentOrchestratorFunctions\": 10,\\r\\n    \"extendedSessionsEnabled\": false,\\r\\n    \"extendedSessionIdleTimeoutInSeconds\": 30,\\r\\n    \"useAppLease\": true,\\r\\n    \"useGracefulShutdown\": false,\\r\\n    \"maxEntityOperationBatchSize\": 50,\\r\\n    \"storeInputsInOrchestrationHistory\": false\\r\\n  }\\r\\n }\\r\\n}\\r\\n```\\r\\n\\r\\n**Durable Functions 1.x**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"extensions\": {\\r\\n    \"durableTask\": {\\r\\n      \"hubName\": \"MyTaskHub\",\\r\\n      \"controlQueueBatchSize\": 32,\\r\\n      \"partitionCount\": 4,\\r\\n      \"controlQueueVisibilityTimeout\": \"00:05:00\",\\r\\n      \"workItemQueueVisibilityTimeout\": \"00:05:00\",\\r\\n      \"maxConcurrentActivityFunctions\": 10,\\r\\n      \"maxConcurrentOrchestratorFunctions\": 10,\\r\\n      \"maxQueuePollingInterval\": \"00:00:30\",\\r\\n      \"azureStorageConnectionStringName\": \"AzureWebJobsStorage\",\\r\\n      \"trackingStoreConnectionStringName\": \"TrackingStorage\",\\r\\n      \"trackingStoreNamePrefix\": \"DurableTask\",\\r\\n      \"traceInputsAndOutputs\": false,\\r\\n      \"logReplayEvents\": false,\\r\\n      \"eventGridTopicEndpoint\": \"https://topic_name.westus2-1.eventgrid.azure.net/api/events\",\\r\\n      \"eventGridKeySettingName\":  \"EventGridKey\",\\r\\n      \"eventGridPublishRetryCount\": 3,\\r\\n      \"eventGridPublishRetryInterval\": \"00:00:30\",\\r\\n      \"eventGridPublishEventTypes\": [\"Started\", \"Completed\", \"Failed\", \"Terminated\"]\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nTask hub names must start with a letter and consist of only letters and numbers. If not specified, the default task hub name for a function app is **TestHubName**. For more information, see [Task hubs](durable-functions-task-hubs).\\r\\n\\r\\n| Property | Default | Description |\\r\\n| --- | --- | --- |\\r\\n| hubName | TestHubName (DurableFunctionsHub if using Durable Functions 1.x) | Alternate [task hub](durable-functions-task-hubs) names can be used to isolate multiple Durable Functions applications from each other, even if they\\'re using the same storage backend. |\\r\\n| controlQueueBatchSize | 32 | The number of messages to pull from the control queue at a time. |\\r\\n| controlQueueBufferThreshold | **Consumption plan for Python**: 32 **Consumption plan for JavaScript and C#**: 128 **Dedicated/Premium plan**: 256 | The number of control queue messages that can be buffered in memory at a time, at which point the dispatcher will wait before dequeuing any additional messages. |\\r\\n| partitionCount | 4 | The partition count for the control queue. May be a positive integer between 1 and 16. |\\r\\n| controlQueueVisibilityTimeout | 5 minutes | The visibility timeout of dequeued control queue messages. |\\r\\n| workItemQueueVisibilityTimeout | 5 minutes | The visibility timeout of dequeued work item queue messages. |\\r\\n| maxConcurrentActivityFunctions | **Consumption plan**: 10 **Dedicated/Premium plan**: 10X the number of processors on the current machine | The maximum number of activity functions that can be processed concurrently on a single host instance. |\\r\\n| maxConcurrentOrchestratorFunctions | **Consumption plan**: 5 **Dedicated/Premium plan**: 10X the number of processors on the current machine | The maximum number of orchestrator functions that can be processed concurrently on a single host instance. |\\r\\n| maxQueuePollingInterval | 30 seconds | The maximum control and work-item queue polling interval in the *hh:mm:ss* format. Higher values can result in higher message processing latencies. Lower values can result in higher storage costs because of increased storage transactions. |\\r\\n| connectionName (2.7.0 and later)connectionStringName (2.x)azureStorageConnectionStringName (1.x) | AzureWebJobsStorage | The name of an app setting or setting collection that specifies how to connect to the underlying Azure Storage resources. When a single app setting is provided, it should be an Azure Storage connection string. |\\r\\n| trackingStoreConnectionName (2.7.0 and later)trackingStoreConnectionStringName | The name of an app setting or setting collection that specifies how to connect to the History and Instances tables. When a single app setting is provided, it should be an Azure Storage connection string. If not specified, the `connectionStringName` (Durable 2.x) or `azureStorageConnectionStringName` (Durable 1.x) connection is used. |\\r\\n| trackingStoreNamePrefix | The prefix to use for the History and Instances tables when `trackingStoreConnectionStringName` is specified. If not set, the default prefix value will be `DurableTask`. If `trackingStoreConnectionStringName` is not specified, then the History and Instances tables will use the `hubName` value as their prefix, and any setting for `trackingStoreNamePrefix` will be ignored. |\\r\\n| traceInputsAndOutputs | false | A value indicating whether to trace the inputs and outputs of function calls. The default behavior when tracing function execution events is to include the number of bytes in the serialized inputs and outputs for function calls. This behavior provides minimal information about what the inputs and outputs look like without bloating the logs or inadvertently exposing sensitive information. Setting this property to true causes the default function logging to log the entire contents of function inputs and outputs. |\\r\\n| traceReplayEvents | false | A value indicating whether to write orchestration replay events to Application Insights. |\\r\\n| eventGridTopicEndpoint | The URL of an Azure Event Grid custom topic endpoint. When this property is set, orchestration life-cycle notification events are published to this endpoint. This property supports App Settings resolution. |\\r\\n| eventGridKeySettingName | The name of the app setting containing the key used for authenticating with the Azure Event Grid custom topic at `EventGridTopicEndpoint`. |\\r\\n| eventGridPublishRetryCount | 0 | The number of times to retry if publishing to the Event Grid Topic fails. |\\r\\n| eventGridPublishRetryInterval | 5 minutes | The Event Grid publishes retry interval in the *hh:mm:ss* format. |\\r\\n| eventGridPublishEventTypes | A list of event types to publish to Event Grid. If not specified, all event types will be published. Allowed values include `Started`, `Completed`, `Failed`, `Terminated`. |\\r\\n| useAppLease | true | When set to `true`, apps will require acquiring an app-level blob lease before processing task hub messages. For more information, see the [disaster recovery and geo-distribution](durable-functions-disaster-recovery-geo-distribution) documentation. Available starting in v2.3.0. |\\r\\n| useLegacyPartitionManagement | false | When set to `false`, uses a partition management algorithm that reduces the possibility of duplicate function execution when scaling out. Available starting in v2.3.0. |\\r\\n| useTablePartitionManagement | false | When set to `true`, uses a partition management algorithm designed to reduce costs for Azure Storage V2 accounts. Available starting in v2.10.0. **This feature is currently in preview and not yet compatible with the Consumption plan.** |\\r\\n| useGracefulShutdown | false | (Preview) Enable gracefully shutting down to reduce the chance of host shutdowns failing in-process function executions. |\\r\\n| maxEntityOperationBatchSize(2.6.1) | **Consumption plan**: 50 **Dedicated/Premium plan**: 5000 | The maximum number of entity operations that are processed as a [batch](durable-functions-perf-and-scale#entity-operation-batching). If set to 1, batching is disabled, and each operation message is processed by a separate function invocation. |\\r\\n| storeInputsInOrchestrationHistory | false | When set to `true`, tells the Durable Task Framework to save activity inputs in the history table. This enables the displaying of activity function inputs when querying orchestration history. |\\r\\n\\r\\nMany of these settings are for optimizing performance. For more information, see [Performance and scale](durable-functions-perf-and-scale).\\r\\n\\r\\n# Bindings for Durable Functions (Azure Functions) (programming-language-csharp)\\r\\n\\r\\nThe [Durable Functions](durable-functions-overview) extension introduces three trigger bindings that control the execution of orchestrator, entity, and activity functions. It also introduces an output binding that acts as a client for the Durable Functions runtime.\\r\\n\\r\\nMake sure to choose your Durable Functions development language at the top of the article.\\r\\n\\r\\n## Orchestration trigger\\r\\n\\r\\nThe orchestration trigger enables you to author [durable orchestrator functions](durable-functions-types-features-overview#orchestrator-functions). This trigger executes when a new orchestration instance is scheduled and when an existing orchestration instance receives an event. Examples of events that can trigger orchestrator functions include durable timer expirations, activity function responses, and events raised by external clients.\\r\\n\\r\\nWhen you author functions in .NET, the orchestration trigger is configured using the [OrchestrationTriggerAttribute](/en-us/dotnet/api/microsoft.azure.webjobs.extensions.durabletask.orchestrationtriggerattribute) .NET attribute.\\r\\n\\r\\nInternally, this trigger binding polls the configured durable store for new orchestration events, such as orchestration start events, durable timer expiration events, activity function response events, and external events raised by other functions.\\r\\n\\r\\n### Trigger behavior\\r\\n\\r\\nHere are some notes about the orchestration trigger:\\r\\n\\r\\n- **Single-threading** - A single dispatcher thread is used for all orchestrator function execution on a single host instance. For this reason, it\\'s important to ensure that orchestrator function code is efficient and doesn\\'t perform any I/O. It is also important to ensure that this thread does not do any async work except when awaiting on Durable Functions-specific task types.\\r\\n- **Poison-message handling** - There\\'s no poison message support in orchestration triggers.\\r\\n- **Message visibility** - Orchestration trigger messages are dequeued and kept invisible for a configurable duration. The visibility of these messages is renewed automatically as long as the function app is running and healthy.\\r\\n- **Return values** - Return values are serialized to JSON and persisted to the orchestration history table in Azure Table storage. These return values can be queried by the orchestration client binding, described later.\\r\\n\\r\\nWarning\\r\\n\\r\\nOrchestrator functions should never use any input or output bindings other than the orchestration trigger binding. Doing so has the potential to cause problems with the Durable Task extension because those bindings may not obey the single-threading and I/O rules. If you\\'d like to use other bindings, add them to an activity function called from your orchestrator function. For more information about coding constraints for orchestrator functions, see the [Orchestrator function code constraints](durable-functions-code-constraints) documentation.\\r\\n\\r\\n### Trigger usage\\r\\n\\r\\nThe orchestration trigger binding supports both inputs and outputs. Here are some things to know about input and output handling:\\r\\n\\r\\n- **inputs** - Orchestration triggers can be invoked with inputs, which are accessed through the context input object. All inputs must be JSON-serializable.\\r\\n- **outputs** - Orchestration triggers support output values as well as inputs. The return value of the function is used to assign the output value and must be JSON-serializable.\\r\\n\\r\\n### Trigger sample\\r\\n\\r\\nThe following example code shows what the simplest \"Hello World\" orchestrator function might look like. Note that this example orchestrator doesn\\'t actually schedule any tasks.\\r\\n\\r\\nThe specific attribute used to define the trigger depends on whether you are running your C# functions [in-process](../functions-dotnet-class-library) or in an [isolated worker process](../dotnet-isolated-process-guide).\\r\\n\\r\\n**In-process**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"HelloWorld\")]\\r\\npublic static string Run([OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    string name = context.GetInput<string>();\\r\\n    return $\"Hello {name}!\";\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous code is for Durable Functions 2.x. For Durable Functions 1.x, you must use `DurableOrchestrationContext` instead of `IDurableOrchestrationContext`. For more information about the differences between versions, see the [Durable Functions Versions](durable-functions-versions) article.\\r\\n\\r\\n**Isolated process**\\r\\n\\r\\n```csharp\\r\\n[Function(\"HelloWorld\")]\\r\\npublic static string Run([OrchestrationTrigger] TaskOrchestrationContext context, string name)\\r\\n{\\r\\n    return $\"Hello {name}!\";\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nIn both Durable functions in-proc and in .NET-isolated, the orchestration input can be extracted via `context.GetInput<T>()`. However, .NET-isolated also supports the input being supplied as a parameter, as shown above. The input binding will bind to the first parameter which has no binding attribute on it and is not a well-known type already covered by other input bindings (ie: `FunctionContext`).\\r\\n\\r\\nMost orchestrator functions call activity functions, so here is a \"Hello World\" example that demonstrates how to call an activity function:\\r\\n\\r\\n**In-process**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"HelloWorld\")]\\r\\npublic static async Task<string> Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    string name = context.GetInput<string>();\\r\\n    string result = await context.CallActivityAsync<string>(\"SayHello\", name);\\r\\n    return result;\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous code is for Durable Functions 2.x. For Durable Functions 1.x, you must use `DurableOrchestrationContext` instead of `IDurableOrchestrationContext`. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n**Isolated process**\\r\\n\\r\\n```csharp\\r\\n[Function(\"HelloWorld\")]\\r\\npublic static async Task<string> Run(\\r\\n    [OrchestrationTrigger] TaskOrchestrationContext context, string name)\\r\\n{\\r\\n    string result = await context.CallActivityAsync<string>(\"SayHello\", name);\\r\\n    return result;\\r\\n}\\r\\n```\\r\\n\\r\\n## Activity trigger\\r\\n\\r\\nThe activity trigger enables you to author functions that are called by orchestrator functions, known as [activity functions](durable-functions-types-features-overview#activity-functions).\\r\\n\\r\\nThe activity trigger is configured using the [ActivityTriggerAttribute](/en-us/dotnet/api/microsoft.azure.webjobs.extensions.durabletask.activitytriggerattribute) .NET attribute.\\r\\n\\r\\nInternally, this trigger binding polls the configured durable store for new activity execution events.\\r\\n\\r\\n### Trigger behavior\\r\\n\\r\\nHere are some notes about the activity trigger:\\r\\n\\r\\n- **Threading** - Unlike the orchestration trigger, activity triggers don\\'t have any restrictions around threading or I/O. They can be treated like regular functions.\\r\\n- **Poison-message handling** - There\\'s no poison message support in activity triggers.\\r\\n- **Message visibility** - Activity trigger messages are dequeued and kept invisible for a configurable duration. The visibility of these messages is renewed automatically as long as the function app is running and healthy.\\r\\n- **Return values** - Return values are serialized to JSON and persisted to the configured durable store.\\r\\n\\r\\n### Trigger usage\\r\\n\\r\\nThe activity trigger binding supports both inputs and outputs, just like the orchestration trigger. Here are some things to know about input and output handling:\\r\\n\\r\\n- **inputs** - Activity triggers can be invoked with inputs from an orchestrator function. All inputs must be JSON-serializable.\\r\\n- **outputs** - Activity functions support output values as well as inputs. The return value of the function is used to assign the output value and must be JSON-serializable.\\r\\n- **metadata** - .NET activity functions can bind to a `string instanceId` parameter to get the instance ID of the calling orchestration.\\r\\n\\r\\n### Trigger sample\\r\\n\\r\\nThe following example code shows what a simple `SayHello` activity function might look like.\\r\\n\\r\\n**In-process**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"SayHello\")]\\r\\npublic static string SayHello([ActivityTrigger] IDurableActivityContext helloContext)\\r\\n{\\r\\n    string name = helloContext.GetInput<string>();\\r\\n    return $\"Hello {name}!\";\\r\\n}\\r\\n```\\r\\n\\r\\nThe default parameter type for the .NET `ActivityTriggerAttribute` binding is [IDurableActivityContext](/en-us/dotnet/api/microsoft.azure.webjobs.extensions.durabletask.idurableactivitycontext) (or [DurableActivityContext](/en-us/dotnet/api/microsoft.azure.webjobs.durableactivitycontext?view=azure-dotnet-legacy&amp;preserve-view=true) for Durable Functions v1). However, .NET activity triggers also support binding directly to JSON-serializeable types (including primitive types), so the same function could be simplified as follows:\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"SayHello\")]\\r\\npublic static string SayHello([ActivityTrigger] string name)\\r\\n{\\r\\n    return $\"Hello {name}!\";\\r\\n}\\r\\n```\\r\\n\\r\\n**Isolated process**\\r\\nIn the .NET-isolated worker, only serializable types representing your input are supported for the `[ActivityTrigger]`.\\r\\n\\r\\n```csharp\\r\\n[Function(\"SayHello\")]\\r\\npublic static string SayHello([ActivityTrigger] string name)\\r\\n{\\r\\n    return $\"Hello {name}!\";\\r\\n}\\r\\n```\\r\\n\\r\\n### Using input and output bindings\\r\\n\\r\\nYou can use regular input and output bindings in addition to the activity trigger binding.\\r\\n\\r\\n## Orchestration client\\r\\n\\r\\nThe orchestration client binding enables you to write functions that interact with orchestrator functions. These functions are often referred to as [client functions](durable-functions-types-features-overview#client-functions). For example, you can act on orchestration instances in the following ways:\\r\\n\\r\\n- Start them.\\r\\n- Query their status.\\r\\n- Terminate them.\\r\\n- Send events to them while they\\'re running.\\r\\n- Purge instance history.\\r\\n\\r\\nYou can bind to the orchestration client by using the [DurableClientAttribute](/en-us/dotnet/api/microsoft.azure.webjobs.extensions.durabletask.durableclientattribute) attribute ([OrchestrationClientAttribute](/en-us/dotnet/api/microsoft.azure.webjobs.orchestrationclientattribute?view=azure-dotnet-legacy&amp;preserve-view=true) in Durable Functions v1.x).\\r\\n\\r\\n### Client usage\\r\\n\\r\\nYou typically bind to [IDurableClient](/en-us/dotnet/api/microsoft.azure.webjobs.extensions.durabletask.idurableclient) ([DurableOr',\n",
       "  'metadata': {}},\n",
       " {'_id': '2c204e1f-5095-6f7e-d2c3-e0d2927232b3',\n",
       "  'title': 'Fan-out/fan-in scenarios in Durable Functions - Azure',\n",
       "  'text': '# Fan-out/fan-in scenario in Durable Functions - Cloud backup example\\r\\n\\r\\n*Fan-out/fan-in* refers to the pattern of executing multiple functions concurrently and then performing some aggregation on the results. This article explains a sample that uses [Durable Functions](durable-functions-overview) to implement a fan-in/fan-out scenario. The sample is a durable function that backs up all or some of an app\\'s site content into Azure Storage.\\r\\n\\r\\nNote\\r\\n\\r\\nVersion 4 of the Node.js programming model for Azure Functions is generally available. The new v4 model is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](../functions-node-upgrade-v4).\\r\\n\\r\\nIn the following code snippets, JavaScript (PM4) denotes programming model V4, the new experience.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\n**C#**\\r\\n- [Complete the quickstart article](durable-functions-isolated-create-first-csharp)\\r\\n- [Clone or download the samples project from GitHub](https://github.com/Azure/azure-functions-durable-extension/tree/master/samples/precompiled)\\r\\n\\r\\n**JavaScript (PM3)**\\r\\n- [Complete the quickstart article](quickstart-js-vscode?pivots=nodejs-model-v3)\\r\\n- [Clone or download the samples project from GitHub](https://github.com/Azure/azure-functions-durable-js/tree/v2.x/samples)\\r\\n\\r\\n**JavaScript (PM4)**\\r\\n- [Complete the quickstart article](quickstart-js-vscode?pivots=nodejs-model-v4)\\r\\n- [Clone or download the samples project from GitHub](https://github.com/Azure/azure-functions-durable-js/tree/v3.x/samples-js)\\r\\n\\r\\n**Python**\\r\\n- [Complete the quickstart article](quickstart-python-vscode)\\r\\n- [Clone or download the samples project from GitHub](https://github.com/Azure/azure-functions-durable-python/tree/master/samples/)\\r\\n\\r\\n## Scenario overview\\r\\n\\r\\nIn this sample, the functions upload all files under a specified directory recursively into blob storage. They also count the total number of bytes that were uploaded.\\r\\n\\r\\nIt\\'s possible to write a single function that takes care of everything. The main problem you would run into is **scalability**. A single function execution can only run on a single virtual machine, so the throughput will be limited by the throughput of that single VM. Another problem is **reliability**. If there\\'s a failure midway through, or if the entire process takes more than 5 minutes, the backup could fail in a partially completed state. It would then need to be restarted.\\r\\n\\r\\nA more robust approach would be to write two regular functions: one would enumerate the files and add the file names to a queue, and another would read from the queue and upload the files to blob storage. This approach is better in terms of throughput and reliability, but it requires you to provision and manage a queue. More importantly, significant complexity is introduced in terms of **state management** and **coordination** if you want to do anything more, like report the total number of bytes uploaded.\\r\\n\\r\\nA Durable Functions approach gives you all of the mentioned benefits with very low overhead.\\r\\n\\r\\n## The functions\\r\\n\\r\\nThis article explains the following functions in the sample app:\\r\\n\\r\\n- `E2_BackupSiteContent`: An [orchestrator function](durable-functions-bindings#orchestration-trigger) that calls `E2_GetFileList` to obtain a list of files to back up, then calls `E2_CopyFileToBlob` to back up each file.\\r\\n- `E2_GetFileList`: An [activity function](durable-functions-bindings#activity-trigger) that returns a list of files in a directory.\\r\\n- `E2_CopyFileToBlob`: An activity function that backs up a single file to Azure Blob Storage.\\r\\n\\r\\n### E2\\\\_BackupSiteContent orchestrator function\\r\\n\\r\\nThis orchestrator function essentially does the following:\\r\\n\\r\\n1. Takes a `rootDirectory` value as an input parameter.\\r\\n2. Calls a function to get a recursive list of files under `rootDirectory`.\\r\\n3. Makes multiple parallel function calls to upload each file into Azure Blob Storage.\\r\\n4. Waits for all uploads to complete.\\r\\n5. Returns the sum total bytes that were uploaded to Azure Blob Storage.\\r\\n\\r\\n**C#**\\r\\nHere is the code that implements the orchestrator function:\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"E2_BackupSiteContent\")]\\r\\npublic static async Task<long> Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext backupContext)\\r\\n{\\r\\n    string rootDirectory = backupContext.GetInput<string>()?.Trim();\\r\\n    if (string.IsNullOrEmpty(rootDirectory))\\r\\n    {\\r\\n        rootDirectory = Directory.GetParent(typeof(BackupSiteContent).Assembly.Location).FullName;\\r\\n    }\\r\\n\\r\\n    string[] files = await backupContext.CallActivityAsync<string[]>(\\r\\n        \"E2_GetFileList\",\\r\\n        rootDirectory);\\r\\n\\r\\n    var tasks = new Task<long>[files.Length];\\r\\n    for (int i = 0; i < files.Length; i++)\\r\\n    {\\r\\n        tasks[i] = backupContext.CallActivityAsync<long>(\\r\\n            \"E2_CopyFileToBlob\",\\r\\n            files[i]);\\r\\n    }\\r\\n\\r\\n    await Task.WhenAll(tasks);\\r\\n\\r\\n    long totalBytes = tasks.Sum(t => t.Result);\\r\\n    return totalBytes;\\r\\n}\\r\\n```\\r\\n\\r\\nNotice the `await Task.WhenAll(tasks);` line. All the individual calls to the `E2_CopyFileToBlob` function were *not* awaited, which allows them to run in parallel. When we pass this array of tasks to `Task.WhenAll`, we get back a task that won\\'t complete *until all the copy operations have completed*. If you\\'re familiar with the Task Parallel Library (TPL) in .NET, then this is not new to you. The difference is that these tasks could be running on multiple virtual machines concurrently, and the Durable Functions extension ensures that the end-to-end execution is resilient to process recycling.\\r\\n\\r\\nAfter awaiting from `Task.WhenAll`, we know that all function calls have completed and have returned values back to us. Each call to `E2_CopyFileToBlob` returns the number of bytes uploaded, so calculating the sum total byte count is a matter of adding all those return values together.\\r\\n\\r\\n**JavaScript (PM3)**\\r\\nThe function uses the standard *function.json* for orchestrator functions.\\r\\n\\r\\n```javascript\\r\\n{\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"name\": \"context\",\\r\\n      \"type\": \"orchestrationTrigger\",\\r\\n      \"direction\": \"in\"\\r\\n    }\\r\\n  ],\\r\\n  \"disabled\": false\\r\\n}\\r\\n```\\r\\n\\r\\nHere is the code that implements the orchestrator function:\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function* (context) {\\r\\n    const rootDirectory = context.df.getInput();\\r\\n    if (!rootDirectory) {\\r\\n        throw new Error(\"A directory path is required as an input.\");\\r\\n    }\\r\\n\\r\\n    const files = yield context.df.callActivity(\"E2_GetFileList\", rootDirectory);\\r\\n\\r\\n    // Backup Files and save Promises into array\\r\\n    const tasks = [];\\r\\n    for (const file of files) {\\r\\n        tasks.push(context.df.callActivity(\"E2_CopyFileToBlob\", file));\\r\\n    }\\r\\n\\r\\n    // wait for all the Backup Files Activities to complete, sum total bytes\\r\\n    const results = yield context.df.Task.all(tasks);\\r\\n    const totalBytes = results.reduce((prev, curr) => prev + curr, 0);\\r\\n\\r\\n    // return results;\\r\\n    return totalBytes;\\r\\n});\\r\\n```\\r\\n\\r\\nNotice the `yield context.df.Task.all(tasks);` line. All the individual calls to the `E2_CopyFileToBlob` function were *not* yielded, which allows them to run in parallel. When we pass this array of tasks to `context.df.Task.all`, we get back a task that won\\'t complete *until all the copy operations have completed*. If you\\'re familiar with [`Promise.all`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/all) in JavaScript, then this is not new to you. The difference is that these tasks could be running on multiple virtual machines concurrently, and the Durable Functions extension ensures that the end-to-end execution is resilient to process recycling.\\r\\n\\r\\nNote\\r\\n\\r\\nAlthough tasks are conceptually similar to JavaScript promises, orchestrator functions should use `context.df.Task.all` and `context.df.Task.any` instead of `Promise.all` and `Promise.race` to manage task parallelization.\\r\\n\\r\\nAfter yielding from `context.df.Task.all`, we know that all function calls have completed and have returned values back to us. Each call to `E2_CopyFileToBlob` returns the number of bytes uploaded, so calculating the sum total byte count is a matter of adding all those return values together.\\r\\n\\r\\n**JavaScript (PM4)**\\r\\nHere is the code that implements the orchestrator function:\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\nconst path = require(\"path\");\\r\\nconst getFileListActivityName = \"getFileList\";\\r\\nconst copyFileToBlobActivityName = \"copyFileToBlob\";\\r\\n\\r\\ndf.app.orchestration(\"backupSiteContent\", function* (context) {\\r\\n    const rootDir = context.df.getInput();\\r\\n    if (!rootDir) {\\r\\n        throw new Error(\"A directory path is required as an input.\");\\r\\n    }\\r\\n\\r\\n    const rootDirAbs = path.resolve(rootDir);\\r\\n    const files = yield context.df.callActivity(getFileListActivityName, rootDirAbs);\\r\\n\\r\\n    // Backup Files and save Tasks into array\\r\\n    const tasks = [];\\r\\n    for (const file of files) {\\r\\n        const input = {\\r\\n            backupPath: path.relative(rootDirAbs, file).replace(\"\\\\\\\\\", \"/\"),\\r\\n            filePath: file,\\r\\n        };\\r\\n        tasks.push(context.df.callActivity(copyFileToBlobActivityName, input));\\r\\n    }\\r\\n\\r\\n    // wait for all the Backup Files Activities to complete, sum total bytes\\r\\n    const results = yield context.df.Task.all(tasks);\\r\\n    const totalBytes = results ? results.reduce((prev, curr) => prev + curr, 0) : 0;\\r\\n\\r\\n    // return results;\\r\\n    return totalBytes;\\r\\n});\\r\\n```\\r\\n\\r\\nNotice the `yield context.df.Task.all(tasks);` line. All the individual calls to the `copyFileToBlob` function were *not* yielded, which allows them to run in parallel. When we pass this array of tasks to `context.df.Task.all`, we get back a task that won\\'t complete *until all the copy operations have completed*. If you\\'re familiar with [`Promise.all`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/all) in JavaScript, then this is not new to you. The difference is that these tasks could be running on multiple virtual machines concurrently, and the Durable Functions extension ensures that the end-to-end execution is resilient to process recycling.\\r\\n\\r\\nNote\\r\\n\\r\\nAlthough Tasks are conceptually similar to JavaScript promises, orchestrator functions should use `context.df.Task.all` and `context.df.Task.any` instead of `Promise.all` and `Promise.race` to manage task parallelization.\\r\\n\\r\\nAfter yielding from `context.df.Task.all`, we know that all function calls have completed and have returned values back to us. Each call to `copyFileToBlob` returns the number of bytes uploaded, so calculating the sum total byte count is a matter of adding all those return values together.\\r\\n\\r\\n**Python**\\r\\nThe function uses the standard *function.json* for orchestrator functions.\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"scriptFile\": \"__init__.py\",\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"name\": \"context\",\\r\\n      \"type\": \"orchestrationTrigger\",\\r\\n      \"direction\": \"in\"\\r\\n    }\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\nHere is the code that implements the orchestrator function:\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n\\r\\n    root_directory: str = context.get_input()\\r\\n\\r\\n    if not root_directory:\\r\\n        raise Exception(\"A directory path is required as input\")\\r\\n\\r\\n    files = yield context.call_activity(\"E2_GetFileList\", root_directory)\\r\\n    tasks = []\\r\\n    for file in files:\\r\\n        tasks.append(context.call_activity(\"E2_CopyFileToBlob\", file))\\r\\n    \\r\\n    results = yield context.task_all(tasks)\\r\\n    total_bytes = sum(results)\\r\\n    return total_bytes\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\nNotice the `yield context.task_all(tasks);` line. All the individual calls to the `E2_CopyFileToBlob` function were *not* yielded, which allows them to run in parallel. When we pass this array of tasks to `context.task_all`, we get back a task that won\\'t complete *until all the copy operations have completed*. If you\\'re familiar with [`asyncio.gather`](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) in Python, then this is not new to you. The difference is that these tasks could be running on multiple virtual machines concurrently, and the Durable Functions extension ensures that the end-to-end execution is resilient to process recycling.\\r\\n\\r\\nNote\\r\\n\\r\\nAlthough tasks are conceptually similar to Python awaitables, orchestrator functions should use `yield` as well as the `context.task_all` and `context.task_any` APIs to manage task parallelization.\\r\\n\\r\\nAfter yielding from `context.task_all`, we know that all function calls have completed and have returned values back to us. Each call to `E2_CopyFileToBlob` returns the number of bytes uploaded, so we can calculate the sum total byte count by adding all the return values together.\\r\\n\\r\\n### Helper activity functions\\r\\n\\r\\nThe helper activity functions, as with other samples, are just regular functions that use the `activityTrigger` trigger binding.\\r\\n\\r\\n#### E2\\\\_GetFileList activity function\\r\\n\\r\\n**C#**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"E2_GetFileList\")]\\r\\npublic static string[] GetFileList(\\r\\n    [ActivityTrigger] string rootDirectory, \\r\\n    ILogger log)\\r\\n{\\r\\n    log.LogInformation($\"Searching for files under \\'{rootDirectory}\\'...\");\\r\\n    string[] files = Directory.GetFiles(rootDirectory, \"*\", SearchOption.AllDirectories);\\r\\n    log.LogInformation($\"Found {files.Length} file(s) under {rootDirectory}.\");\\r\\n\\r\\n    return files;\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript (PM3)**\\r\\nThe *function.json* file for `E2_GetFileList` looks like the following:\\r\\n\\r\\n```javascript\\r\\n{\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"name\": \"rootDirectory\",\\r\\n      \"type\": \"activityTrigger\",\\r\\n      \"direction\": \"in\"\\r\\n    }\\r\\n  ],\\r\\n  \"disabled\": false\\r\\n}\\r\\n```\\r\\n\\r\\nAnd here is the implementation:\\r\\n\\r\\n```javascript\\r\\nconst readdirp = require(\"readdirp\");\\r\\n\\r\\nmodule.exports = function (context, rootDirectory) {\\r\\n    context.log(`Searching for files under \\'${rootDirectory}\\'...`);\\r\\n    const allFilePaths = [];\\r\\n\\r\\n    readdirp(\\r\\n        { root: rootDirectory, entryType: \"all\" },\\r\\n        function (fileInfo) {\\r\\n            if (!fileInfo.stat.isDirectory()) {\\r\\n                allFilePaths.push(fileInfo.fullPath);\\r\\n            }\\r\\n        },\\r\\n        function (err, res) {\\r\\n            if (err) {\\r\\n                throw err;\\r\\n            }\\r\\n\\r\\n            context.log(`Found ${allFilePaths.length} under ${rootDirectory}.`);\\r\\n            context.done(null, allFilePaths);\\r\\n        }\\r\\n    );\\r\\n};\\r\\n```\\r\\n\\r\\nThe function uses the `readdirp` module (version 2.x) to recursively read the directory structure.\\r\\n\\r\\n**JavaScript (PM4)**\\r\\nHere is the implementation of the `getFileList` activity function:\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\nconst readdirp = require(\"readdirp\");\\r\\nconst getFileListActivityName = \"getFileList\";\\r\\n\\r\\ndf.app.activity(getFileListActivityName, {\\r\\n    handler: async function (rootDirectory, context) {\\r\\n        context.log(`Searching for files under \\'${rootDirectory}\\'...`);\\r\\n\\r\\n        const allFilePaths = [];\\r\\n        for await (const entry of readdirp(rootDirectory, { type: \"files\" })) {\\r\\n            allFilePaths.push(entry.fullPath);\\r\\n        }\\r\\n        context.log(`Found ${allFilePaths.length} under ${rootDirectory}.`);\\r\\n        return allFilePaths;\\r\\n    },\\r\\n});\\r\\n```\\r\\n\\r\\nThe function uses the `readdirp` module (version `3.x`) to recursively read the directory structure.\\r\\n\\r\\n**Python**\\r\\nThe *function.json* file for `E2_GetFileList` looks like the following:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"scriptFile\": \"__init__.py\",\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"name\": \"rootDirectory\",\\r\\n      \"type\": \"activityTrigger\",\\r\\n      \"direction\": \"in\"\\r\\n    }\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\nAnd here is the implementation:\\r\\n\\r\\n```python\\r\\nimport os\\r\\nfrom os.path import dirname\\r\\nfrom typing import List\\r\\n\\r\\ndef main(rootDirectory: str) -> List[str]:\\r\\n\\r\\n    all_file_paths = []\\r\\n    # We walk the file system\\r\\n    for path, _, files in os.walk(rootDirectory):\\r\\n        # We copy the code for activities and orchestrators\\r\\n        if \"E2_\" in path:\\r\\n            # For each file, we add their full-path to the list\\r\\n            for name in files:\\r\\n                if name == \"__init__.py\" or name == \"function.json\":\\r\\n                    file_path = os.path.join(path, name)\\r\\n                    all_file_paths.append(file_path)\\r\\n    \\r\\n    return all_file_paths\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nYou might be wondering why you couldn\\'t just put this code directly into the orchestrator function. You could, but this would break one of the fundamental rules of orchestrator functions, which is that they should never do I/O, including local file system access. For more information, see [Orchestrator function code constraints](durable-functions-code-constraints).\\r\\n\\r\\n#### E2\\\\_CopyFileToBlob activity function\\r\\n\\r\\n**C#**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"E2_CopyFileToBlob\")]\\r\\npublic static async Task<long> CopyFileToBlob(\\r\\n    [ActivityTrigger] string filePath,\\r\\n    Binder binder,\\r\\n    ILogger log)\\r\\n{\\r\\n    long byteCount = new FileInfo(filePath).Length;\\r\\n\\r\\n    // strip the drive letter prefix and convert to forward slashes\\r\\n    string blobPath = filePath\\r\\n        .Substring(Path.GetPathRoot(filePath).Length)\\r\\n        .Replace(\\'\\\\\\\\\\', \\'/\\');\\r\\n    string outputLocation = $\"backups/{blobPath}\";\\r\\n\\r\\n    log.LogInformation($\"Copying \\'{filePath}\\' to \\'{outputLocation}\\'. Total bytes = {byteCount}.\");\\r\\n\\r\\n    // copy the file contents into a blob\\r\\n    using (Stream source = File.Open(filePath, FileMode.Open, FileAccess.Read, FileShare.Read))\\r\\n    using (Stream destination = await binder.BindAsync<CloudBlobStream>(\\r\\n        new BlobAttribute(outputLocation, FileAccess.Write)))\\r\\n    {\\r\\n        await source.CopyToAsync(destination);\\r\\n    }\\r\\n\\r\\n    return byteCount;\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nYou will need to install the `Microsoft.Azure.WebJobs.Extensions.Storage` NuGet package to run the sample code.\\r\\n\\r\\nThe function uses some advanced features of Azure Functions bindings (that is, the use of the [`Binder` parameter](../functions-dotnet-class-library#binding-at-runtime)), but you don\\'t need to worry about those details for the purpose of this walkthrough.\\r\\n\\r\\n**JavaScript (PM3)**\\r\\nThe *function.json* file for `E2_CopyFileToBlob` is similarly simple:\\r\\n\\r\\n```javascript\\r\\n{\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"name\": \"filePath\",\\r\\n      \"type\": \"activityTrigger\",\\r\\n      \"direction\": \"in\"\\r\\n    },\\r\\n    {\\r\\n      \"name\": \"out\",\\r\\n      \"type\": \"blob\",\\r\\n      \"path\": \"\",\\r\\n      \"connection\": \"AzureWebJobsStorage\",\\r\\n      \"direction\": \"out\"\\r\\n    }\\r\\n  ],\\r\\n  \"disabled\": false\\r\\n}\\r\\n```\\r\\n\\r\\nThe JavaScript implementation uses the [Azure Storage SDK for Node](https://github.com/Azure/azure-storage-node) to upload the files to Azure Blob Storage.\\r\\n\\r\\n```javascript\\r\\nconst fs = require(\"fs\");\\r\\nconst path = require(\"path\");\\r\\nconst storage = require(\"azure-storage\");\\r\\n\\r\\nmodule.exports = function (context, filePath) {\\r\\n    const container = \"backups\";\\r\\n    const root = path.parse(filePath).root;\\r\\n    const blobPath = filePath.substring(root.length).replace(\"\\\\\\\\\", \"/\");\\r\\n    const outputLocation = `backups/${blobPath}`;\\r\\n    const blobService = storage.createBlobService();\\r\\n\\r\\n    blobService.createContainerIfNotExists(container, (error) => {\\r\\n        if (error) {\\r\\n            throw error;\\r\\n        }\\r\\n\\r\\n        fs.stat(filePath, function (error, stats) {\\r\\n            if (error) {\\r\\n                throw error;\\r\\n            }\\r\\n            context.log(\\r\\n                `Copying \\'${filePath}\\' to \\'${outputLocation}\\'. Total bytes = ${stats.size}.`\\r\\n            );\\r\\n\\r\\n            const readStream = fs.createReadStream(filePath);\\r\\n\\r\\n            blobService.createBlockBlobFromStream(\\r\\n                container,\\r\\n                blobPath,\\r\\n                readStream,\\r\\n                stats.size,\\r\\n                function (error) {\\r\\n                    if (error) {\\r\\n                        throw error;\\r\\n                    }\\r\\n\\r\\n                    context.done(null, stats.size);\\r\\n                }\\r\\n            );\\r\\n        });\\r\\n    });\\r\\n};\\r\\n```\\r\\n\\r\\n**JavaScript (PM4)**\\r\\nThe JavaScript implementation of `copyFileToBlob` uses an Azure Storage output binding to upload the files to Azure Blob storage.\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\nconst fs = require(\"fs/promises\");\\r\\nconst { output } = require(\"@azure/functions\");\\r\\n\\r\\nconst copyFileToBlobActivityName = \"copyFileToBlob\";\\r\\n\\r\\nconst blobOutput = output.storageBlob({\\r\\n    path: \"backups/{backupPath}\",\\r\\n    connection: \"StorageConnString\",\\r\\n});\\r\\n\\r\\ndf.app.activity(copyFileToBlobActivityName, {\\r\\n    extraOutputs: [blobOutput],\\r\\n    handler: async function ({ backupPath, filePath }, context) {\\r\\n        const outputLocation = `backups/${backupPath}`;\\r\\n        const stats = await fs.stat(filePath);\\r\\n        context.log(`Copying \\'${filePath}\\' to \\'${outputLocation}\\'. Total bytes = ${stats.size}.`);\\r\\n\\r\\n        const fileContents = await fs.readFile(filePath);\\r\\n\\r\\n        context.extraOutputs.set(blobOutput, fileContents);\\r\\n\\r\\n        return stats.size;\\r\\n    },\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\nThe *function.json* file for `E2_CopyFileToBlob` is similarly simple:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"scriptFile\": \"__init__.py\",\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"name\": \"filePath\",\\r\\n      \"type\": \"activityTrigger\",\\r\\n      \"direction\": \"in\"\\r\\n    }\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\nThe Python implementation uses the [Azure Storage SDK for Python](https://github.com/Azure/azure-storage-python) to upload the files to Azure Blob Storage.\\r\\n\\r\\n```python\\r\\nimport os\\r\\nimport pathlib\\r\\nfrom azure.storage.blob import BlobServiceClient\\r\\nfrom azure.core.exceptions import ResourceExistsError\\r\\n\\r\\nconnect_str = os.getenv(\\'AzureWebJobsStorage\\')\\r\\n\\r\\ndef main(filePath: str) -> str:\\r\\n    # Create the BlobServiceClient object which will be used to create a container client\\r\\n    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\\r\\n    \\r\\n    # Create a unique name for the container\\r\\n    container_name = \"backups\"\\r\\n    \\r\\n    # Create the container if it does not exist\\r\\n    try:\\r\\n        blob_service_client.create_container(container_name)\\r\\n    except ResourceExistsError:\\r\\n        pass\\r\\n\\r\\n    # Create a blob client using the local file name as the name for the blob\\r\\n    parent_dir, fname = pathlib.Path(filePath).parts[-2:] # Get last two path components\\r\\n    blob_name = parent_dir + \"_\" + fname\\r\\n    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\\r\\n\\r\\n    # Count bytes in file\\r\\n    byte_count = os.path.getsize(filePath)\\r\\n\\r\\n    # Upload the created file\\r\\n    with open(filePath, \"rb\") as data:\\r\\n        blob_client.upload_blob(data)\\r\\n\\r\\n    return byte_count\\r\\n```\\r\\n\\r\\nThe implementation loads the file from disk and asynchronously streams the contents into a blob of the same name in the \"backups\" container. The return value is the number of bytes copied to storage, that is then used by the orchestrator function to compute the aggregate sum.\\r\\n\\r\\nNote\\r\\n\\r\\nThis is a perfect example of moving I/O operations into an `activityTrigger` function. Not only can the work be distributed across many different machines, but you also get the benefits of checkpointing the progress. If the host process gets terminated for any reason, you know which uploads have already completed.\\r\\n\\r\\n## Run the sample\\r\\n\\r\\nYou can start the orchestration, on Windows, by sending the following HTTP POST request.\\r\\n\\r\\n```\\r\\nPOST http://{host}/orchestrators/E2_BackupSiteContent\\r\\nContent-Type: application/json\\r\\nContent-Length: 20\\r\\n\\r\\n\"D:\\\\\\\\home\\\\\\\\LogFiles\"\\r\\n```\\r\\n\\r\\nAlternatively, on a Linux Function App (Python currently only runs on Linux for App Service), you can start the orchestration like so:\\r\\n\\r\\n```\\r\\nPOST http://{host}/orchestrators/E2_BackupSiteContent\\r\\nContent-Type: application/json\\r\\nContent-Length: 20\\r\\n\\r\\n\"/home/site/wwwroot\"\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe `HttpStart` function that you are invoking only works with JSON-formatted content. For this reason, the `Content-Type: application/json` header is required and the directory path is encoded as a JSON string. Moreover, HTTP snippet assumes there is an entry in the `host.json` file which removes the default `api/` prefix from all HTTP trigger functions URLs. You can find the markup for this configuration in the `host.json` file in the samples.\\r\\n\\r\\nThis HTTP request triggers the `E2_BackupSiteContent` orchestrator and passes the string `D:\\\\home\\\\LogFiles` as a parameter. The response provides a link to get the status of the backup operation:\\r\\n\\r\\n```\\r\\nHTTP/1.1 202 Accepted\\r\\nContent-Length: 719\\r\\nContent-Type: application/json; charset=utf-8\\r\\nLocation: http://{host}/runtime/webhooks/durabletask/instances/b4e9bdcc435d460f8dc008115ff0a8a9?taskHub=DurableFunctionsHub&connection=Storage&code={systemKey}\\r\\n\\r\\n(...trimmed...)\\r\\n```\\r\\n\\r\\nDepending on how many log files you have in your function app, this operation could take several minutes to complete. You can get the latest status by querying the URL in the `Location` header of the previous HTTP 202 response.\\r\\n\\r\\n```\\r\\nGET http://{host}/runtime/webhooks/durabletask/instances/b4e9bdcc435d460f8dc008115ff0a8a9?taskHub=DurableFunctionsHub&connection=Storage&code={systemKey}\\r\\n```\\r\\n\\r\\n```\\r\\nHTTP/1.1 202 Accepted\\r\\nContent-Length: 148\\r\\nContent-Type: application/json; charset=utf-8\\r\\nLocation: http://{host}/runtime/webhooks/durabletask/instances/b4e9bdcc435d460f8dc008115ff0a8a9?taskHub=DurableFunctionsHub&connection=Storage&code={systemKey}\\r\\n\\r\\n{\"runtimeStatus\":\"Running\",\"input\":\"D:\\\\\\\\home\\\\\\\\LogFiles\",\"output\":null,\"createdTime\":\"2019-06-29T18:50:55Z\",\"lastUpdatedTime\":\"2019-06-29T18:51:16Z\"}\\r\\n```\\r\\n\\r\\nIn this case, the function is still running. You are able to see the input that was saved into the orchestrator state and the last updated time. You can continue to use the `Location` header values to poll for completion. When the status is \"Completed\", you see an HTTP response value similar to the following:\\r\\n\\r\\n```\\r\\nHTTP/1.1 200 OK\\r\\nContent-Length: 152\\r\\nContent-Type: application/json; charset=utf-8\\r\\n\\r\\n{\"runtimeStatus\":\"Completed\",\"input\":\"D:\\\\\\\\home\\\\\\\\LogFiles\",\"output\":452071,\"createdTime\":\"2019-06-29T18:50:55Z\",\"lastUpdatedTime\":\"2019-06-29T18:51:26Z\"}\\r\\n```\\r\\n\\r\\nNow you can see that the orchestration is complete and approximately how much time it took to complete. You also see a value for the `output` field, which indicates that around 450 KB of logs were uploaded.',\n",
       "  'metadata': {}},\n",
       " {'_id': 'cb9016ff-7b0a-fc3f-3981-29efd386ee6b',\n",
       "  'title': 'Durable orchestrator code constraints - Azure Functions',\n",
       "  'text': '# Orchestrator function code constraints\\r\\n\\r\\nDurable Functions is an extension of [Azure Functions](../functions-overview) that lets you build stateful apps. You can use an [orchestrator function](durable-functions-orchestrations) to orchestrate the execution of other durable functions within a function app. Orchestrator functions are stateful, reliable, and potentially long-running.\\r\\n\\r\\n## Orchestrator code constraints\\r\\n\\r\\nOrchestrator functions use [event sourcing](/en-us/azure/architecture/patterns/event-sourcing) to ensure reliable execution and to maintain local variable state. The [replay behavior](durable-functions-orchestrations#reliability) of orchestrator code creates constraints on the type of code that you can write in an orchestrator function. For example, orchestrator functions must be *deterministic*: an orchestrator function will be replayed multiple times, and it must produce the same result each time.\\r\\n\\r\\n### Using deterministic APIs\\r\\n\\r\\nThis section provides some simple guidelines that help ensure your code is deterministic.\\r\\n\\r\\nOrchestrator functions can call any API in their target languages. However, it\\'s important that orchestrator functions call only deterministic APIs. A *deterministic API* is an API that always returns the same value given the same input, no matter when or how often it\\'s called.\\r\\n\\r\\nThe following sections provide guidance on APIs and patterns that you should avoid because they are *not* deterministic. These restrictions apply only to orchestrator functions. Other function types don\\'t have such restrictions.\\r\\n\\r\\nNote\\r\\n\\r\\nSeveral types of code constraints are described below. This list is unfortunately not comprehensive and some use cases might not be covered. The most important thing to consider when writing orchestrator code is whether an API you\\'re using is deterministic. Once you\\'re comfortable with thinking this way, it\\'s easy to understand which APIs are safe to use and which are not without needing to refer to this documented list.\\r\\n\\r\\n#### Dates and times\\r\\n\\r\\nAPIs that return the current date or time are nondeterministic and should never be used in orchestrator functions. This is because each orchestrator function replay will produce a different value. You should instead use the Durable Functions equivalent API for getting the current date or time, which remains consistent across replays.\\r\\n\\r\\n**C#**\\r\\nDo not use `DateTime.Now`, `DateTime.UtcNow`, or equivalent APIs for getting the current time. Classes such as [`Stopwatch`](/en-us/dotnet/api/system.diagnostics.stopwatch) should also be avoided. For .NET in-process orchestrator functions, use the `IDurableOrchestrationContext.CurrentUtcDateTime` property to get the current time. For .NET isolated orchestrator functions, use the `TaskOrchestrationContext.CurrentDateTimeUtc` property to get the current time.\\r\\n\\r\\n```csharp\\r\\nDateTime startTime = context.CurrentUtcDateTime;\\r\\n// do some work\\r\\nTimeSpan totalTime = context.CurrentUtcDateTime.Subtract(startTime);\\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\nDo not use APIs like `new Date()` or `Date.now()` to get the current date and time. Instead, use `DurableOrchestrationContext.currentUtcDateTime`.\\r\\n\\r\\n```javascript\\r\\n// create a timer that expires 2 minutes from now\\r\\nconst expiration = moment.utc(context.df.currentUtcDateTime).add(2, \"m\");\\r\\nconst timeoutTask = context.df.createTimer(expiration.toDate());\\r\\n```\\r\\n\\r\\n**Python**\\r\\nDo not use `datetime.now()`, `gmtime()`, or similar APIs to get the current time. Instead, use `DurableOrchestrationContext.current_utc_datetime`.\\r\\n\\r\\n```python\\r\\n# create a timer that expires 2 minutes from now\\r\\nexpiration = context.current_utc_datetime + timedelta(seconds=120)\\r\\ntimeout_task = context.create_timer(expiration)\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\nDo not use cmdlets like `Get-Date` or .NET APIs like `[System.DateTime]::Now` to get the current time. Instead, use `$Context.CurrentUtcDateTime`.\\r\\n\\r\\n```powershell\\r\\n$expiryTime = $Context.Input.ExpiryTime\\r\\nwhile ($Context.CurrentUtcDateTime -lt $expiryTime) {\\r\\n    # do work\\r\\n}\\r\\n```\\r\\n\\r\\n**Java**\\r\\nDo not use APIs like `LocalDateTime.now()` or `Instant.now()` to get the current date and time. Instead, use `TaskOrchestrationContext.getCurrentInstant()`.\\r\\n\\r\\n```java\\r\\nInstant startTime = ctx.getCurrentInstant();\\r\\n// do some work\\r\\nDuration totalTime  = Duration.between(startTime, ctx.getCurrentInstant());\\r\\n```\\r\\n\\r\\n#### GUIDs and UUIDs\\r\\n\\r\\nAPIs that return a random GUID or UUID are nondeterministic because the generated value is different for each replay. Depending on which language you use, a built-in API for generating deterministic GUIDs or UUIDs may be available. Otherwise, use an activity function to return a randomly generated GUID or UUID.\\r\\n\\r\\n**C#**\\r\\nDo not use APIs like `Guid.NewGuid()` to generate random GUIDs. Instead, use the context object\\'s `NewGuid()` API to generate a random GUID that\\'s safe for orchestrator replay.\\r\\n\\r\\n```csharp\\r\\nGuid randomGuid = context.NewGuid();\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nGUIDs generated with orchestration context APIs are [Type 5 UUIDs](https://en.wikipedia.org/wiki/Universally_unique_identifier#Versions_3_and_5_%28namespace_name-based%29).\\r\\n\\r\\n**JavaScript**\\r\\nDo not use the `uuid` module or the `crypto.randomUUID()` function to generate random UUIDs. Instead, use the context object\\'s built-in `newGuid()` method to generate a random GUID that\\'s safe for orchestrator replay.\\r\\n\\r\\n```javascript\\r\\nconst randomGuid = context.df.newGuid();\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nUUIDs generated with orchestration context APIs are [Type 5 UUIDs](https://en.wikipedia.org/wiki/Universally_unique_identifier#Versions_3_and_5_%28namespace_name-based%29).\\r\\n\\r\\n**Python**\\r\\nDo not use the `uuid` module to generate random UUIDs. Instead, use the context object\\'s built-in `new_guid()` method to generate a random UUID that\\'s safe for orchestrator replay.\\r\\n\\r\\n```python\\r\\nrandomGuid = context.new_guid()\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nUUIDs generated with orchestration context APIs are [Type 5 UUIDs](https://en.wikipedia.org/wiki/Universally_unique_identifier#Versions_3_and_5_%28namespace_name-based%29).\\r\\n\\r\\n**PowerShell**\\r\\nDo not use cmdlets like `New-Guid` or .NET APIs like `[System.Guid]::NewGuid()` directly in orchestrator functions. Instead, generate random GUIDs in activity functions and return them to the orchestrator functions.\\r\\n\\r\\n**Java**\\r\\nDo not use the `java.util.UUID.randomUUID()` or similar methods for generating new UUIDs directly in orchestrator functions. Instead, generate random UUIDs in activity functions and return them to the orchestrator functions.\\r\\n\\r\\n#### Random numbers\\r\\n\\r\\nUse an activity function to return random numbers to an orchestrator function. The return values of activity functions are always safe for replay because they are saved into the orchestration history.\\r\\n\\r\\nAlternatively, a random number generator with a fixed seed value can be used directly in an orchestrator function. This approach is safe as long as the same sequence of numbers is generated for each orchestration replay.\\r\\n\\r\\n#### Bindings\\r\\n\\r\\nAn orchestrator function must not use any bindings, including even the [orchestration client](durable-functions-bindings#orchestration-client) and [entity client](durable-functions-bindings#entity-client) bindings. Always use input and output bindings from within a client or activity function. This is important because orchestrator functions may be replayed multiple times, causing nondeterministic and duplicate I/O with external systems.\\r\\n\\r\\n#### Static variables\\r\\n\\r\\nAvoid using static variables in orchestrator functions because their values can change over time, resulting in nondeterministic runtime behavior. Instead, use constants, or limit the use of static variables to activity functions.\\r\\n\\r\\nNote\\r\\n\\r\\nEven outside of orchestrator functions, using static variables in Azure Functions can be problematic for a variety of reasons since there\\'s no guarantee that static state will persist across multiple function executions. Static variables should be avoided except in very specific usecases, such as best-effort in-memory caching in activity or entity functions.\\r\\n\\r\\n#### Environment variables\\r\\n\\r\\nDo not use environment variables in orchestrator functions. Their values can change over time, resulting in nondeterministic runtime behavior. If an orchestrator function needs configuration that\\'s defined in an environment variable, you must pass the configuration value into the orchestrator function as an input or as the return value of an activity function.\\r\\n\\r\\n#### Network and HTTP\\r\\n\\r\\nUse activity functions to make outbound network calls. If you need to make an HTTP call from your orchestrator function, you also can use the [durable HTTP APIs](durable-functions-http-features#consuming-http-apis).\\r\\n\\r\\n#### Thread-blocking APIs\\r\\n\\r\\nBlocking APIs like \"sleep\" can cause performance and scale problems for orchestrator functions and should be avoided. In the Azure Functions Consumption plan, they can even result in unnecessary execution time charges. Use alternatives to blocking APIs when they\\'re available. For example, use [Durable timers](durable-functions-timers) to create delays that are safe for replay and don\\'t count towards the execution time of an orchestrator function.\\r\\n\\r\\n#### Async APIs\\r\\n\\r\\nOrchestrator code must never start any async operation except those defined by the orchestration trigger\\'s context object. For example, never use `Task.Run`, `Task.Delay`, and `HttpClient.SendAsync` in .NET or `setTimeout` and `setInterval` in JavaScript. An orchestrator function should only schedule async work using Durable SDK APIs, like scheduling activity functions. Any other type of async invocations should be done inside activity functions.\\r\\n\\r\\n#### Async JavaScript functions\\r\\n\\r\\nAlways declare JavaScript orchestrator functions as synchronous generator functions. You must not declare JavaScript orchestrator functions as `async` because the Node.js runtime doesn\\'t guarantee that asynchronous functions are deterministic.\\r\\n\\r\\n#### Python coroutines\\r\\n\\r\\nYou must not declare Python orchestrator functions as coroutines. In other words, never declare Python orchestrator functions with the `async` keyword because coroutine semantics do not align with the Durable Functions replay model. You must always declare Python orchestrator functions as generators, meaning that you should expect the `context` API to use `yield` instead of `await`.\\r\\n\\r\\n#### .NET threading APIs\\r\\n\\r\\nThe Durable Task Framework runs orchestrator code on a single thread and can\\'t interact with any other threads. Running async continuations on a worker pool thread an orchestration\\'s execution can result in nondeterministic execution or deadlocks. For this reason, orchestrator functions should almost never use threading APIs. For example, never use `ConfigureAwait(continueOnCapturedContext: false)` in an orchestrator function. This ensures that task continuations run on the orchestrator function\\'s original `SynchronizationContext`.\\r\\n\\r\\nNote\\r\\n\\r\\nThe Durable Task Framework attempts to detect accidental use of non-orchestrator threads in orchestrator functions. If it finds a violation, the framework throws a **NonDeterministicOrchestrationException** exception. However, this detection behavior won\\'t catch all violations, and you shouldn\\'t depend on it.\\r\\n\\r\\n## Versioning\\r\\n\\r\\nA durable orchestration might run continuously for days, months, years, or even [eternally](durable-functions-eternal-orchestrations). Any code updates made to Durable Functions apps that affect unfinished orchestrations might break the orchestrations\\' replay behavior. That\\'s why it\\'s important to plan carefully when making updates to code. For a more detailed description of how to version your code, see the [versioning article](durable-functions-versioning).\\r\\n\\r\\n## Durable tasks\\r\\n\\r\\nNote\\r\\n\\r\\nThis section describes internal implementation details of the Durable Task Framework. You can use durable functions without knowing this information. It is intended only to help you understand the replay behavior.\\r\\n\\r\\nTasks that can safely wait in orchestrator functions are occasionally referred to as *durable tasks*. The Durable Task Framework creates and manages these tasks. Examples are the tasks returned by `CallActivityAsync`, `WaitForExternalEvent`, and `CreateTimer` in .NET orchestrator functions.\\r\\n\\r\\nThese durable tasks are internally managed by a list of `TaskCompletionSource` objects in .NET. During replay, these tasks are created as part of orchestrator code execution. They\\'re finished as the dispatcher enumerates the corresponding history events.\\r\\n\\r\\nThe tasks are executed synchronously using a single thread until all the history has been replayed. Durable tasks that aren\\'t finished by the end of history replay have appropriate actions carried out. For example, a message might be enqueued to call an activity function.\\r\\n\\r\\nThis section\\'s description of runtime behavior should help you understand why an orchestrator function can\\'t use `await` or `yield` in a nondurable task. There are two reasons: the dispatcher thread can\\'t wait for the task to finish, and any callback by that task might potentially corrupt the tracking state of the orchestrator function. Some runtime checks are in place to help detect these violations.\\r\\n\\r\\nTo learn more about how the Durable Task Framework executes orchestrator functions, consult the [Durable Task source code on GitHub](https://github.com/Azure/durabletask). In particular, see [TaskOrchestrationExecutor.cs](https://github.com/Azure/durabletask/blob/master/src/DurableTask.Core/TaskOrchestrationExecutor.cs) and [TaskOrchestrationContext.cs](https://github.com/Azure/durabletask/blob/master/src/DurableTask.Core/TaskOrchestrationContext.cs).',\n",
       "  'metadata': {}},\n",
       " {'_id': 'fa226d2a-4a18-3e04-f76d-1764488af67d',\n",
       "  'title': 'Quickstart: Authenticate a Durable Functions app by using Microsoft Entra ID',\n",
       "  'text': \"Quickstart: Authenticate a Durable Functions app by using Microsoft Entra ID\\nMicrosoft Entra ID is a cloud-based identity and access management service. Identity-based connections allow Durable Functions, a feature of Azure Functions, to make authorized requests against Microsoft Entra-protected resources, such as an Azure Storage account, without using manually managed secrets. When Durable Functions uses the default Azure storage provider, it must authenticate against an Azure storage account.\\nIn this quickstart, you complete steps to set up a Durable Functions app to use two different kinds of identity-based connections:\\nManaged identity credentials (recommended)\\nClient secret credentials\\nIf you don't have an Azure account, create a free account before you begin.\\nPrerequisites\\nTo complete this quickstart, you need:\\nAn existing Durable Functions project created in the Azure portal or a local Durable Functions project deployed to Azure.\\nFamiliarity running a Durable Functions app in Azure.\\nIf you don't have an existing Durable Functions project deployed in Azure, we recommend that you start with one of the following quickstarts:\\nCreate a Durable Functions app - C#\\nCreate a Durable Functions app - JavaScript\\nCreate a Durable Functions app - Python\\nCreate a Durable Functions app - PowerShell\\nCreate a Durable Functions app - Java\\nConfigure your app to use managed identity credentials\\nYour app can use a managed identity to easily access other Microsoft Entra-protected resources, such as an instance of Azure Key Vault. Managed identity access is supported in the Durable Functions extension version 2.7.0 and later.\\nNote\\nA managed identity is available to apps only when they execute in Azure. When an app is configured to use identity-based connections, a locally executing app instead uses your developer credentials to authenticate with Azure resources. Then, when the app is deployed in Azure, it uses your managed identity configuration.\\nEnable a managed identity\\nTo begin, enable a managed identity for your application. Your function app must have either a system-assigned managed identity or a user-assigned managed identity. To enable a managed identity for your function app, and to learn more about the differences between the two types of identities, see the managed identity overview.\\nAssign access roles to the managed identity\\nNext, in the Azure portal, assign three role-based access control (RBAC) roles to your managed identity resource:\\nStorage Queue Data Contributor\\nStorage Blob Data Contributor\\nStorage Table Data Contributor\\nConfigure the managed identity\\nBefore you can use your app's managed identity, make some changes to the app configuration:\\nIn the Azure portal, on your function app resource menu under Settings, select Configuration.\\nIn the list of settings, select AzureWebJobsStorage and select the Delete icon.\\nAdd a setting to link your Azure storage account to the application.\\nUse one of the following methods depending on the cloud that your app runs in:\\nAzure cloud: If your app runs in public Azure, add a setting that identifies an Azure storage account name:\\nAzureWebJobsStorage__<accountName>\\nExample: AzureWebJobsStorage__mystorageaccount123\\nNon-Azure cloud: If your application runs in a cloud outside of Azure, you must add a specific service URI (an endpoint) for the storage account instead of an account name.\\nNote\\nIf you use Azure Government or any other cloud that's separate from public Azure, you must use the option to provide a specific service URI. For more information about using Azure Storage with Azure Government, see Develop by using the Storage API in Azure Government.\\nAzureWebJobsStorage__<blobServiceUri>\\nExample: AzureWebJobsStorage__https://mystorageaccount123.blob.core.windows.net/\\nAzureWebJobsStorage__<queueServiceUri>\\nExample: AzureWebJobsStorage__https://mystorageaccount123.queue.core.windows.net/\\nAzureWebJobsStorage__<tableServiceUri>\\nExample: AzureWebJobsStorage__https://mystorageaccount123.table.core.windows.net/\\nYou can get the values for these URI variables in the storage account information on the Endpoints tab.\\nFinish your managed identity configuration:\\nIf you use a system-assigned identity, make no other changes.\\nIf you use a user-assigned identity, add the following settings to your app configuration:\\nFor AzureWebJobsStorage__credential, enter managedidentity.\\nFor AzureWebJobsStorage__clientId, get this GUID value from the Microsoft Entra admin center.\\nConfigure your app to use client secret credentials\\nRegistering a client application in Microsoft Entra ID is another way you can configure access to an Azure service for your Durable Functions app. In the following steps, you use client secret credentials for authentication to your Azure Storage account. Function apps can use this method both locally and in Azure. Using a client secret credential is less recommended than using managed identity credentials because a client secret is more complex to set up and manage. A client secret credential also requires sharing a secret credential with the Azure Functions service.\\nRegister the client application with Microsoft Entra ID\\nIn the Azure portal, register the client application with Microsoft Entra ID.\\nCreate a client secret for your application. In your registered application, complete these steps:\\nSelect Certificates & secrets > New client secret.\\nFor Description, enter a unique description.\\nFor Expires, enter a valid time for the secret to expire.\\nCopy the secret value to use later.\\nThe secret's value doesn't appear again after you leave the pane, so be sure that you copy the secret and save it.\\nAssign access roles to your application\\nNext, assign three RBAC roles to your client application:\\nStorage Queue Data Contributor\\nStorage Blob Data Contributor\\nStorage Table Data Contributor\\nTo add the roles:\\nIn the Azure portal, go to your function's storage account.\\nOn the resource menu, select Access Control (IAM), and then select Add role assignment.\\nSelect a role to add, select Next, and then search for your application. Review the role assignment, and then add the role.\\nConfigure the client secret\\nIn the Azure portal, run and test the application. To run and test the app locally, specify the following settings in the function’s local.settings.json file.\\nIn the Azure portal, on your function app resource menu under Settings, select Configuration.\\nIn the list of settings, select AzureWebJobsStorage and select the Delete icon.\\nAdd a setting to link your Azure storage account to the application.\\nUse one of the following methods depending on the cloud that your app runs in:\\nAzure cloud: If your app runs in public Azure, add a setting that identifies an Azure storage account name:\\nAzureWebJobsStorage__<accountName>\\nExample: AzureWebJobsStorage__mystorageaccount123\\nNon-Azure cloud: If your application runs in a cloud outside of Azure, you must add a specific service URI (an endpoint) for the storage account instead of an account name.\\nNote\\nIf you use Azure Government or any other cloud that's separate from public Azure, you must use the option to provide a specific service URI. For more information about using Azure Storage with Azure Government, see Develop by using the Storage API in Azure Government.\\nAzureWebJobsStorage__<blobServiceUri>\\nExample: AzureWebJobsStorage__https://mystorageaccount123.blob.core.windows.net/\\nAzureWebJobsStorage__<queueServiceUri>\\nExample: AzureWebJobsStorage__https://mystorageaccount123.queue.core.windows.net/\\nAzureWebJobsStorage__<tableServiceUri>\\nExample: AzureWebJobsStorage__https://mystorageaccount123.table.core.windows.net/\\nYou can get the values for these URI variables in the storage account information on the Endpoints tab.\\nTo add client secret credentials, set the following values:\\nAzureWebJobsStorage__clientId: Get this GUID value on the Microsoft Entra application pane.\\nAzureWebJobsStorage__ClientSecret: The secret value that you generated in the Microsoft Entra admin center in an earlier step.\\nAzureWebJobsStorage__tenantId: The tenant ID that the Microsoft Entra application is registered in. Get this GUID value on the Microsoft Entra application pane.\\nThe values to use for the client ID and the tenant ID appear on your client application Overview pane. The client secret value is the one that you saved in an earlier step. The client secret's value isn't available after the pane is refreshed.\",\n",
       "  'metadata': {}},\n",
       " {'_id': '9ca9728d-ba33-e32f-757a-7e8e79c7b585',\n",
       "  'title': 'Configure Durable Functions app with managed identity',\n",
       "  'text': '# Quickstart: Configure Durable Functions with managed identity\\r\\n\\r\\nA managed identity from the access management service [Microsoft Entra ID](../../active-directory/fundamentals/active-directory-whatis) allows your app to access other Microsoft Entra protected resources, such as an Azure Storage account, without handling secrets manually. The identity is managed by the Azure platform, so you do *not* need to provision or rotate any secrets. The recommended way to authenticate access to Azure resources is through using such an identity.\\r\\n\\r\\nIn this quickstart, you complete steps to configure a Durable Functions app using the default **Azure Storage provider** to use identity-based connections for storage account access.\\r\\n\\r\\nNote\\r\\n\\r\\nManaged identity is supported in [Durable Functions extension](https://www.nuget.org/packages/Microsoft.Azure.WebJobs.Extensions.DurableTask) versions **2.7.0** and greater.\\r\\n\\r\\nIf you don\\'t have an Azure account, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nTo complete this quickstart, you need:\\r\\n\\r\\n- An existing Durable Functions project created in the Azure portal or a local Durable Functions project deployed to Azure.\\r\\n- Familiarity running a Durable Functions app in Azure.\\r\\n\\r\\nIf you don\\'t have an existing Durable Functions project deployed in Azure, we recommend that you start with one of the following quickstarts:\\r\\n\\r\\n- [Create your first durable function - C#](durable-functions-create-first-csharp)\\r\\n- [Create your first durable function - JavaScript](quickstart-js-vscode)\\r\\n- [Create your first durable function - Python](quickstart-python-vscode)\\r\\n- [Create your first durable function - PowerShell](quickstart-powershell-vscode)\\r\\n- [Create your first durable function - Java](quickstart-java)\\r\\n\\r\\n## Local development\\r\\n\\r\\n### Use Azure Storage emulator\\r\\n\\r\\nWhen developing locally, it\\'s recommended that you use Azurite, which is Azure Storage\\'s local emulator. Configure your app to the emulator by specifying `\"AzureWebJobsStorage\": \"UseDevelopmentStorage = true\"` in the local.settings.json.\\r\\n\\r\\n### Identity-based connections for local development\\r\\n\\r\\nStrictly speaking, a managed identity is only available to apps when executing on Azure. However, you can still configure a locally running app to use identity-based connection by using your developer credentials to authenticate against Azure resources. Then, when deployed on Azure, the app will utilize your managed identity configuration instead.\\r\\n\\r\\nWhen using developer credentials, the connection attempts to get a token from the following locations, in the said order, for access to your Azure resources:\\r\\n\\r\\n- A local cache shared between Microsoft applications\\r\\n- The current user context in Visual Studio\\r\\n- The current user context in Visual Studio Code\\r\\n- The current user context in the Azure CLI\\r\\n\\r\\nIf none of these options are successful, an error stating that the app cannot retrieve authentication token for your Azure resources shows up.\\r\\n\\r\\n#### Configure runtime to use local developer identity\\r\\n\\r\\n1. Specify the name of your Azure Storage account in local.settings.json, for example:\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n       \"IsEncrypted\": false,\\r\\n       \"Values\": {\\r\\n          \"AzureWebJobsStorage__accountName\": \"<<your Azure Storage account name>>\",\\r\\n          \"FUNCTIONS_WORKER_RUNTIME\": \"dotnet-isolated\"\\r\\n       }\\r\\n    }\\r\\n    ```\\r\\n2. Go to the Azure Storage account resource on the Azure portal, navigate to the **Access Control (IAM)** tab, and click on **Add role assignment**. Find the following roles:\\r\\n\\r\\n    - Storage Queue Data Contributor\\r\\n    - Storage Blob Data Contributor\\r\\n    - Storage Table Data Contributor\\r\\n\\r\\n    Assign the roles to yourself by clicking **\"+ Select members\"** and finding your email in the pop-up window. (This email is the one you use to log into Microsoft applications, Azure CLI, or editors in the Visual Studio family.)\\r\\n\\r\\n## Identity-based connections for app deployed to Azure\\r\\n\\r\\n### Enable managed identity resource\\r\\n\\r\\nTo begin, enable a managed identity for your application. Your function app must have either a system-assigned managed identity or a user-assigned managed identity. To enable a managed identity for your function app, and to learn more about the differences between the two types of identities, see the [managed identity overview](../../app-service/overview-managed-identity).\\r\\n\\r\\n### Assign access roles to the managed identity\\r\\n\\r\\nNavigate to your app\\'s Azure Storage resource on the Azure portal and [assign](/en-us/entra/identity/managed-identities-azure-resources/how-to-assign-access-azure-resource) three role-based access control (RBAC) roles to your managed identity resource:\\r\\n\\r\\n- Storage Queue Data Contributor\\r\\n- Storage Blob Data Contributor\\r\\n- Storage Table Data Contributor\\r\\n\\r\\nTo find your identity resource, select assign access to **Managed identity** and then **+ Select members**\\r\\n\\r\\n### Add managed identity configuration to your app\\r\\n\\r\\nBefore you can use your app\\'s managed identity, make some changes to the app settings:\\r\\n\\r\\n1. In the Azure portal, on your function app resource menu under **Settings**, select **Environment variables**.\\r\\n2. In the list of settings, find **AzureWebJobsStorage** and select the **Delete** icon.\\r\\n3. Add a setting to link your Azure storage account to the application.\\r\\n\\r\\n    Use *one of the following methods* depending on the cloud that your app runs in:\\r\\n\\r\\n    - **Azure cloud**: If your app runs in *global Azure*, add the setting `AzureWebJobsStorage__accountName` that identifies an Azure storage account name. Example value: `mystorageaccount123`\\r\\n    - **Non-Azure cloud**: If your application runs in a cloud outside of Azure, you must add the following three settings to provide specific service URIs (or *endpoints*) of the storage account instead of an account name.\\r\\n\\r\\n        - Setting name: `AzureWebJobsStorage__blobServiceUri`\\r\\n\\r\\n            Example value: `https://mystorageaccount123.blob.core.windows.net/`\\r\\n        - Setting name: `AzureWebJobsStorage__queueServiceUri`\\r\\n\\r\\n            Example value: `https://mystorageaccount123.queue.core.windows.net/`\\r\\n        - Setting name: `AzureWebJobsStorage__tableServiceUri`\\r\\n\\r\\n            Example value: `https://mystorageaccount123.table.core.windows.net/`\\r\\n\\r\\n    You can get the values for these URI variables in the storage account information from the **Endpoints** tab.\\r\\n\\r\\n    Note\\r\\n\\r\\n    If you are using [Azure Government](../../azure-government/documentation-government-welcome) or any other cloud that\\'s separate from global Azure, you must use the option that provides specific service URIs instead of just the storage account name. For more information on using Azure Storage with Azure Government, see the [Develop by using the Storage API in Azure Government](../../azure-government/documentation-government-get-started-connect-to-storage).\\r\\n4. Finish your managed identity configuration (remember to click \"Apply\" after making the setting changes):\\r\\n\\r\\n    - If you use a *system-assigned identity*, make no other changes.\\r\\n    - If you use a *user-assigned identity*, add the following settings in your app configuration:\\r\\n\\r\\n        - **AzureWebJobsStorage\\\\_\\\\_credential**, enter **managedidentity**\\r\\n        - **AzureWebJobsStorage\\\\_\\\\_clientId**, get this GUID value from your managed identity resource\\r\\n\\r\\n    Note\\r\\n\\r\\n    Durable Functions does *not* support `managedIdentityResourceId` when using user-assigned identity. Use `clientId` instead.',\n",
       "  'metadata': {}},\n",
       " {'_id': '2d357af8-d7e3-6679-24ed-d887d0b46e13',\n",
       "  'title': 'Create your first durable function in Azure using C#',\n",
       "  'text': 'Create your first durable function in C#\\nDurable Functions is an extension of Azure Functions that lets you write stateful functions in a serverless environment. The extension manages state, checkpoints, and restarts for you.\\nIn this article, you learn how to use Visual Studio Code to locally create and test a \"hello world\" durable function. This function orchestrates and chains together calls to other functions. You can then publish the function code to Azure. These tools are available as part of the Visual Studio Code Azure Functions extension.\\nPrerequisites\\nTo complete this tutorial:\\nInstall Visual Studio Code.\\nInstall the following Visual Studio Code extensions:\\nAzure Functions\\nC#\\nMake sure that you have the latest version of the Azure Functions Core Tools.\\nDurable Functions require an Azure storage account. You need an Azure subscription.\\nMake sure that you have version 3.1 or a later version of the .NET Core SDK installed.\\nIf you don\\'t have an Azure subscription, create an Azure free account before you begin.\\nCreate your local project\\nIn this section, you use Visual Studio Code to create a local Azure Functions project.\\nIn Visual Studio Code, press F1 (or Ctrl/Cmd+Shift+P) to open the command palette. In the command palette, search for and select Azure Functions: Create New Project....\\nChoose an empty folder location for your project and choose Select.\\nFollow the prompts and provide the following information:\\nPrompt\\nValue\\nDescription\\nSelect a language for your function app project\\nC#\\nCreate a local C# Functions project.\\nSelect a version\\nAzure Functions v4\\nYou only see this option when the Core Tools aren\\'t already installed. In this case, Core Tools are installed the first time you run the app.\\nSelect a template for your project\\'s first function\\nSkip for now\\nSelect how you would like to open your project\\nOpen in current window\\nReopens Visual Studio Code in the folder you selected.\\nVisual Studio Code installs the Azure Functions Core Tools if needed. It also creates a function app project in a folder. This project contains the host.json and local.settings.json configuration files.\\nAdd functions to the app\\nThe following steps use a template to create the durable function code in your project.\\nIn the command palette, search for and select Azure Functions: Create Function....\\nFollow the prompts and provide the following information:\\nPrompt\\nValue\\nDescription\\nSelect a template for your function\\nDurableFunctionsOrchestration\\nCreate a Durable Functions orchestration\\nProvide a function name\\nHelloOrchestration\\nName of the class in which functions are created\\nProvide a namespace\\nCompany.Function\\nNamespace for the generated class\\nWhen Visual Studio Code prompts you to select a storage account, choose Select storage account. Follow the prompts and provide the following information to create a new storage account in Azure:\\nPrompt\\nValue\\nDescription\\nSelect subscription\\nname of your subscription\\nSelect your Azure subscription\\nSelect a storage account\\nCreate a new storage account\\nEnter the name of the new storage account\\nunique name\\nName of the storage account to create\\nSelect a resource group\\nunique name\\nName of the resource group to create\\nSelect a location\\nregion\\nSelect a region close to you\\nA class containing the new functions is added to the project. Visual Studio Code also adds the storage account connection string to local.settings.json and a reference to the Microsoft.Azure.WebJobs.Extensions.DurableTask NuGet package to the .csproj project file.\\nOpen the new HelloOrchestration.cs file to view the contents. This durable function is a simple function chaining example with the following methods:\\nMethod\\nFunctionName\\nDescription\\nRunOrchestrator\\nHelloOrchestration\\nManages the durable orchestration. In this case, the orchestration starts, creates a list, and adds the result of three functions calls to the list. When the three function calls are complete, it returns the list.\\nSayHello\\nHelloOrchestration_Hello\\nThe function returns a hello. It\\'s the function that contains the business logic that is being orchestrated.\\nHttpStart\\nHelloOrchestration_HttpStart\\nAn HTTP-triggered function that starts an instance of the orchestration and returns a check status response.\\nNow that you\\'ve created your function project and a durable function, you can test it on your local computer.\\nTest the function locally\\nAzure Functions Core Tools lets you run an Azure Functions project on your local development computer. You\\'re prompted to install these tools the first time you start a function from Visual Studio Code.\\nTo test your function, set a breakpoint in the SayHello activity function code and press F5 to start the function app project. Output from Core Tools is displayed in the Terminal panel.\\nNote\\nFor more information on debugging, see Durable Functions Diagnostics.\\nIn the Terminal panel, copy the URL endpoint of your HTTP-triggered function.\\nUse a tool like Postman or cURL, and then send an HTTP POST request to the URL endpoint.\\nThe response is the HTTP function\\'s initial result, letting us know that the durable orchestration has started successfully. It isn\\'t yet the end result of the orchestration. The response includes a few useful URLs. For now, let\\'s query the status of the orchestration.\\nCopy the URL value for statusQueryGetUri, paste it into the browser\\'s address bar, and execute the request. Alternatively, you can also continue to use Postman to issue the GET request.\\nThe request will query the orchestration instance for the status. You must get an eventual response, which shows us that the instance has completed and includes the outputs or results of the durable function. It looks like:\\nTo stop debugging, press Shift + F5 in Visual Studio Code.\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to publish the project to Azure.\\nSign in to Azure\\nBefore you can create Azure resources or publish your app, you must sign in to Azure.\\nIf you aren\\'t already signed in, choose the Azure icon in the Activity bar. Then in the Resources area, choose Sign in to Azure....\\nIf you\\'re already signed in and can see your existing subscriptions, go to the next section. If you don\\'t yet have an Azure account, choose Create an Azure Account.... Students can choose Create an Azure for Students Account....\\nWhen prompted in the browser, choose your Azure account and sign in using your Azure account credentials. If you create a new account, you can sign in after your account is created.\\nAfter you\\'ve successfully signed in, you can close the new browser window. The subscriptions that belong to your Azure account are displayed in the sidebar.\\nCreate the function app in Azure\\nIn this section, you create a function app and related resources in your Azure subscription.\\nIn Visual Studio Code, press F1 to open the command palette and search for and run the command Azure Functions: Create Function App in Azure....\\nProvide the following information at the prompts:\\nPrompt\\nSelection\\nSelect subscription\\nChoose the subscription to use. You won\\'t see this prompt when you have only one subscription visible under Resources.\\nEnter a globally unique name for the function app\\nType a name that is valid in a URL path. The name you type is validated to make sure that it\\'s unique in Azure Functions.\\nSelect a runtime stack\\nChoose the language version on which you\\'ve been running locally.\\nSelect a location for new resources\\nFor better performance, choose a region near you.\\nThe extension shows the status of individual resources as they\\'re being created in Azure in the Azure: Activity Log panel.\\nWhen the creation is complete, the following Azure resources are created in your subscription. The resources are named based on your function app name:\\nA resource group, which is a logical container for related resources.\\nA standard Azure Storage account, which maintains state and other information about your projects.\\nA function app, which provides the environment for executing your function code. A function app lets you group functions as a logical unit for easier management, deployment, and sharing of resources within the same hosting plan.\\nAn App Service plan, which defines the underlying host for your function app.\\nAn Application Insights instance connected to the function app, which tracks usage of your functions in the app.\\nA notification is displayed after your function app is created and the deployment package is applied.\\nTip\\nBy default, the Azure resources required by your function app are created based on the function app name you provide. By default, they\\'re also created in the same new resource group with the function app. If you want to either customize the names of these resources or reuse existing resources, you need to publish the project with advanced create options instead.\\nDeploy the project to Azure\\nImportant\\nDeploying to an existing function app always overwrites the contents of that app in Azure.\\nIn the command palette, search for and run the command Azure Functions: Deploy to Function App....\\nSelect the function app you just created. When prompted about overwriting previous deployments, select Deploy to deploy your function code to the new function app resource.\\nAfter deployment completes, select View Output to view the creation and deployment results, including the Azure resources that you created. If you miss the notification, select the bell icon in the lower right corner to see it again.\\nTest your function in Azure\\nCopy the URL of the HTTP trigger from the Output panel. The URL that calls your HTTP-triggered function must be in the following format:\\nhttps://<functionappname>.azurewebsites.net/api/HelloOrchestration_HttpStart\\nPaste this new URL for the HTTP request into your browser\\'s address bar. You must get the same status response as before when using the published app.\\nNext steps\\nYou have used Visual Studio Code to create and publish a C# durable function app.\\nLearn about common durable function patterns\\nIn this article, you learn how to use Visual Studio 2022 to locally create and test a \"hello world\" durable function. This function orchestrates and chains-together calls to other functions. You then publish the function code to Azure. These tools are available as part of the Azure development workload in Visual Studio 2022.\\nPrerequisites\\nTo complete this tutorial:\\nInstall Visual Studio 2022. Make sure that the Azure development workload is also installed. Visual Studio 2019 also supports Durable Functions development, but the UI and steps differ.\\nVerify that you have the Azurite Emulator installed and running.\\nIf you don\\'t have an Azure subscription, create an Azure free account before you begin.\\nCreate a function app project\\nThe Azure Functions template creates a project that can be published to a function app in Azure. A function app lets you group functions as a logical unit for easier management, deployment, scaling, and sharing of resources.\\nIn Visual Studio, select New > Project from the File menu.\\nIn the Create a new project dialog, search for functions, choose the Azure Functions template, and then select Next.\\nEnter a Project name for your project, and select OK. The project name must be valid as a C# namespace, so don\\'t use underscores, hyphens, or nonalphanumeric characters.\\nUnder Additional information, use the settings specified in the table that follows the image.\\nSetting\\nSuggested value\\nDescription\\nFunctions worker\\n.NET 6\\nCreates a function project that supports .NET 6 and the Azure Functions Runtime 4.0. For more information, see How to target Azure Functions runtime version.\\nFunction\\nEmpty\\nCreates an empty function app.\\nStorage account\\nStorage Emulator\\nA storage account is required for durable function state management.\\nSelect Create to create an empty function project. This project has the basic configuration files needed to run your functions.\\nAdd functions to the app\\nThe following steps use a template to create the durable function code in your project.\\nRight-click the project in Visual Studio and select Add > New Azure Function.\\nVerify Azure Function is selected from the add menu, enter a name for your C# file, and then select Add.\\nSelect the Durable Functions Orchestration template and then select Add.\\nA new durable function is added to the app. Open the new .cs file to view the contents. This durable function is a simple function chaining example with the following methods:\\nMethod\\nFunctionName\\nDescription\\nRunOrchestrator\\n<file-name>\\nManages the durable orchestration. In this case, the orchestration starts, creates a list, and adds the result of three functions calls to the list. When the three function calls are complete, it returns the list.\\nSayHello\\n<file-name>_Hello\\nThe function returns a hello. It\\'s the function that contains the business logic that is being orchestrated.\\nHttpStart\\n<file-name>_HttpStart\\nAn HTTP-triggered function that starts an instance of the orchestration and returns a check status response.\\nYou can test it on your local computer now that you\\'ve created your function project and a durable function.\\nTest the function locally\\nAzure Functions Core Tools lets you run an Azure Functions project on your local development computer. You\\'re prompted to install these tools the first time you start a function from Visual Studio.\\nTo test your function, press F5. If prompted, accept the request from Visual Studio to download and install Azure Functions Core (CLI) tools. You may also need to enable a firewall exception so that the tools can handle HTTP requests.\\nCopy the URL of your function from the Azure Functions runtime output.\\nPaste the URL for the HTTP request into your browser\\'s address bar and execute the request. The following shows the response in the browser to the local GET request returned by the function:\\nThe response is the HTTP function\\'s initial result, letting us know that the durable orchestration has started successfully. It isn\\'t yet the end result of the orchestration. The response includes a few useful URLs. For now, let\\'s query the status of the orchestration.\\nCopy the URL value for statusQueryGetUri, paste it into the browser\\'s address bar, and execute the request.\\nThe request will query the orchestration instance for the status. You must get an eventual response that looks like the following.  This output shows us the instance has completed and includes the outputs or results of the durable function.\\nTo stop debugging, press Shift + F5.\\nAfter you\\'ve verified that the function runs correctly on your local computer, it\\'s time to publish the project to Azure.\\nPublish the project to Azure\\nYou must have a function app in your Azure subscription before publishing your project. You can create a function app right from Visual Studio.\\nIn Solution Explorer, right-click the project and select Publish. In Target, select Azure then Next.\\nSelect Azure Function App (Windows) for the Specific target, which creates a function app that runs on Windows, and then select Next.\\nIn the Function Instance, choose Create a new Azure Function...\\nCreate a new instance using the values specified in the following table:\\nSetting\\nValue\\nDescription\\nName\\nGlobally unique name\\nName that uniquely identifies your new function app. Accept this name or enter a new name. Valid characters are: a-z, 0-9, and -.\\nSubscription\\nYour subscription\\nThe Azure subscription to use. Accept this subscription or select a new one from the drop-down list.\\nResource group\\nName of your resource group\\nThe resource group in which you want to create your function app. Select New to create a new resource group. You can also choose an existing resource group from the drop-down list.\\nPlan Type\\nConsumption\\nWhen you publish your project to a function app that runs in a Consumption plan, you pay only for executions of your functions app. Other hosting plans incur higher costs.\\nLocation\\nLocation of the app service\\nChoose a Location in a region near you or other services your functions access.\\nAzure Storage\\nGeneral-purpose storage account\\nAn Azure storage account is required by the Functions runtime. Select New to configure a general-purpose storage account. You can also choose an existing account that meets the storage account requirements.\\nApplication Insights\\nApplication Insights instance\\nYou should enable Application Insights integration for your function app. Select New to create a new instance, either in a new or in an existing Log Analytics workspace. You can also choose an existing instance.\\nSelect Create to create a function app and its related resources in Azure. The status of resource creation is shown in the lower-left of the window.\\nIn the Functions instance, make sure that the Run from package file is checked. Your function app is deployed using Zip Deploy with Run-From-Package mode enabled. Zip Deploy is the recommended deployment method for your functions project resulting in better performance.\\nSelect Finish, and on the Publish page, select Publish to deploy the package containing your project files to your new function app in Azure.\\nAfter the deployment completes, the root URL of the function app in Azure is shown in the Publish tab.\\nIn the Publish tab, in the Hosting section, choose Open in Azure portal. This opens the new function app Azure resource in the Azure portal.\\nTest your function in Azure\\nCopy the base URL of the function app from the Publish profile page. Replace the localhost:port portion of the URL you used when testing the function locally with the new base URL.\\nThe URL that calls your durable function HTTP trigger must be in the following format:\\nhttps://<APP_NAME>.azurewebsites.net/api/<FUNCTION_NAME>_HttpStart\\nPaste this new URL for the HTTP request into your browser\\'s address bar. You must get the same status response as before when using the published app.\\nNext steps\\nYou have used Visual Studio to create and publish a C# durable function app.\\nLearn about common durable function patterns',\n",
       "  'metadata': {}},\n",
       " {'_id': '417a31a8-ccb2-31bb-4e69-854580e2293a',\n",
       "  'title': 'Create Durable Functions using the Azure portal',\n",
       "  'text': '# Create Durable Functions using the Azure portal\\r\\n\\r\\nThe [Durable Functions](durable-functions-overview) extension for Azure Functions is provided in the NuGet package [Microsoft.Azure.WebJobs.Extensions.DurableTask](https://www.nuget.org/packages/Microsoft.Azure.WebJobs.Extensions.DurableTask). This extension must be installed in your function app. This article shows how to install this package so that you can develop durable functions in the Azure portal.\\r\\n\\r\\nNote\\r\\n\\r\\n- If you are developing durable functions in C#, you should instead consider [Visual Studio 2019 development](durable-functions-isolated-create-first-csharp).\\r\\n- If you are developing durable functions in JavaScript, you should instead consider [Visual Studio Code development](quickstart-js-vscode).\\r\\n\\r\\n## Create a function app\\r\\n\\r\\nYou must have a function app to host the execution of any function. A function app lets you group your functions as a logical unit for easier management, deployment, scaling, and sharing of resources. You can create a .NET or JavaScript app.\\r\\n\\r\\n1. From the Azure portal menu or the **Home** page, select **Create a resource**.\\r\\n2. In the **New** page, select **Compute** &gt; **Function App**.\\r\\n3. Under **Select a hosting option**, select **Consumption** &gt; **Select** to create your app in the default **Consumption** plan. In this [serverless](https://azure.microsoft.com/overview/serverless-computing/) hosting option, you pay only for the time your functions run. [Premium plan](../functions-premium-plan) also offers dynamic scaling. When you run in an App Service plan, you must manage the [scaling of your function app](../functions-scale).\\r\\n4. On the **Basics** page, use the function app settings as specified in the following table:\\r\\n\\r\\n    | Setting | Suggested value | Description |\\r\\n    | --- | --- | --- |\\r\\n    | **Subscription** | Your subscription | The subscription under which you create your new function app. |\\r\\n    | **[Resource Group](../../azure-resource-manager/management/overview)** | *myResourceGroup* | Name for the new resource group in which you create your function app. You should create a new resource group because there are [known limitations when creating new function apps in an existing resource group](../functions-scale#limitations-for-creating-new-function-apps-in-an-existing-resource-group). |\\r\\n    | **Function App name** | Globally unique name | Name that identifies your new function app. Valid characters are `a-z` (case insensitive), `0-9`, and `-`. |\\r\\n    | **Runtime stack** | Preferred language | Choose a runtime that supports your favorite function programming language. In-portal editing is only available for JavaScript, PowerShell, Python, TypeScript, and C# script.To create a C# Script app that supports in-portal editing, you must choose a runtime **Version** that supports the **in-process model**.C# class library and Java functions must be [developed locally](../functions-develop-local#local-development-environments). |\\r\\n    | **Version** | Version number | Choose the version of your installed runtime. |\\r\\n    | **Region** | Preferred region | Select a [region](https://azure.microsoft.com/regions/) that\\'s near you or near other services that your functions can access. |\\r\\n    | **Operating system** | Windows | An operating system is preselected for you based on your runtime stack selection, but you can change the setting if necessary. In-portal editing is only supported on Windows. |\\r\\n5. Accept the default options in the remaining tabs, including the default behavior of creating a new storage account on the **Storage** tab and a new Application Insight instance on the **Monitoring** tab. You can also choose to use an existing storage account or Application Insights instance.\\r\\n6. Select **Review + create** to review the app configuration you chose, and then select **Create** to provision and deploy the function app.\\r\\n7. Select the **Notifications** icon in the upper-right corner of the portal and watch for the **Deployment succeeded** message.\\r\\n8. Select **Go to resource** to view your new function app. You can also select **Pin to dashboard**. Pinning makes it easier to return to this function app resource from your dashboard.\\r\\n\\r\\nBy default, the function app created uses version 2.x of the Azure Functions runtime. The Durable Functions extension works on both versions 1.x and 2.x of the Azure Functions runtime in C#, and version 2.x in JavaScript. However, templates are only available when targeting version 2.x of the runtime regardless of the chosen language.\\r\\n\\r\\n## Install the durable-functions npm package (JavaScript only)\\r\\n\\r\\nIf you are creating JavaScript Durable Functions, you\\'ll need to install the [`durable-functions` npm package](https://www.npmjs.com/package/durable-functions):\\r\\n\\r\\n1. From your function app\\'s page, select **Advanced Tools** under **Development Tools** in the left pane.\\r\\n2. In the **Advanced Tools** page, select **Go**.\\r\\n3. Inside the Kudu console, select **Debug console**, and then **CMD**.\\r\\n4. Your function app\\'s file directory structure should display. Navigate to the `site/wwwroot` folder. From there, you can upload a `package.json` file by dragging and dropping it into the file directory window. A sample `package.json` is below:\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n      \"dependencies\": {\\r\\n        \"durable-functions\": \"^1.3.1\"\\r\\n      }\\r\\n    }\\r\\n    ```\\r\\n5. Once your `package.json` is uploaded, run the `npm install` command from the Kudu Remote Execution Console.\\r\\n\\r\\n## Create an orchestrator function\\r\\n\\r\\n1. In your function app, select **Functions** from the left pane, and then select **Add** from the top menu.\\r\\n2. In the search field of the **New Function** page, enter `durable`, and then choose the **Durable Functions HTTP starter** template.\\r\\n3. For the **New Function** name, enter `HttpStart`, and then select **Create Function**.\\r\\n\\r\\n    The function created is used to start the orchestration.\\r\\n4. Create another function in the function app, this time by using the **Durable Functions orchestrator** template. Name your new orchestration function `HelloSequence`.\\r\\n5. Create a third function named `Hello` by using the **Durable Functions activity** template.\\r\\n\\r\\n## Test the durable function orchestration\\r\\n\\r\\n1. Go back to the **HttpStart** function, choose **Get function Url**, and select the **Copy to clipboard** icon to copy the URL. You use this URL to start the **HelloSequence** function.\\r\\n2. Use a secure HTTP test tool to send an HTTP POST request to the URL endpoint. This example is a cURL command that sends a POST request to the durable function:\\r\\n\\r\\n    ```bash\\r\\n    curl -X POST https://{your-function-app-name}.azurewebsites.net/api/orchestrators/{functionName} --header \"Content-Length: 0\"\\r\\n    ```\\r\\n\\r\\n    In this example, `{your-function-app-name}` is the domain that is the name of your function app, and `{functionName}` is the **HelloSequence** orchestrator function. The response message contains a set of URI endpoints that you can use to monitor and manage the execution, which looks like the following example:\\r\\n\\r\\n    ```json\\r\\n    {  \\r\\n       \"id\":\"10585834a930427195479de25e0b952d\",\\r\\n       \"statusQueryGetUri\":\"https://...\",\\r\\n       \"sendEventPostUri\":\"https://...\",\\r\\n       \"terminatePostUri\":\"https://...\",\\r\\n       \"rewindPostUri\":\"https://...\"\\r\\n    }\\r\\n    ```\\r\\n\\r\\n    Make sure to choose an HTTP test tool that keeps your data secure. For more information, see [HTTP test tools](../functions-develop-local#http-test-tools).\\r\\n3. Call the `statusQueryGetUri` endpoint URI and you see the current status of the durable function, which might look like this example:\\r\\n\\r\\n    ```json\\r\\n        {\\r\\n            \"runtimeStatus\": \"Running\",\\r\\n            \"input\": null,\\r\\n            \"output\": null,\\r\\n            \"createdTime\": \"2017-12-01T05:37:33Z\",\\r\\n            \"lastUpdatedTime\": \"2017-12-01T05:37:36Z\"\\r\\n        }\\r\\n    ```\\r\\n4. Continue calling the `statusQueryGetUri` endpoint until the status changes to **Completed**, and you see a response like the following example:\\r\\n\\r\\n    ```json\\r\\n    {\\r\\n            \"runtimeStatus\": \"Completed\",\\r\\n            \"input\": null,\\r\\n            \"output\": [\\r\\n                \"Hello Tokyo!\",\\r\\n                \"Hello Seattle!\",\\r\\n                \"Hello London!\"\\r\\n            ],\\r\\n            \"createdTime\": \"2017-12-01T05:38:22Z\",\\r\\n            \"lastUpdatedTime\": \"2017-12-01T05:38:28Z\"\\r\\n        }\\r\\n    ```\\r\\n\\r\\nYour first durable function is now up and running in Azure.',\n",
       "  'metadata': {}},\n",
       " {'_id': '52fdd3e9-b916-d71a-7d1b-6b77fe941aad',\n",
       "  'title': 'Custom orchestration status in Durable Functions - Azure',\n",
       "  'text': '# Custom orchestration status in Durable Functions (Azure Functions)\\r\\n\\r\\nCustom orchestration status lets you set a custom status value for your orchestrator function. This status is provided via the [HTTP GetStatus API](durable-functions-http-api#get-instance-status) or the equivalent [SDK API](durable-functions-instance-management#query-instances) on the orchestration client object.\\r\\n\\r\\n## Sample use cases\\r\\n\\r\\n### Visualize progress\\r\\n\\r\\nClients can poll the status end point and display a progress UI that visualizes the current execution stage. The following sample demonstrates progress sharing:\\r\\n\\r\\n**C#**\\r\\nNote\\r\\n\\r\\nThese C# examples are written for Durable Functions 2.x and are not compatible with Durable Functions 1.x. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"E1_HelloSequence\")]\\r\\npublic static async Task<List<string>> Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    var outputs = new List<string>();\\r\\n\\r\\n    outputs.Add(await context.CallActivityAsync<string>(\"E1_SayHello\", \"Tokyo\"));\\r\\n    context.SetCustomStatus(\"Tokyo\");\\r\\n    outputs.Add(await context.CallActivityAsync<string>(\"E1_SayHello\", \"Seattle\"));\\r\\n    context.SetCustomStatus(\"Seattle\");\\r\\n    outputs.Add(await context.CallActivityAsync<string>(\"E1_SayHello\", \"London\"));\\r\\n    context.SetCustomStatus(\"London\");\\r\\n\\r\\n    // returns [\"Hello Tokyo!\", \"Hello Seattle!\", \"Hello London!\"]\\r\\n    return outputs;\\r\\n}\\r\\n\\r\\n[FunctionName(\"E1_SayHello\")]\\r\\npublic static string SayHello([ActivityTrigger] string name)\\r\\n{\\r\\n    return $\"Hello {name}!\";\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\n`E1_HelloSequence` orchestrator function:\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context){\\r\\n    const outputs = [];\\r\\n\\r\\n    outputs.push(yield context.df.callActivity(\"E1_SayHello\", \"Tokyo\"));\\r\\n    context.df.setCustomStatus(\"Tokyo\");\\r\\n    outputs.push(yield context.df.callActivity(\"E1_SayHello\", \"Seattle\"));\\r\\n    context.df.setCustomStatus(\"Seattle\");\\r\\n    outputs.push(yield context.df.callActivity(\"E1_SayHello\", \"London\"));\\r\\n    context.df.setCustomStatus(\"London\");\\r\\n\\r\\n    // returns [\"Hello Tokyo!\", \"Hello Seattle!\", \"Hello London!\"]\\r\\n    return outputs;\\r\\n});\\r\\n```\\r\\n\\r\\n`E1_SayHello` activity function:\\r\\n\\r\\n```javascript\\r\\nmodule.exports = async function(context, name) {\\r\\n    return `Hello ${name}!`;\\r\\n};\\r\\n```\\r\\n\\r\\n**Python**\\r\\n### `E1_HelloSequence` Orchestrator function\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    \\r\\n    output1 = yield context.call_activity(\\'E1_SayHello\\', \\'Tokyo\\')\\r\\n    context.set_custom_status(\\'Tokyo\\')\\r\\n    output2 = yield context.call_activity(\\'E1_SayHello\\', \\'Seattle\\')\\r\\n    context.set_custom_status(\\'Seattle\\')\\r\\n    output3 = yield context.call_activity(\\'E1_SayHello\\', \\'London\\')\\r\\n    context.set_custom_status(\\'London\\')\\r\\n    \\r\\n    return [output1, output2, output3]\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n### `E1_SayHello` Activity function\\r\\n\\r\\n```python\\r\\ndef main(name: str) -> str:\\r\\n    return f\"Hello {name}!\"\\r\\n\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n### `E1_HelloSequence` Orchestrator function\\r\\n\\r\\n```powershell\\r\\nparam($Context)\\r\\n\\r\\n$output = @()\\r\\n\\r\\n$output += Invoke-DurableActivity -FunctionName \\'E1_SayHello\\' -Input \\'Tokyo\\'\\r\\nSet-DurableCustomStatus -CustomStatus \\'Tokyo\\'\\r\\n\\r\\n$output += Invoke-DurableActivity -FunctionName \\'E1_SayHello\\' -Input \\'Seattle\\'\\r\\nSet-DurableCustomStatus -CustomStatus \\'Seattle\\'\\r\\n\\r\\n$output += Invoke-DurableActivity -FunctionName \\'E1_SayHello\\' -Input \\'London\\'\\r\\nSet-DurableCustomStatus -CustomStatus \\'London\\'\\r\\n\\r\\nreturn $output\\r\\n```\\r\\n\\r\\n### `E1_SayHello` Activity function\\r\\n\\r\\n```powershell\\r\\nparam($name)\\r\\n\\r\\n\"Hello $name\"\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"HelloCities\")\\r\\npublic String helloCitiesOrchestrator(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    String result = \"\";\\r\\n    result += ctx.callActivity(\"SayHello\", \"Tokyo\", String.class).await() + \", \";\\r\\n    ctx.setCustomStatus(\"Tokyo\");\\r\\n    result += ctx.callActivity(\"SayHello\", \"London\", String.class).await() + \", \";\\r\\n    ctx.setCustomStatus(\"London\");\\r\\n    result += ctx.callActivity(\"SayHello\", \"Seattle\", String.class).await();\\r\\n    ctx.setCustomStatus(\"Seattle\");\\r\\n    return result;\\r\\n}\\r\\n\\r\\n@FunctionName(\"SayHello\")\\r\\npublic String sayHello(@DurableActivityTrigger(name = \"name\") String name) {\\r\\n    return String.format(\"Hello %s!\", name);\\r\\n}\\r\\n```\\r\\n\\r\\nAnd then the client will receive the output of the orchestration only when `CustomStatus` field is set to \"London\":\\r\\n\\r\\n**C#**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"HttpStart\")]\\r\\npublic static async Task<HttpResponseMessage> Run(\\r\\n    [HttpTrigger(AuthorizationLevel.Function, methods: \"post\", Route = \"orchestrators/{functionName}\")] HttpRequestMessage req,\\r\\n    [DurableClient] IDurableOrchestrationClient starter,\\r\\n    string functionName,\\r\\n    ILogger log)\\r\\n{\\r\\n    // Function input comes from the request content.\\r\\n    dynamic eventData = await req.Content.ReadAsAsync<object>();\\r\\n    string instanceId = await starter.StartNewAsync(functionName, (string)eventData);\\r\\n\\r\\n    log.LogInformation($\"Started orchestration with ID = \\'{instanceId}\\'.\");\\r\\n\\r\\n    DurableOrchestrationStatus durableOrchestrationStatus = await starter.GetStatusAsync(instanceId);\\r\\n    while (durableOrchestrationStatus.CustomStatus.ToString() != \"London\")\\r\\n    {\\r\\n        await Task.Delay(200);\\r\\n        durableOrchestrationStatus = await starter.GetStatusAsync(instanceId);\\r\\n    }\\r\\n\\r\\n    HttpResponseMessage httpResponseMessage = new HttpResponseMessage(HttpStatusCode.OK)\\r\\n    {\\r\\n        Content = new StringContent(JsonConvert.SerializeObject(durableOrchestrationStatus))\\r\\n    };\\r\\n\\r\\n    return httpResponseMessage;\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = async function(context, req) {\\r\\n    const client = df.getClient(context);\\r\\n\\r\\n    // Function input comes from the request content.\\r\\n    const eventData = req.body;\\r\\n    const instanceId = await client.startNew(req.params.functionName, undefined, eventData);\\r\\n\\r\\n    context.log(`Started orchestration with ID = \\'${instanceId}\\'.`);\\r\\n\\r\\n    let durableOrchestrationStatus = await client.getStatus(instanceId);\\r\\n    while (durableOrchestrationStatus.customStatus.toString() !== \"London\") {\\r\\n        await new Promise((resolve) => setTimeout(resolve, 200));\\r\\n        durableOrchestrationStatus = await client.getStatus(instanceId);\\r\\n    }\\r\\n\\r\\n    const httpResponseMessage = {\\r\\n        status: 200,\\r\\n        body: JSON.stringify(durableOrchestrationStatus),\\r\\n    };\\r\\n\\r\\n    return httpResponseMessage;\\r\\n};\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nIn JavaScript, the `customStatus` field will be set when the next `yield` or `return` action is scheduled.\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport json\\r\\nimport logging\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\nfrom time import sleep\\r\\n\\r\\nasync def main(req: func.HttpRequest, starter: str) -> func.HttpResponse:\\r\\n    client = df.DurableOrchestrationClient(starter)    \\r\\n    instance_id = await client.start_new(req.params.functionName, None, None)\\r\\n\\r\\n    logging.info(f\"Started orchestration with ID = \\'{instance_id}\\'.\")\\r\\n\\r\\n    durable_orchestration_status = await client.get_status(instance_id)\\r\\n    while durable_orchestration_status.custom_status != \\'London\\':\\r\\n        sleep(0.2)\\r\\n        durable_orchestration_status = await client.get_status(instance_id)\\r\\n\\r\\n    return func.HttpResponse(body=\\'Success\\', status_code=200, mimetype=\\'application/json\\')\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nIn Python, the `custom_status` field will be set when the next `yield` or `return` action is scheduled.\\r\\n\\r\\n**PowerShell**\\r\\nThe feature is not currently implemented in PowerShell\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"StartHelloCities\")\\r\\npublic HttpResponseMessage startHelloCities(\\r\\n        @HttpTrigger(name = \"req\") HttpRequestMessage<Void> req,\\r\\n        @DurableClientInput(name = \"durableContext\") DurableClientContext durableContext,\\r\\n        final ExecutionContext context) throws InterruptedException {\\r\\n\\r\\n    DurableTaskClient client = durableContext.getClient();\\r\\n    String instanceId = client.scheduleNewOrchestrationInstance(\"HelloCities\");\\r\\n    context.getLogger().info(\"Created new Java orchestration with instance ID = \" + instanceId);\\r\\n\\r\\n    OrchestrationMetadata metadata;\\r\\n    try {\\r\\n        metadata = client.waitForInstanceStart(instanceId, Duration.ofMinutes(5), true);\\r\\n    } catch (TimeoutException ex) {\\r\\n        return req.createResponseBuilder(HttpStatus.INTERNAL_SERVER_ERROR).build();\\r\\n    }\\r\\n\\r\\n    while (metadata.readCustomStatusAs(String.class) != \"London\") {\\r\\n        Thread.sleep(200);\\r\\n        metadata = client.getInstanceMetadata(instanceId, true);\\r\\n    }\\r\\n\\r\\n    return req.createResponseBuilder(HttpStatus.OK).build();\\r\\n}\\r\\n```\\r\\n\\r\\n### Output customization\\r\\n\\r\\nAnother interesting scenario is segmenting users by returning customized output based on unique characteristics or interactions. With the help of custom orchestration status, the client-side code will stay generic. All main modifications will happen on the server side as shown in the following sample:\\r\\n\\r\\n**C#**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"CityRecommender\")]\\r\\npublic static void Run(\\r\\n  [OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n  int userChoice = context.GetInput<int>();\\r\\n\\r\\n  switch (userChoice)\\r\\n  {\\r\\n    case 1:\\r\\n    context.SetCustomStatus(new\\r\\n    {\\r\\n      recommendedCities = new[] {\"Tokyo\", \"Seattle\"},\\r\\n      recommendedSeasons = new[] {\"Spring\", \"Summer\"}\\r\\n     });\\r\\n      break;\\r\\n    case 2:\\r\\n      context.SetCustomStatus(new\\r\\n      {\\r\\n        recommendedCities = new[] {\"Seattle, London\"},\\r\\n        recommendedSeasons = new[] {\"Summer\"}\\r\\n      });\\r\\n        break;\\r\\n      case 3:\\r\\n      context.SetCustomStatus(new\\r\\n      {\\r\\n        recommendedCities = new[] {\"Tokyo, London\"},\\r\\n        recommendedSeasons = new[] {\"Spring\", \"Summer\"}\\r\\n      });\\r\\n        break;\\r\\n  }\\r\\n\\r\\n  // Wait for user selection and refine the recommendation\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\n#### `CityRecommender` orchestrator\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context) {\\r\\n    const userChoice = context.df.getInput();\\r\\n\\r\\n    switch (userChoice) {\\r\\n        case 1:\\r\\n            context.df.setCustomStatus({\\r\\n                recommendedCities: [ \"Tokyo\", \"Seattle\" ],\\r\\n                recommendedSeasons: [ \"Spring\", \"Summer\" ],\\r\\n            });\\r\\n            break;\\r\\n        case 2:\\r\\n            context.df.setCustomStatus({\\r\\n                recommendedCities: [ \"Seattle\", \"London\" ],\\r\\n                recommendedSeasons: [ \"Summer\" ],\\r\\n            });\\r\\n            break;\\r\\n        case 3:\\r\\n            context.df.setCustomStatus({\\r\\n                recommendedCity: [ \"Tokyo\", \"London\" ],\\r\\n                recommendedSeasons: [ \"Spring\", \"Summer\" ],\\r\\n            });\\r\\n            break;\\r\\n    }\\r\\n\\r\\n    // Wait for user selection and refine the recommendation\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n#### `CityRecommender` orchestrator\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    userChoice = int(context.get_input())\\r\\n\\r\\n    if userChoice == 1:\\r\\n        context.set_custom_status({\\r\\n            \\'recommendedCities\\' : [\\'Tokyo\\', \\'Seattle\\'], \\r\\n            \\'recommendedSeasons\\' : [\\'Spring\\', \\'Summer\\']\\r\\n        }))\\r\\n    if userChoice == 2:\\r\\n        context.set_custom_status({\\r\\n            \\'recommendedCities\\' : [\\'Seattle\\', \\'London\\']\\r\\n            \\'recommendedSeasons\\' : [\\'Summer\\']\\r\\n        }))\\r\\n    if userChoice == 3:\\r\\n        context.set_custom_status({\\r\\n            \\'recommendedCities\\' : [\\'Tokyo\\', \\'London\\'], \\r\\n            \\'recommendedSeasons\\' : [\\'Spring\\', \\'Summer\\']\\r\\n        }))\\r\\n\\r\\n    # Wait for user selection and refine the recommendation\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n#### `CityRecommender` orchestrator\\r\\n\\r\\n```powershell\\r\\nparam($Context)\\r\\n\\r\\n$userChoice = $Context.Input -as [int]\\r\\n\\r\\nif ($userChoice -eq 1) {\\r\\n    Set-DurableCustomStatus -CustomStatus @{ recommendedCities = @(\\'Tokyo\\', \\'Seattle\\'); \\r\\n                                             recommendedSeasons = @(\\'Spring\\', \\'Summer\\') \\r\\n                                            }  \\r\\n}\\r\\n\\r\\nif ($userChoice -eq 2) {\\r\\n    Set-DurableCustomStatus -CustomStatus @{ recommendedCities = @(\\'Seattle\\', \\'London\\'); \\r\\n                                             recommendedSeasons = @(\\'Summer\\') \\r\\n                                            }  \\r\\n}\\r\\n\\r\\nif ($userChoice -eq 3) {\\r\\n    Set-DurableCustomStatus -CustomStatus @{ recommendedCities = @(\\'Tokyo\\', \\'London\\'); \\r\\n                                             recommendedSeasons = @(\\'Spring\\', \\'Summer\\') \\r\\n                                            }  \\r\\n}\\r\\n\\r\\n# Wait for user selection and refine the recommendation\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"CityRecommender\")\\r\\npublic void cityRecommender(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    int userChoice = ctx.getInput(int.class);\\r\\n    switch (userChoice) {\\r\\n        case 1:\\r\\n            ctx.setCustomStatus(new Recommendation(\\r\\n                    new String[]{ \"Tokyo\", \"Seattle\" },\\r\\n                    new String[]{ \"Spring\", \"Summer\" }));\\r\\n            break;\\r\\n        case 2:\\r\\n            ctx.setCustomStatus(new Recommendation(\\r\\n                    new String[]{ \"Seattle\", \"London\" },\\r\\n                    new String[]{ \"Summer\" }));\\r\\n            break;\\r\\n        case 3:\\r\\n            ctx.setCustomStatus(new Recommendation(\\r\\n                    new String[]{ \"Tokyo\", \"London\" },\\r\\n                    new String[]{ \"Spring\", \"Summer\" }));\\r\\n            break;\\r\\n    }\\r\\n\\r\\n    // Wait for user selection with an external event\\r\\n}\\r\\n\\r\\nclass Recommendation {\\r\\n    public Recommendation() { }\\r\\n\\r\\n    public Recommendation(String[] cities, String[] seasons) {\\r\\n        this.recommendedCities = cities;\\r\\n        this.recommendedSeasons = seasons;\\r\\n    }\\r\\n\\r\\n    public String[] recommendedCities;\\r\\n    public String[] recommendedSeasons;\\r\\n}\\r\\n```\\r\\n\\r\\n### Instruction specification\\r\\n\\r\\nThe orchestrator can provide unique instructions to the clients via the custom state. The custom status instructions will be mapped to the steps in the orchestration code:\\r\\n\\r\\n**C#**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"ReserveTicket\")]\\r\\npublic static async Task<bool> Run(\\r\\n  [OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n  string userId = context.GetInput<string>();\\r\\n\\r\\n  int discount = await context.CallActivityAsync<int>(\"CalculateDiscount\", userId);\\r\\n\\r\\n  context.SetCustomStatus(new\\r\\n  {\\r\\n    discount = discount,\\r\\n    discountTimeout = 60,\\r\\n    bookingUrl = \"https://www.myawesomebookingweb.com\",\\r\\n  });\\r\\n\\r\\n  bool isBookingConfirmed = await context.WaitForExternalEvent<bool>(\"BookingConfirmed\");\\r\\n\\r\\n  context.SetCustomStatus(isBookingConfirmed\\r\\n    ? new {message = \"Thank you for confirming your booking.\"}\\r\\n    : new {message = \"The booking was not confirmed on time. Please try again.\"});\\r\\n\\r\\n  return isBookingConfirmed;\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context) {\\r\\n    const userId = context.df.getInput();\\r\\n\\r\\n    const discount = yield context.df.callActivity(\"CalculateDiscount\", userId);\\r\\n\\r\\n    context.df.setCustomStatus({\\r\\n        discount,\\r\\n        discountTimeout = 60,\\r\\n        bookingUrl = \"https://www.myawesomebookingweb.com\",\\r\\n    });\\r\\n\\r\\n    const isBookingConfirmed = yield context.df.waitForExternalEvent(\"bookingConfirmed\");\\r\\n\\r\\n    context.df.setCustomStatus(isBookingConfirmed\\r\\n        ? { message: \"Thank you for confirming your booking.\" }\\r\\n        : { message: \"The booking was not confirmed on time. Please try again.\" }\\r\\n    );\\r\\n\\r\\n    return isBookingConfirmed;\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    userId = int(context.get_input())\\r\\n\\r\\n    discount = yield context.call_activity(\\'CalculateDiscount\\', userId)\\r\\n\\r\\n    status = { \\'discount\\' : discount,\\r\\n        \\'discountTimeout\\' : 60,\\r\\n        \\'bookingUrl\\' : \"https://www.myawesomebookingweb.com\",\\r\\n    }\\r\\n    context.set_custom_status(status)\\r\\n\\r\\n    is_booking_confirmed = yield context.wait_for_external_event(\\'BookingConfirmed\\')\\r\\n    context.set_custom_status({\\'message\\': \\'Thank you for confirming your booking.\\'} if is_booking_confirmed \\r\\n        else {\\'message\\': \\'The booking was not confirmed on time. Please try again.\\'})\\r\\n    return is_booking_confirmed\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n```powershell\\r\\nparam($Context)\\r\\n\\r\\n$userId = $Context.Input -as [int]\\r\\n\\r\\n$discount = Invoke-DurableActivity -FunctionName \\'CalculateDiscount\\' -Input $userId\\r\\n\\r\\n$status = @{\\r\\n            discount = $discount;\\r\\n            discountTimeout = 60;\\r\\n            bookingUrl = \"https://www.myawesomebookingweb.com\"\\r\\n            }\\r\\n\\r\\nSet-DurableCustomStatus -CustomStatus $status\\r\\n\\r\\n$isBookingConfirmed = Invoke-DurableActivity -FunctionName \\'BookingConfirmed\\'\\r\\n\\r\\nif ($isBookingConfirmed) {\\r\\n    Set-DurableCustomStatus -CustomStatus @{message = \\'Thank you for confirming your booking.\\'}\\r\\n} else {\\r\\n    Set-DurableCustomStatus -CustomStatus @{message = \\'The booking was not confirmed on time. Please try again.\\'}\\r\\n}\\r\\n\\r\\nreturn $isBookingConfirmed\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"ReserveTicket\")\\r\\npublic boolean reserveTicket(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    String userID = ctx.getInput(String.class);\\r\\n    int discount = ctx.callActivity(\"CalculateDiscount\", userID, int.class).await();\\r\\n    ctx.setCustomStatus(new DiscountInfo(discount, 60, \"https://www.myawesomebookingweb.com\"));\\r\\n\\r\\n    boolean isConfirmed = ctx.waitForExternalEvent(\"BookingConfirmed\", boolean.class).await();\\r\\n    if (isConfirmed) {\\r\\n        ctx.setCustomStatus(\"Thank you for confirming your booking.\");\\r\\n    } else {\\r\\n        ctx.setCustomStatus(\"There was a problem confirming your booking. Please try again.\");\\r\\n    }\\r\\n\\r\\n    return isConfirmed;\\r\\n}\\r\\n\\r\\nclass DiscountInfo {\\r\\n    public DiscountInfo() { }\\r\\n    public DiscountInfo(int discount, int discountTimeout, String bookingUrl) {\\r\\n        this.discount = discount;\\r\\n        this.discountTimeout = discountTimeout;\\r\\n        this.bookingUrl = bookingUrl;\\r\\n    }\\r\\n    public int discount;\\r\\n    public int discountTimeout;\\r\\n    public String bookingUrl;\\r\\n}\\r\\n```\\r\\n\\r\\n## Querying custom status with HTTP\\r\\n\\r\\nThe following example shows how custom status values can be queried using the built-in HTTP APIs.\\r\\n\\r\\n**C#**\\r\\n\\r\\n```csharp\\r\\npublic static async Task SetStatusTest([OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    // ...do work...\\r\\n\\r\\n    // update the status of the orchestration with some arbitrary data\\r\\n    var customStatus = new { nextActions = new [] {\"A\", \"B\", \"C\"}, foo = 2, };\\r\\n    context.SetCustomStatus(customStatus);\\r\\n\\r\\n    // ...do more work...\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context) {\\r\\n    // ...do work...\\r\\n\\r\\n    // update the status of the orchestration with some arbitrary data\\r\\n    const customStatus = { nextActions: [ \"A\", \"B\", \"C\" ], foo: 2, };\\r\\n    context.df.setCustomStatus(customStatus);\\r\\n\\r\\n    // ...do more work...\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    # ...do work...\\r\\n\\r\\n    custom_status = {\\'nextActions\\': [\\'A\\',\\'B\\',\\'C\\'], \\'foo\\':2}\\r\\n    context.set_custom_status(custom_status)\\r\\n\\r\\n    # ...do more work...\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n```powershell\\r\\nparam($Context)\\r\\n\\r\\n# ...do work...\\r\\n\\r\\nSet-DurableCustomStatus -CustomStatus @{ nextActions = @(\\'A\\', \\'B\\', \\'C\\'); \\r\\n                                         foo = 2 \\r\\n                                        }  \\r\\n\\r\\n# ...do more work...\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"MyCustomStatusOrchestrator\")\\r\\npublic void myCustomStatusOrchestrator(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    // ... do work ...\\r\\n\\r\\n    // update the status of the orchestration with some arbitrary data\\r\\n    CustomStatusPayload payload = new CustomStatusPayload();\\r\\n    payload.nextActions = new String[] { \"A\", \"B\", \"C\" };\\r\\n    payload.foo = 2;\\r\\n    ctx.setCustomStatus(payload);\\r\\n\\r\\n    // ... do more work ...\\r\\n}\\r\\n\\r\\nclass CustomStatusPayload {\\r\\n    public String[] nextActions;\\r\\n    public int foo;\\r\\n}\\r\\n```\\r\\n\\r\\nWhile the orchestration is running, external clients can fetch this custom status:\\r\\n\\r\\n```http\\r\\nGET /runtime/webhooks/durabletask/instances/instance123\\r\\n```\\r\\n\\r\\nClients will get the following response:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"runtimeStatus\": \"Running\",\\r\\n  \"input\": null,\\r\\n  \"customStatus\": { \"nextActions\": [\"A\", \"B\", \"C\"], \"foo\": 2 },\\r\\n  \"output\": null,\\r\\n  \"createdTime\": \"2019-10-06T18:30:24Z\",\\r\\n  \"lastUpdatedTime\": \"2019-10-06T19:40:30Z\"\\r\\n}\\r\\n```\\r\\n\\r\\nWarning\\r\\n\\r\\nThe custom status payload is limited to 16 KB of UTF-16 JSON text. We recommend you use external storage if you need a larger payload.',\n",
       "  'metadata': {}},\n",
       " {'_id': '6359a97e-de26-4b85-cdfe-d59ba13717a5',\n",
       "  'title': 'Diagnostics in Durable Functions - Azure',\n",
       "  'text': '# Diagnostics in Durable Functions in Azure\\r\\n\\r\\nThere are several options for diagnosing issues with [Durable Functions](durable-functions-overview). Some of these options are the same for regular functions and some of them are unique to Durable Functions.\\r\\n\\r\\n## Application Insights\\r\\n\\r\\n[Application Insights](/en-us/azure/azure-monitor/app/app-insights-overview) is the recommended way to do diagnostics and monitoring in Azure Functions. The same applies to Durable Functions. For an overview of how to leverage Application Insights in your function app, see [Monitor Azure Functions](../functions-monitoring).\\r\\n\\r\\nThe Azure Functions Durable Extension also emits *tracking events* that allow you to trace the end-to-end execution of an orchestration. These tracking events can be found and queried using the [Application Insights Analytics](/en-us/azure/azure-monitor/logs/log-query-overview) tool in the Azure portal.\\r\\n\\r\\n### Tracking data\\r\\n\\r\\nEach lifecycle event of an orchestration instance causes a tracking event to be written to the **traces** collection in Application Insights. This event contains a **customDimensions** payload with several fields. Field names are all prepended with `prop__`.\\r\\n\\r\\n- **hubName**: The name of the task hub in which your orchestrations are running.\\r\\n- **appName**: The name of the function app. This field is useful when you have multiple function apps sharing the same Application Insights instance.\\r\\n- **slotName**: The [deployment slot](../functions-deployment-slots) in which the current function app is running. This field is useful when you use deployment slots to version your orchestrations.\\r\\n- **functionName**: The name of the orchestrator or activity function.\\r\\n- **functionType**: The type of the function, such as **Orchestrator** or **Activity**.\\r\\n- **instanceId**: The unique ID of the orchestration instance.\\r\\n- **state**: The lifecycle execution state of the instance. Valid values include:\\r\\n    - **Scheduled**: The function was scheduled for execution but hasn\\'t started running yet.\\r\\n    - **Started**: The function has started running but has not yet awaited or completed.\\r\\n    - **Awaited**: The orchestrator has scheduled some work and is waiting for it to complete.\\r\\n    - **Listening**: The orchestrator is listening for an external event notification.\\r\\n    - **Completed**: The function has completed successfully.\\r\\n    - **Failed**: The function failed with an error.\\r\\n- **reason**: Additional data associated with the tracking event. For example, if an instance is waiting for an external event notification, this field indicates the name of the event it is waiting for. If a function has failed, this field will contain the error details.\\r\\n- **isReplay**: Boolean value indicating whether the tracking event is for replayed execution.\\r\\n- **extensionVersion**: The version of the Durable Task extension. The version information is especially important data when reporting possible bugs in the extension. Long-running instances may report multiple versions if an update occurs while it is running.\\r\\n- **sequenceNumber**: Execution sequence number for an event. Combined with the timestamp helps to order the events by execution time. *Note that this number will be reset to zero if the host restarts while the instance is running, so it\\'s important to always sort by timestamp first, then sequenceNumber.*\\r\\n\\r\\nThe verbosity of tracking data emitted to Application Insights can be configured in the `logger` (Functions 1.x) or `logging` (Functions 2.0) section of the `host.json` file.\\r\\n\\r\\n#### Functions 1.0\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"logger\": {\\r\\n        \"categoryFilter\": {\\r\\n            \"categoryLevels\": {\\r\\n                \"Host.Triggers.DurableTask\": \"Information\"\\r\\n            }\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n#### Functions 2.0\\r\\n\\r\\n```json\\r\\n{\\r\\n    \"logging\": {\\r\\n        \"logLevel\": {\\r\\n            \"Host.Triggers.DurableTask\": \"Information\",\\r\\n        },\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nBy default, all *non-replay* tracking events are emitted. The volume of data can be reduced by setting `Host.Triggers.DurableTask` to `\"Warning\"` or `\"Error\"` in which case tracking events will only be emitted for exceptional situations. To enable emitting the verbose orchestration replay events, set the `logReplayEvents` to `true` in the [host.json](durable-functions-bindings#host-json) configuration file.\\r\\n\\r\\nNote\\r\\n\\r\\nBy default, Application Insights telemetry is sampled by the Azure Functions runtime to avoid emitting data too frequently. This can cause tracking information to be lost when many lifecycle events occur in a short period of time. The [Azure Functions Monitoring article](../configure-monitoring#configure-sampling) explains how to configure this behavior.\\r\\n\\r\\nInputs and outputs of orchestrator, activity, and entity functions are not logged by default. This default behavior is recommended because logging inputs and outputs could increase Application Insights costs. Function input and output payloads may also contain sensitive information. Instead, the number of bytes for function inputs and outputs are logged instead of the actual payloads by default. If you want the Durable Functions extension to log the full input and output payloads, set the `traceInputsAndOutputs` property to `true` in the [host.json](durable-functions-bindings#host-json) configuration file.\\r\\n\\r\\n### Single instance query\\r\\n\\r\\nThe following query shows historical tracking data for a single instance of the [Hello Sequence](durable-functions-sequence) function orchestration. It\\'s written using the [Kusto Query Language](/en-us/azure/data-explorer/kusto/query/). It filters out replay execution so that only the *logical* execution path is shown. Events can be ordered by sorting by `timestamp` and `sequenceNumber` as shown in the query below:\\r\\n\\r\\n```kusto\\r\\nlet targetInstanceId = \"ddd1aaa685034059b545eb004b15d4eb\";\\r\\nlet start = datetime(2018-03-25T09:20:00);\\r\\ntraces\\r\\n| where timestamp > start and timestamp < start + 30m\\r\\n| where customDimensions.Category == \"Host.Triggers.DurableTask\"\\r\\n| extend functionName = customDimensions[\"prop__functionName\"]\\r\\n| extend instanceId = customDimensions[\"prop__instanceId\"]\\r\\n| extend state = customDimensions[\"prop__state\"]\\r\\n| extend isReplay = tobool(tolower(customDimensions[\"prop__isReplay\"]))\\r\\n| extend sequenceNumber = tolong(customDimensions[\"prop__sequenceNumber\"])\\r\\n| where isReplay != true\\r\\n| where instanceId == targetInstanceId\\r\\n| sort by timestamp asc, sequenceNumber asc\\r\\n| project timestamp, functionName, state, instanceId, sequenceNumber, appName = cloud_RoleName\\r\\n```\\r\\n\\r\\nThe result is a list of tracking events that shows the execution path of the orchestration, including any activity functions ordered by the execution time in ascending order.\\r\\n\\r\\n### Instance summary query\\r\\n\\r\\nThe following query displays the status of all orchestration instances that were run in a specified time range.\\r\\n\\r\\n```kusto\\r\\nlet start = datetime(2017-09-30T04:30:00);\\r\\ntraces\\r\\n| where timestamp > start and timestamp < start + 1h\\r\\n| where customDimensions.Category == \"Host.Triggers.DurableTask\"\\r\\n| extend functionName = tostring(customDimensions[\"prop__functionName\"])\\r\\n| extend instanceId = tostring(customDimensions[\"prop__instanceId\"])\\r\\n| extend state = tostring(customDimensions[\"prop__state\"])\\r\\n| extend isReplay = tobool(tolower(customDimensions[\"prop__isReplay\"]))\\r\\n| extend output = tostring(customDimensions[\"prop__output\"])\\r\\n| where isReplay != true\\r\\n| summarize arg_max(timestamp, *) by instanceId\\r\\n| project timestamp, instanceId, functionName, state, output, appName = cloud_RoleName\\r\\n| order by timestamp asc\\r\\n```\\r\\n\\r\\nThe result is a list of instance IDs and their current runtime status.\\r\\n\\r\\n## Durable Task Framework Logging\\r\\n\\r\\nThe Durable extension logs are useful for understanding the behavior of your orchestration logic. However, these logs don\\'t always contain enough information to debug framework-level performance and reliability issues. Starting in **v2.3.0** of the Durable extension, logs emitted by the underlying Durable Task Framework (DTFx) are also available for collection.\\r\\n\\r\\nWhen looking at logs emitted by the DTFx, it\\'s important to understand that the DTFx engine is composed of two components: the core dispatch engine (`DurableTask.Core`) and one of many supported storage providers (Durable Functions uses `DurableTask.AzureStorage` by default but [other options are available](durable-functions-storage-providers)).\\r\\n\\r\\n- **DurableTask.Core**: Core orchestration execution and low-level scheduling logs and telemetry.\\r\\n- **DurableTask.AzureStorage**: Backend logs specific to the Azure Storage state provider. These logs include detailed interactions with the internal queues, blobs, and storage tables used to store and fetch internal orchestration state.\\r\\n- **DurableTask.Netherite**: Backend logs specific to the [Netherite storage provider](https://microsoft.github.io/durabletask-netherite), if enabled.\\r\\n- **DurableTask.SqlServer**: Backend logs specific to the [Microsoft SQL (MSSQL) storage provider](https://microsoft.github.io/durabletask-mssql), if enabled.\\r\\n\\r\\nYou can enable these logs by updating the `logging/logLevel` section of your function app\\'s **host.json** file. The following example shows how to enable warning and error logs from both `DurableTask.Core` and `DurableTask.AzureStorage`:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"version\": \"2.0\",\\r\\n  \"logging\": {\\r\\n    \"logLevel\": {\\r\\n      \"DurableTask.AzureStorage\": \"Warning\",\\r\\n      \"DurableTask.Core\": \"Warning\"\\r\\n    }\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\nIf you have Application Insights enabled, these logs will be automatically added to the `trace` collection. You can search them the same way that you search for other `trace` logs using Kusto queries.\\r\\n\\r\\nNote\\r\\n\\r\\nFor production applications, it is recommended that you enable `DurableTask.Core` and the appropriate storage provider (e.g. `DurableTask.AzureStorage`) logs using the `\"Warning\"` filter. Higher verbosity filters such as `\"Information\"` are very useful for debugging performance issues. However, these log events can be high-volume and can significantly increase Application Insights data storage costs.\\r\\n\\r\\nThe following Kusto query shows how to query for DTFx logs. The most important part of the query is `where customerDimensions.Category startswith \"DurableTask\"` since that filters the results to logs in the `DurableTask.Core` and `DurableTask.AzureStorage` categories.\\r\\n\\r\\n```kusto\\r\\ntraces\\r\\n| where customDimensions.Category startswith \"DurableTask\"\\r\\n| project\\r\\n    timestamp,\\r\\n    severityLevel,\\r\\n    Category = customDimensions.Category,\\r\\n    EventId = customDimensions.EventId,\\r\\n    message,\\r\\n    customDimensions\\r\\n| order by timestamp asc \\r\\n```\\r\\n\\r\\nThe result is a set of logs written by the Durable Task Framework log providers.\\r\\n\\r\\nFor more information about what log events are available, see the [Durable Task Framework structured logging documentation on GitHub](https://github.com/Azure/durabletask/tree/master/src/DurableTask.Core/Logging#durabletaskcore-logging).\\r\\n\\r\\n## App Logging\\r\\n\\r\\nIt\\'s important to keep the orchestrator replay behavior in mind when writing logs directly from an orchestrator function. For example, consider the following orchestrator function:\\r\\n\\r\\n**C# (InProc)**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"FunctionChain\")]\\r\\npublic static async Task Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context,\\r\\n    ILogger log)\\r\\n{\\r\\n    log.LogInformation(\"Calling F1.\");\\r\\n    await context.CallActivityAsync(\"F1\");\\r\\n    log.LogInformation(\"Calling F2.\");\\r\\n    await context.CallActivityAsync(\"F2\");\\r\\n    log.LogInformation(\"Calling F3\");\\r\\n    await context.CallActivityAsync(\"F3\");\\r\\n    log.LogInformation(\"Done!\");\\r\\n}\\r\\n```\\r\\n\\r\\n**C# (Isolated)**\\r\\n\\r\\n```csharp\\r\\n[Function(\"FunctionChain\")]\\r\\npublic static async Task Run(\\r\\n    [OrchestrationTrigger] TaskOrchestrationContext context,\\r\\n    FunctionContext executionContext)\\r\\n{\\r\\n    ILogger log = executionContext.GetLogger(\"FunctionChain\");\\r\\n    log.LogInformation(\"Calling F1.\");\\r\\n    await context.CallActivityAsync(\"F1\");\\r\\n    log.LogInformation(\"Calling F2.\");\\r\\n    await context.CallActivityAsync(\"F2\");\\r\\n    log.LogInformation(\"Calling F3\");\\r\\n    await context.CallActivityAsync(\"F3\");\\r\\n    log.LogInformation(\"Done!\");\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context){\\r\\n    context.log(\"Calling F1.\");\\r\\n    yield context.df.callActivity(\"F1\");\\r\\n    context.log(\"Calling F2.\");\\r\\n    yield context.df.callActivity(\"F2\");\\r\\n    context.log(\"Calling F3.\");\\r\\n    yield context.df.callActivity(\"F3\");\\r\\n    context.log(\"Done!\");\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport logging\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    logging.info(\"Calling F1.\")\\r\\n    yield context.call_activity(\"F1\")\\r\\n    logging.info(\"Calling F2.\")\\r\\n    yield context.call_activity(\"F2\")\\r\\n    logging.info(\"Calling F3.\")\\r\\n    yield context.call_activity(\"F3\")\\r\\n    return None\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"FunctionChain\")\\r\\npublic void functionChain(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx,\\r\\n        ExecutionContext functionContext) {\\r\\n    Logger log = functionContext.getLogger();\\r\\n    log.info(\"Calling F1.\");\\r\\n    ctx.callActivity(\"F1\").await();\\r\\n    log.info(\"Calling F2.\");\\r\\n    ctx.callActivity(\"F2\").await();\\r\\n    log.info(\"Calling F3.\");\\r\\n    ctx.callActivity(\"F3\").await();\\r\\n    log.info(\"Done!\");\\r\\n}\\r\\n```\\r\\n\\r\\nThe resulting log data is going to look something like the following example output:\\r\\n\\r\\n```txt\\r\\nCalling F1.\\r\\nCalling F1.\\r\\nCalling F2.\\r\\nCalling F1.\\r\\nCalling F2.\\r\\nCalling F3.\\r\\nCalling F1.\\r\\nCalling F2.\\r\\nCalling F3.\\r\\nDone!\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nRemember that while the logs claim to be calling F1, F2, and F3, those functions are only *actually* called the first time they are encountered. Subsequent calls that happen during replay are skipped and the outputs are replayed to the orchestrator logic.\\r\\n\\r\\nIf you want to only write logs on non-replay executions, you can write a conditional expression to log only if the \"is replaying\" flag is `false`. Consider the example above, but this time with replay checks.\\r\\n\\r\\n**C# (InProc)**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"FunctionChain\")]\\r\\npublic static async Task Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context,\\r\\n    ILogger log)\\r\\n{\\r\\n    if (!context.IsReplaying) log.LogInformation(\"Calling F1.\");\\r\\n    await context.CallActivityAsync(\"F1\");\\r\\n    if (!context.IsReplaying) log.LogInformation(\"Calling F2.\");\\r\\n    await context.CallActivityAsync(\"F2\");\\r\\n    if (!context.IsReplaying) log.LogInformation(\"Calling F3\");\\r\\n    await context.CallActivityAsync(\"F3\");\\r\\n    log.LogInformation(\"Done!\");\\r\\n}\\r\\n```\\r\\n\\r\\nStarting in Durable Functions 2.0, .NET orchestrator functions also have the option to create an `ILogger` that automatically filters out log statements during replay. This automatic filtering is done using the [IDurableOrchestrationContext.CreateReplaySafeLogger(ILogger)](/en-us/dotnet/api/microsoft.azure.webjobs.extensions.durabletask.durablecontextextensions.createreplaysafelogger) API.\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"FunctionChain\")]\\r\\npublic static async Task Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context,\\r\\n    ILogger log)\\r\\n{\\r\\n    log = context.CreateReplaySafeLogger(log);\\r\\n    log.LogInformation(\"Calling F1.\");\\r\\n    await context.CallActivityAsync(\"F1\");\\r\\n    log.LogInformation(\"Calling F2.\");\\r\\n    await context.CallActivityAsync(\"F2\");\\r\\n    log.LogInformation(\"Calling F3\");\\r\\n    await context.CallActivityAsync(\"F3\");\\r\\n    log.LogInformation(\"Done!\");\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous C# examples are for Durable Functions 2.x. For Durable Functions 1.x, you must use `DurableOrchestrationContext` instead of `IDurableOrchestrationContext`. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n**C# (Isolated)**\\r\\nIn Durable Functions for .NET-isolated, you can create an `ILogger` that automatically filters out log statements during replay. The main difference with Durable Functions in-proc is that you do not provide an existing `ILogger`. This logger is created via the `TaskOrchestrationContext.CreateReplaySafeLogger` overloads.\\r\\n\\r\\n```csharp\\r\\n[Function(\"FunctionChain\")]\\r\\npublic static async Task Run([OrchestrationTrigger] TaskOrchestrationContext context)\\r\\n{\\r\\n    ILogger log = context.CreateReplaySafeLogger(\"FunctionChain\");\\r\\n    log.LogInformation(\"Calling F1.\");\\r\\n    await context.CallActivityAsync(\"F1\");\\r\\n    log.LogInformation(\"Calling F2.\");\\r\\n    await context.CallActivityAsync(\"F2\");\\r\\n    log.LogInformation(\"Calling F3\");\\r\\n    await context.CallActivityAsync(\"F3\");\\r\\n    log.LogInformation(\"Done!\");\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe ability to wrap an existing `ILogger` into a replay-safe logger has been removed in Durable Functions for .NET isolated worker.\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context){\\r\\n    if (!context.df.isReplaying) context.log(\"Calling F1.\");\\r\\n    yield context.df.callActivity(\"F1\");\\r\\n    if (!context.df.isReplaying) context.log(\"Calling F2.\");\\r\\n    yield context.df.callActivity(\"F2\");\\r\\n    if (!context.df.isReplaying) context.log(\"Calling F3.\");\\r\\n    yield context.df.callActivity(\"F3\");\\r\\n    context.log(\"Done!\");\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport logging\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    if not context.is_replaying:\\r\\n        logging.info(\"Calling F1.\")\\r\\n    yield context.call_activity(\"F1\")\\r\\n    if not context.is_replaying:\\r\\n        logging.info(\"Calling F2.\")\\r\\n    yield context.call_activity(\"F2\")\\r\\n    if not context.is_replaying:\\r\\n        logging.info(\"Calling F3.\")\\r\\n    yield context.call_activity(\"F3\")\\r\\n    logging.info(\"Done!\")\\r\\n    return None\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"FunctionChain\")\\r\\npublic void functionChain(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx,\\r\\n        ExecutionContext functionContext) {\\r\\n    Logger log = functionContext.getLogger();\\r\\n    if (!ctx.getIsReplaying()) log.info(\"Calling F1.\");\\r\\n    ctx.callActivity(\"F1\").await();\\r\\n    if (!ctx.getIsReplaying()) log.info(\"Calling F2.\");\\r\\n    ctx.callActivity(\"F2\").await();\\r\\n    if (!ctx.getIsReplaying()) log.info(\"Calling F3.\");\\r\\n    ctx.callActivity(\"F3\").await();\\r\\n    log.info(\"Done!\");\\r\\n}\\r\\n```\\r\\n\\r\\nWith the previously mentioned changes, the log output is as follows:\\r\\n\\r\\n```txt\\r\\nCalling F1.\\r\\nCalling F2.\\r\\nCalling F3.\\r\\nDone!\\r\\n```\\r\\n\\r\\n## Custom Status\\r\\n\\r\\nCustom orchestration status lets you set a custom status value for your orchestrator function. This custom status is then visible to external clients via the [HTTP status query API](durable-functions-http-api#get-instance-status) or via language-specific API calls. The custom orchestration status enables richer monitoring for orchestrator functions. For example, the orchestrator function code can invoke the \"set custom status\" API to update the progress for a long-running operation. A client, such as a web page or other external system, could then periodically query the HTTP status query APIs for richer progress information. Sample code for setting a custom status value in an orchestrator function is provided below:\\r\\n\\r\\n**C# (InProc)**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"SetStatusTest\")]\\r\\npublic static async Task SetStatusTest([OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    // ...do work...\\r\\n\\r\\n    // update the status of the orchestration with some arbitrary data\\r\\n    var customStatus = new { completionPercentage = 90.0, status = \"Updating database records\" };\\r\\n    context.SetCustomStatus(customStatus);\\r\\n\\r\\n    // ...do more work...\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous C# example is for Durable Functions 2.x. For Durable Functions 1.x, you must use `DurableOrchestrationContext` instead of `IDurableOrchestrationContext`. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n**C# (Isolated)**\\r\\n\\r\\n```csharp\\r\\n[Function(\"SetStatusTest\")]\\r\\npublic static async Task SetStatusTest([OrchestrationTrigger] TaskOrchestrationContext context)\\r\\n{\\r\\n    // ...do work...\\r\\n\\r\\n    // update the status of the orchestration with some arbitrary data\\r\\n    var customStatus = new { completionPercentage = 90.0, status = \"Updating database records\" };\\r\\n    context.SetCustomStatus(customStatus);\\r\\n\\r\\n    // ...do more work...\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context) {\\r\\n    // ...do work...\\r\\n\\r\\n    // update the status of the orchestration with some arbitrary data\\r\\n    const customStatus = { completionPercentage: 90.0, status: \"Updating database records\", };\\r\\n    context.df.setCustomStatus(customStatus);\\r\\n\\r\\n    // ...do more work...\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport logging\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    # ...do work...\\r\\n\\r\\n    # update the status of the orchestration with some arbitrary data\\r\\n    custom_status = {\\'completionPercentage\\': 90.0, \\'status\\': \\'Updating database records\\'}\\r\\n    context.set_custom_status(custom_status)\\r\\n    # ...do more work...\\r\\n\\r\\n    return None\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"SetStatusTest\")\\r\\npublic void setStatusTest(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    // ...do work...\\r\\n\\r\\n    // update the status of the orchestration with some arbitrary data\\r\\n    ctx.setCustomStatus(new Object() {\\r\\n        public final double completionPercentage = 90.0;\\r\\n        public final String status = \"Updating database records\";\\r\\n    });\\r\\n\\r\\n    // ...do more work...\\r\\n}\\r\\n```\\r\\n\\r\\nWhile the orchestration is running, external clients can fetch this custom status:\\r\\n\\r\\n```http\\r\\nGET /runtime/webhooks/durabletask/instances/instance123?code=XYZ\\r\\n\\r\\n```\\r\\n\\r\\nClients will get the following response:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"runtimeStatus\": \"Running\",\\r\\n  \"input\": null,\\r\\n  \"customStatus\": { \"completionPercentage\": 90.0, \"status\": \"Updating database records\" },\\r\\n  \"output\": null,\\r\\n  \"createdTime\": \"2017-10-06T18:30:24Z\",\\r\\n  \"lastUpdatedTime\": \"2017-10-06T19:40:30Z\"\\r\\n}\\r\\n```\\r\\n\\r\\nWarning\\r\\n\\r\\nThe custom status payload is limited to 16 KB of UTF-16 JSON text because it needs to be able to fit in an Azure Table Storage column. You can use external storage if you need larger payload.\\r\\n\\r\\n## Distributed Tracing\\r\\n\\r\\nDistributed Tracing tracks requests and shows how different services interact with each other. In Durable Functions, it also correlates orchestrations and activities together. This is helpful to understand how much time steps of the orchestration take relative to the entire orchestration. It is also useful to understand where an application is having an issue or where an exception was thrown. This feature is supported for all languages and storage providers.\\r\\n\\r\\nNote\\r\\n\\r\\nDistributed Tracing V2 requires [Durable Functions v2.12.0](https://www.nuget.org/packages/Microsoft.Azure.WebJobs.Extensions.DurableTask/2.12.0) or greater. Also, Distributed Tracing V2 is in a preview state and therefore some Durable Functions patterns are not instrumented. For example, Durable Entities operations are not instrumented and traces will not show up in Application Insights.\\r\\n\\r\\n### Setting up Distributed Tracing\\r\\n\\r\\nTo set up distributed tracing, please update the host.json and set up an Application Insights resource.\\r\\n\\r\\n#### host.json\\r\\n\\r\\n```\\r\\n\"durableTask\": {\\r\\n  \"tracing\": {\\r\\n    \"distributedTracingEnabled\": true,\\r\\n    \"Version\": \"V2\"\\r\\n  }\\r\\n}\\r\\n```\\r\\n\\r\\n#### Application Insights\\r\\n\\r\\nIf the Function app is not configured with an Application Insights resource, then please configure it following the instructions [here](../configure-monitoring#enable-application-insights-integration).\\r\\n\\r\\n### Inspecting the traces\\r\\n\\r\\nIn the Application Insights resource, navigate to **Transaction Search**. In the results, check for `Request` and `Dependency` events that start with Durable Functions specific prefixes (e.g. `orchestration:`, `activity:`, etc.). Selecting one of these events will open up a Gantt chart that will show the end to end distributed trace.\\r\\n\\r\\n### Troubleshooting\\r\\n\\r\\nIf you don\\'t see the traces in Application Insights, please make sure to wait about five minutes after running the application to ensure that all of the data is propagated to the Application Insights resource.\\r\\n\\r\\n## Debugging\\r\\n\\r\\nAzure Functions supports debugging function code directly, and that same support carries forward to Durable Functions, whether running in Azure or locally. However, there are a few behaviors to be aware of when debugging:\\r\\n\\r\\n- **Replay**: Orchestrator functions regularly [replay](durable-functions-orchestrations#reliability) when new inputs are received. This behavior means a single *logical* execution of an orchestrator function can result in hitting the same breakpoint multiple times, especially if it is set early in the function code.\\r\\n- **Await**: Whenever an `await` is encountered in an orchestrator function, it yields control back to the Durable Task Framework dispatcher. If it is the first time a particular `await` has been encountered, the associated task is *never* resumed. Because the task never resumes, stepping *over* the await (F10 in Visual Studio) is not possible. Stepping over only works when a task is being replayed.\\r\\n- **Messaging timeouts**: Durable Functions internally uses queue messages to drive execution of orchestrator, activity, and entity functions. In a multi-VM environment, breaking into the debugging for extended periods of time could cause another VM to pick up the message, resulting in duplicate execution. This behavior exists for regular queue-trigger functions as well, but is important to point out in this context since the queues are an implementation detail.\\r\\n- **Stopping and starting**: Messages in Durable functions persist between debug sessions. If you stop debugging and terminate the local host process while a durable function is executing, that function may re-execute automatically in a future debug session. This behavior can be confusing when not expected. Using a [fresh task hub](durable-functions-task-hubs#task-hub-management) or clearing the task hub contents between debug sessions is one technique to avoid this behavior.\\r\\n\\r\\nTip\\r\\n\\r\\nWhen setting breakpoints in orchestrator functions, if you want to only break on non-replay execution, you can set a conditional breakpoint that breaks only if the \"is replaying\" value is `false`.\\r\\n\\r\\n## Storage\\r\\n\\r\\nBy default, Durable Functions stores state in Azure Storage. This behavior means you can inspect the state of your orchestrations using tools such as [Microsoft Azure Storage Explorer](../../vs-azure-tools-storage-manage-with-storage-explorer).\\r\\n\\r\\nThis is useful for debugging because you see exactly what state an orchestration may be in. Messages in the queues can also be examined to learn what work is pending (or stuck in some cases).\\r\\n\\r\\nWarning\\r\\n\\r\\nWhile it\\'s convenient to see execution history in table storage, avoid taking any dependency on this table. It may change as the Durable Functions extension evolves.\\r\\n\\r\\nNote\\r\\n\\r\\nOther storage providers can be configured instead of the default Azure Storage provider. Depending on the storage provider configured for your app, you may need to use different tools to inspect the underlying state. For more information, see the [Durable Functions Storage Providers](durable-functions-storage-providers) documentation.\\r\\n\\r\\n## Durable Functions Monitor\\r\\n\\r\\n[Durable Functions Monitor](https://github.com/microsoft/DurableFunctionsMonitor) is a graphical tool for monitoring, managing, and debugging orchestration and entity instances. It is available as a Visual Studio Code extension or a standalone app. Information about set up and a list of features can be found in [this Wiki](https://github.com/microsoft/DurableFunctionsMonitor/wiki).\\r\\n\\r\\n## Durable Functions troubleshooting guide\\r\\n\\r\\nTo troubleshoot common problem symptoms such as orchestrations being stuck, failing to start, running slowly, etc., refer to this [troubleshooting guide](durable-functions-troubleshooting-guide).',\n",
       "  'metadata': {}},\n",
       " {'_id': 'f833fcb1-b9ac-fc4f-73af-621c48236611',\n",
       "  'title': 'Disaster recovery and geo-distribution Azure Durable Functions',\n",
       "  'text': \"# Disaster recovery and geo-distribution in Azure Durable Functions\\r\\n\\r\\nMicrosoft strives to ensure that Azure services are always available. However, unplanned service outages may occur. If your application requires resiliency, Microsoft recommends configuring your app for geo-redundancy. Additionally, customers should have a disaster recovery plan in place for handling a regional service outage. An important part of a disaster recovery plan is preparing to fail over to the secondary replica of your app and storage in the event that the primary replica becomes unavailable.\\r\\n\\r\\nIn Durable Functions, all state is persisted in Azure Storage by default. A [task hub](durable-functions-task-hubs) is a logical container for Azure Storage resources that are used for [orchestrations](durable-functions-types-features-overview#orchestrator-functions) and [entities](durable-functions-types-features-overview#entity-functions). Orchestrator, activity, and entity functions can only interact with each other when they belong to the same task hub. This document will refer to task hubs when describing scenarios for keeping these Azure Storage resources highly available.\\r\\n\\r\\nNote\\r\\n\\r\\nThe guidance in this article assumes that you are using the default Azure Storage provider for storing Durable Functions runtime state. However, it's possible to configure alternate storage providers that store state elsewhere, like a SQL Server database. Different disaster recovery and geo-distribution strategies may be required for the alternate storage providers. For more information on the alternate storage providers, see the [Durable Functions storage providers](durable-functions-storage-providers) documentation.\\r\\n\\r\\nOrchestrations and entities can be triggered using [client functions](durable-functions-types-features-overview#client-functions) that are themselves triggered via HTTP or one of the other supported Azure Functions trigger types. They can also be triggered using [built-in HTTP APIs](durable-functions-http-features#built-in-http-apis). For simplicity, this article will focus on scenarios involving Azure Storage and HTTP-based function triggers, and options to increase availability and minimize downtime during disaster recovery activities. Other trigger types, such as Service Bus or Azure Cosmos DB triggers, will not be explicitly covered.\\r\\n\\r\\nThe following scenarios are based on Active-Passive configurations, since they are guided by the usage of Azure Storage. This pattern consists of deploying a backup (passive) function app to a different region. Traffic Manager will monitor the primary (active) function app for HTTP availability. It will fail over to the backup function app if the primary fails. For more information, see [Azure Traffic Manager](https://azure.microsoft.com/services/traffic-manager/)'s [Priority Traffic-Routing Method.](../../traffic-manager/traffic-manager-routing-methods#priority-traffic-routing-method)\\r\\n\\r\\nNote\\r\\n\\r\\n- The proposed Active-Passive configuration ensures that a client is always able to trigger new orchestrations via HTTP. However, as a consequence of having two function apps sharing the same task hub in storage, some background storage transactions will be distributed between both of them. This configuration therefore incurs some added egress costs for the secondary function app.\\r\\n- The underlying storage account and task hub are created in the primary region, and are shared by both function apps.\\r\\n- All function apps that are redundantly deployed must share the same function access keys in the case of being activated via HTTP. The Functions Runtime exposes a [management API](https://github.com/Azure/azure-functions-host/wiki/Key-management-API) that enables consumers to programmatically add, delete, and update function keys. Key management is also possible using [Azure Resource Manager APIs](https://www.markheath.net/post/managing-azure-functions-keys-2).\\r\\n\\r\\n## Scenario 1 - Load balanced compute with shared storage\\r\\n\\r\\nIf the compute infrastructure in Azure fails, the function app may become unavailable. To minimize the possibility of such downtime, this scenario uses two function apps deployed to different regions. Traffic Manager is configured to detect problems in the primary function app and automatically redirect traffic to the function app in the secondary region. This function app shares the same Azure Storage account and Task Hub. Therefore, the state of the function apps isn't lost and work can resume normally. Once health is restored to the primary region, Azure Traffic Manager will start routing requests to that function app automatically.\\r\\n\\r\\nThere are several benefits when using this deployment scenario:\\r\\n\\r\\n- If the compute infrastructure fails, work can resume in the failover region without data loss.\\r\\n- Traffic Manager takes care of the automatic failover to the healthy function app automatically.\\r\\n- Traffic Manager automatically re-establishes traffic to the primary function app after the outage has been corrected.\\r\\n\\r\\nHowever, using this scenario consider:\\r\\n\\r\\n- If the function app is deployed using a dedicated App Service plan, replicating the compute infrastructure in the failover datacenter increases costs.\\r\\n- This scenario covers outages at the compute infrastructure, but the storage account continues to be the single point of failure for the function App. If a Storage outage occurs, the application suffers downtime.\\r\\n- If the function app is failed over, there will be increased latency since it will access its storage account across regions.\\r\\n- Accessing the storage service from a different region where it's located incurs in higher cost due to network egress traffic.\\r\\n- This scenario depends on Traffic Manager. Considering [how Traffic Manager works](../../traffic-manager/traffic-manager-how-it-works), it may be some time until a client application that consumes a Durable Function needs to query again the function app address from Traffic Manager.\\r\\n\\r\\nNote\\r\\n\\r\\nStarting in **v2.3.0** of the Durable Functions extension, two function apps can be run safely at the same time with the same storage account and task hub configuration. The first app to start will acquire an application-level blob lease that prevents other apps from stealing messages from the task hub queues. If this first app stops running, its lease will expire and can be acquired by a second app, which will then proceed to process task hub messages.\\r\\n\\r\\nPrior to v2.3.0, function apps that are configured to use the same storage account will process messages and update storage artifacts concurrently, resulting in much higher overall latencies and egress costs. If the primary and replica apps ever have different code deployed to them, even temporarily, then orchestrations could also fail to execute correctly because of orchestrator function inconsistencies across the two apps. It is therefore recommended that all apps that require geo-distribution for disaster recovery purposes use v2.3.0 or higher of the Durable extension.\\r\\n\\r\\n## Scenario 2 - Load balanced compute with regional storage\\r\\n\\r\\nThe preceding scenario covers only the case of failure in the compute infrastructure. If the storage service fails, it will result in an outage of the function app. To ensure continuous operation of the durable functions, this scenario uses a local storage account on each region to which the function apps are deployed.\\r\\n\\r\\nThis approach adds improvements on the previous scenario:\\r\\n\\r\\n- If the function app fails, Traffic Manager takes care of failing over to the secondary region. However, because the function app relies on its own storage account, the durable functions continue to work.\\r\\n- During a failover, there is no additional latency in the failover region since the function app and the storage account are colocated.\\r\\n- Failure of the storage layer will cause failures in the durable functions, which in turn will trigger a redirection to the failover region. Again, since the function app and storage are isolated per region, the durable functions will continue to work.\\r\\n\\r\\nImportant considerations for this scenario:\\r\\n\\r\\n- If the function app is deployed using a dedicated App Service plan, replicating the compute infrastructure in the failover datacenter increases costs.\\r\\n- Current state isn't failed over, which implies that existing orchestrations and entities will be effectively paused and unavailable until the primary region recovers.\\r\\n\\r\\nTo summarize, the tradeoff between the first and second scenario is that latency is preserved and egress costs are minimized but existing orchestrations and entities will be unavailable during the downtime. Whether these tradeoffs are acceptable depends on the requirements of the application.\\r\\n\\r\\n## Scenario 3 - Load balanced compute with GRS shared storage\\r\\n\\r\\nThis scenario is a modification over the first scenario, implementing a shared storage account. The main difference is that the storage account is created with geo-replication enabled. Functionally, this scenario provides the same advantages as Scenario 1, but it enables additional data recovery advantages:\\r\\n\\r\\n- Geo-redundant storage (GRS) and Read-access GRS (RA-GRS) maximize availability for your storage account.\\r\\n- If there is a regional outage of the Storage service, you can [manually initiate a failover to the secondary replica](../../storage/common/storage-initiate-account-failover). In extreme circumstances where a region is lost due to a significant disaster, Microsoft may initiate a regional failover. In this case, no action on your part is required.\\r\\n- When a failover happens, state of the durable functions will be preserved up to the last replication of the storage account, which typically occurs every few minutes.\\r\\n\\r\\nAs with the other scenarios, there are important considerations:\\r\\n\\r\\n- A failover to the replica may take some time. Until the failover completes and Azure Storage DNS records have been updated, the function app will suffer an outage.\\r\\n- There is an increased cost for using geo-replicated storage accounts.\\r\\n- GRS replication copies your data asynchronously. Some of the latest transactions might be lost because of the latency of the replication process.\\r\\n\\r\\nNote\\r\\n\\r\\nAs described in Scenario 1, it is strongly recommended that function apps deployed with this strategy use **v2.3.0** or higher of the Durable Functions extension.\\r\\n\\r\\nFor more information, see the [Azure Storage disaster recovery and storage account failover](../../storage/common/storage-disaster-recovery-guidance) documentation.\",\n",
       "  'metadata': {}},\n",
       " {'_id': 'ac7a376c-acee-e440-b477-180b2f0c947e',\n",
       "  'title': \"Developer's Guide to Durable Entities in .NET - Azure Functions\",\n",
       "  'text': '# Developer\\'s guide to durable entities in .NET (in-proc)\\r\\n\\r\\nIn this article, we describe the available interfaces for developing durable entities with .NET in detail, including examples and general advice.\\r\\n\\r\\nEntity functions provide serverless application developers with a convenient way to organize application state as a collection of fine-grained entities. For more detail about the underlying concepts, see the [Durable Entities: Concepts](durable-functions-entities) article.\\r\\n\\r\\nWe currently offer two APIs for defining entities:\\r\\n\\r\\n- The **class-based syntax** represents entities and operations as classes and methods. This syntax produces easily readable code and allows operations to be invoked in a type-checked manner through interfaces.\\r\\n- The **function-based syntax** is a lower-level interface that represents entities as functions. It provides precise control over how the entity operations are dispatched, and how the entity state is managed.\\r\\n\\r\\nThis article focuses primarily on the class-based syntax, as we expect it to be better suited for most applications. However, the function-based syntax can be appropriate for applications that wish to define or manage their own abstractions for entity state and operations. Also, it can be appropriate for implementing libraries that require genericity not currently supported by the class-based syntax.\\r\\n\\r\\nNote\\r\\n\\r\\nThe class-based syntax is just a layer on top of the function-based syntax, so both variants can be used interchangeably in the same application.\\r\\n\\r\\n## Defining entity classes\\r\\n\\r\\nThe following example is an implementation of a `Counter` entity that stores a single value of type integer, and offers four operations `Add`, `Reset`, `Get`, and `Delete`.\\r\\n\\r\\n```csharp\\r\\n[JsonObject(MemberSerialization.OptIn)]\\r\\npublic class Counter\\r\\n{\\r\\n    [JsonProperty(\"value\")]\\r\\n    public int Value { get; set; }\\r\\n\\r\\n    public void Add(int amount) \\r\\n    {\\r\\n        this.Value += amount;\\r\\n    }\\r\\n\\r\\n    public Task Reset() \\r\\n    {\\r\\n        this.Value = 0;\\r\\n        return Task.CompletedTask;\\r\\n    }\\r\\n\\r\\n    public Task<int> Get() \\r\\n    {\\r\\n        return Task.FromResult(this.Value);\\r\\n    }\\r\\n\\r\\n    public void Delete() \\r\\n    {\\r\\n        Entity.Current.DeleteState();\\r\\n    }\\r\\n\\r\\n    [FunctionName(nameof(Counter))]\\r\\n    public static Task Run([EntityTrigger] IDurableEntityContext ctx)\\r\\n        => ctx.DispatchAsync<Counter>();\\r\\n}\\r\\n```\\r\\n\\r\\nThe `Run` function contains the boilerplate required for using the class-based syntax. It must be a *static* Azure Function. It executes once for each operation message that is processed by the entity. When `DispatchAsync<T>` is called and the entity isn\\'t already in memory, it constructs an object of type `T` and populates its fields from the last persisted JSON found in storage (if any). Then it invokes the method with the matching name.\\r\\n\\r\\nThe `EntityTrigger` Function, `Run` in this sample, doesn\\'t need to reside within the Entity class itself. It can reside within any valid location for an Azure Function: inside the top-level namespace, or inside a top-level class. However, if nested deeper (e.g, the Function is declared inside a *nested* class), then this Function won\\'t be recognized by the latest runtime.\\r\\n\\r\\nNote\\r\\n\\r\\nThe state of a class-based entity is **created implicitly** before the entity processes an operation, and can be **deleted explicitly** in an operation by calling `Entity.Current.DeleteState()`.\\r\\n\\r\\n### Class Requirements\\r\\n\\r\\nEntity classes are POCOs (plain old CLR objects) that require no special superclasses, interfaces, or attributes. However:\\r\\n\\r\\n- The class must be constructible (see Entity construction).\\r\\n- The class must be JSON-serializable (see Entity serialization).\\r\\n\\r\\nAlso, any method that is intended to be invoked as an operation must satisfy other requirements:\\r\\n\\r\\n- An operation must have at most one argument, and must not have any overloads or generic type arguments.\\r\\n- An operation meant to be called from an orchestration using an interface must return `Task` or `Task<T>`.\\r\\n- Arguments and return values must be serializable values or objects.\\r\\n\\r\\n### What can operations do?\\r\\n\\r\\nAll entity operations can read and update the entity state, and changes to the state are automatically persisted to storage. Moreover, operations can perform external I/O or other computations, within the general limits common to all Azure Functions.\\r\\n\\r\\nOperations also have access to functionality provided by the `Entity.Current` context:\\r\\n\\r\\n- `EntityName`: the name of the currently executing entity.\\r\\n- `EntityKey`: the key of the currently executing entity.\\r\\n- `EntityId`: the ID of the currently executing entity (includes name and key).\\r\\n- `SignalEntity`: sends a one-way message to an entity.\\r\\n- `CreateNewOrchestration`: starts a new orchestration.\\r\\n- `DeleteState`: deletes the state of this entity.\\r\\n\\r\\nFor example, we can modify the counter entity so it starts an orchestration when the counter reaches 100 and passes the entity ID as an input argument:\\r\\n\\r\\n```csharp\\r\\npublic void Add(int amount) \\r\\n{\\r\\n    if (this.Value < 100 && this.Value + amount >= 100)\\r\\n    {\\r\\n        Entity.Current.StartNewOrchestration(\"MilestoneReached\", Entity.Current.EntityId);\\r\\n    }\\r\\n    this.Value += amount;      \\r\\n}\\r\\n```\\r\\n\\r\\n## Accessing entities directly\\r\\n\\r\\nClass-based entities can be accessed directly, using explicit string names for the entity and its operations. This section provides examples. For a deeper explanation of the underlying concepts (such as signals vs. calls), see the discussion in [Access entities](durable-functions-entities#access-entities).\\r\\n\\r\\nNote\\r\\n\\r\\nWhere possible, you should accesses entities through interfaces, because it provides more type checking.\\r\\n\\r\\n### Example: client signals entity\\r\\n\\r\\nThe following Azure Http Function implements a DELETE operation using REST conventions. It sends a delete signal to the counter entity whose key is passed in the URL path.\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"DeleteCounter\")]\\r\\npublic static async Task<HttpResponseMessage> DeleteCounter(\\r\\n    [HttpTrigger(AuthorizationLevel.Function, \"delete\", Route = \"Counter/{entityKey}\")] HttpRequestMessage req,\\r\\n    [DurableClient] IDurableEntityClient client,\\r\\n    string entityKey)\\r\\n{\\r\\n    var entityId = new EntityId(\"Counter\", entityKey);\\r\\n    await client.SignalEntityAsync(entityId, \"Delete\");    \\r\\n    return req.CreateResponse(HttpStatusCode.Accepted);\\r\\n}\\r\\n```\\r\\n\\r\\n### Example: client reads entity state\\r\\n\\r\\nThe following Azure Http Function implements a GET operation using REST conventions. It reads the current state of the counter entity whose key is passed in the URL path.\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"GetCounter\")]\\r\\npublic static async Task<HttpResponseMessage> GetCounter(\\r\\n    [HttpTrigger(AuthorizationLevel.Function, \"get\", Route = \"Counter/{entityKey}\")] HttpRequestMessage req,\\r\\n    [DurableClient] IDurableEntityClient client,\\r\\n    string entityKey)\\r\\n{\\r\\n    var entityId = new EntityId(\"Counter\", entityKey);\\r\\n    var state = await client.ReadEntityStateAsync<Counter>(entityId); \\r\\n    return req.CreateResponse(state);\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe object returned by `ReadEntityStateAsync` is just a local copy, that is, a snapshot of the entity state from some earlier point in time. In particular, it can be stale, and modifying this object has no effect on the actual entity.\\r\\n\\r\\n### Example: orchestration first signals then calls entity\\r\\n\\r\\nThe following orchestration signals a counter entity to increment it, and then calls the same entity to read its latest value.\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"IncrementThenGet\")]\\r\\npublic static async Task<int> Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    var entityId = new EntityId(\"Counter\", \"myCounter\");\\r\\n\\r\\n    // One-way signal to the entity - does not await a response\\r\\n    context.SignalEntity(entityId, \"Add\", 1);\\r\\n\\r\\n    // Two-way call to the entity which returns a value - awaits the response\\r\\n    int currentValue = await context.CallEntityAsync<int>(entityId, \"Get\");\\r\\n\\r\\n    return currentValue;\\r\\n}\\r\\n```\\r\\n\\r\\n## Accessing entities through interfaces\\r\\n\\r\\nInterfaces can be used for accessing entities via generated proxy objects. This approach ensures that the name and argument type of an operation matches what is implemented. We recommend using interfaces for accessing entities whenever possible.\\r\\n\\r\\nFor example, we can modify the counter example as follows:\\r\\n\\r\\n```csharp\\r\\npublic interface ICounter\\r\\n{\\r\\n    void Add(int amount);\\r\\n    Task Reset();\\r\\n    Task<int> Get();\\r\\n    void Delete();\\r\\n}\\r\\n\\r\\npublic class Counter : ICounter\\r\\n{\\r\\n    ...\\r\\n}\\r\\n```\\r\\n\\r\\nEntity classes and entity interfaces are similar to the grains and grain interfaces popularized by [Orleans](https://www.microsoft.com/research/project/orleans-virtual-actors/). For a more information about similarities and differences between Durable Entities and Orleans, see [Comparison with virtual actors](durable-functions-entities#comparison-with-virtual-actors).\\r\\n\\r\\nBesides providing type checking, interfaces are useful for a better separation of concerns within the application. For example, since an entity can implement multiple interfaces, a single entity can serve multiple roles. Also, since an interface can be implemented by multiple entities, general communication patterns can be implemented as reusable libraries.\\r\\n\\r\\n### Example: client signals entity through interface\\r\\n\\r\\nClient code can use `SignalEntityAsync<TEntityInterface>` to send signals to entities that implement `TEntityInterface`. For example:\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"DeleteCounter\")]\\r\\npublic static async Task<HttpResponseMessage> DeleteCounter(\\r\\n    [HttpTrigger(AuthorizationLevel.Function, \"delete\", Route = \"Counter/{entityKey}\")] HttpRequestMessage req,\\r\\n    [DurableClient] IDurableEntityClient client,\\r\\n    string entityKey)\\r\\n{\\r\\n    var entityId = new EntityId(\"Counter\", entityKey);\\r\\n    await client.SignalEntityAsync<ICounter>(entityId, proxy => proxy.Delete());    \\r\\n    return req.CreateResponse(HttpStatusCode.Accepted);\\r\\n}\\r\\n```\\r\\n\\r\\nIn this example, the `proxy` parameter is a dynamically generated instance of `ICounter`, which internally translates the call to `Delete` into a signal.\\r\\n\\r\\nNote\\r\\n\\r\\nThe `SignalEntityAsync` APIs can be used only for one-way operations. Even if an operation returns `Task<T>`, the value of the `T` parameter will always be null or `default`, not the actual result. For example, it doesn\\'t make sense to signal the `Get` operation, as no value is returned. Instead, clients can use either `ReadStateAsync` to access the counter state directly, or can start an orchestrator function that calls the `Get` operation.\\r\\n\\r\\n### Example: orchestration first signals then calls entity through proxy\\r\\n\\r\\nTo call or signal an entity from within an orchestration, `CreateEntityProxy` can be used, along with the interface type, to generate a proxy for the entity. This proxy can then be used to call or signal operations:\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"IncrementThenGet\")]\\r\\npublic static async Task<int> Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    var entityId = new EntityId(\"Counter\", \"myCounter\");\\r\\n    var proxy = context.CreateEntityProxy<ICounter>(entityId);\\r\\n\\r\\n    // One-way signal to the entity - does not await a response\\r\\n    proxy.Add(1);\\r\\n\\r\\n    // Two-way call to the entity which returns a value - awaits the response\\r\\n    int currentValue = await proxy.Get();\\r\\n\\r\\n    return currentValue;\\r\\n}\\r\\n```\\r\\n\\r\\nImplicitly, any operations that return `void` are signaled, and any operations that return `Task` or `Task<T>` are called. One can change this default behavior, and signal operations even if they return Task, by using the `SignalEntity<IInterfaceType>` method explicitly.\\r\\n\\r\\n### Shorter option for specifying the target\\r\\n\\r\\nWhen calling or signaling an entity using an interface, the first argument must specify the target entity. The target can be specified either by specifying the entity ID, or, in cases where there\\'s just one class that implements the entity, just the entity key:\\r\\n\\r\\n```csharp\\r\\ncontext.SignalEntity<ICounter>(new EntityId(nameof(Counter), \"myCounter\"), ...);\\r\\ncontext.SignalEntity<ICounter>(\"myCounter\", ...);\\r\\n```\\r\\n\\r\\nIf only the entity key is specified and a unique implementation can\\'t be found at runtime, `InvalidOperationException` is thrown.\\r\\n\\r\\n### Restrictions on entity interfaces\\r\\n\\r\\nAs usual, all parameter and return types must be JSON-serializable. Otherwise, serialization exceptions are thrown at runtime.\\r\\n\\r\\nWe also enforce some more rules:\\r\\n\\r\\n- Entity interfaces must be defined in the same assembly as the entity class.\\r\\n- Entity interfaces must only define methods.\\r\\n- Entity interfaces must not contain generic parameters.\\r\\n- Entity interface methods must not have more than one parameter.\\r\\n- Entity interface methods must return `void`, `Task`, or `Task<T>`.\\r\\n\\r\\nIf any of these rules are violated, an `InvalidOperationException` is thrown at runtime when the interface is used as a type argument to `SignalEntity`, `SignalEntityAsync`, or `CreateEntityProxy`. The exception message explains which rule was broken.\\r\\n\\r\\nNote\\r\\n\\r\\nInterface methods returning `void` can only be signaled (one-way), not called (two-way). Interface methods returning `Task` or `Task<T>` can be either called or signalled. If called, they return the result of the operation, or re-throw exceptions thrown by the operation. However, when signalled, they do not return the actual result or exception from the operation, but just the default value.\\r\\n\\r\\n## Entity serialization\\r\\n\\r\\nSince the state of an entity is durably persisted, the entity class must be serializable. The Durable Functions runtime uses the [Json.NET](https://www.newtonsoft.com/json) library for this purpose, which supports policies and attributes to control the serialization and deserialization process. Most commonly used C# data types (including arrays and collection types) are already serializable, and can easily be used for defining the state of durable entities.\\r\\n\\r\\nFor example, Json.NET can easily serialize and deserialize the following class:\\r\\n\\r\\n```csharp\\r\\n[JsonObject(MemberSerialization = MemberSerialization.OptIn)]\\r\\npublic class User\\r\\n{\\r\\n    [JsonProperty(\"name\")]\\r\\n    public string Name { get; set; }\\r\\n\\r\\n    [JsonProperty(\"yearOfBirth\")]\\r\\n    public int YearOfBirth { get; set; }\\r\\n\\r\\n    [JsonProperty(\"timestamp\")]\\r\\n    public DateTime Timestamp { get; set; }\\r\\n\\r\\n    [JsonProperty(\"contacts\")]\\r\\n    public Dictionary<Guid, Contact> Contacts { get; set; } = new Dictionary<Guid, Contact>();\\r\\n\\r\\n    [JsonObject(MemberSerialization = MemberSerialization.OptOut)]\\r\\n    public struct Contact\\r\\n    {\\r\\n        public string Name;\\r\\n        public string Number;\\r\\n    }\\r\\n\\r\\n    ...\\r\\n}\\r\\n```\\r\\n\\r\\n### Serialization Attributes\\r\\n\\r\\nIn the example above, we chose to include several attributes to make the underlying serialization more visible:\\r\\n\\r\\n- We annotate the class with `[JsonObject(MemberSerialization.OptIn)]` to remind us that the class must be serializable, and to persist only members that are explicitly marked as JSON properties.\\r\\n- We annotate the fields to be persisted with `[JsonProperty(\"name\")]` to remind us that a field is part of the persisted entity state, and to specify the property name to be used in the JSON representation.\\r\\n\\r\\nHowever, these attributes aren\\'t required; other conventions or attributes are permitted as long as they work with Json.NET. For example, one can use `[DataContract]` attributes, or no attributes at all:\\r\\n\\r\\n```csharp\\r\\n[DataContract]\\r\\npublic class Counter\\r\\n{\\r\\n    [DataMember]\\r\\n    public int Value { get; set; }\\r\\n    ...\\r\\n}\\r\\n\\r\\npublic class Counter\\r\\n{\\r\\n    public int Value;\\r\\n    ...\\r\\n}\\r\\n```\\r\\n\\r\\nBy default, the name of the class isn\\'t\\\\* stored as part of the JSON representation: that is, we use `TypeNameHandling.None` as the default setting. This default behavior can be overridden using `JsonObject` or `JsonProperty` attributes.\\r\\n\\r\\n### Making changes to class definitions\\r\\n\\r\\nSome care is required when making changes to a class definition after an application has been run, because the stored JSON object can no longer match the new class definition. Still, it\\'s often possible to deal correctly with changing data formats as long as one understands the deserialization process used by `JsonConvert.PopulateObject`.\\r\\n\\r\\nFor example, here are some examples of changes and their effect:\\r\\n\\r\\n- When a new property is added, which isn\\'t present in the stored JSON, it assumes its default value.\\r\\n- When a property is removed, which is present in the stored JSON, the previous content is lost.\\r\\n- When a property is renamed, the effect is as if removing the old one and adding a new one.\\r\\n- When the type of a property is changed so it can no longer be deserialized from the stored JSON, an exception is thrown.\\r\\n- When the type of a property is changed, but it can still be deserialized from the stored JSON, it does so.\\r\\n\\r\\nThere are many options available for customizing the behavior of Json.NET. For example, to force an exception if the stored JSON contains a field that isn\\'t present in the class, specify the attribute `JsonObject(MissingMemberHandling = MissingMemberHandling.Error)`. It\\'s also possible to write custom code for deserialization that can read JSON stored in arbitrary formats.\\r\\n\\r\\n## Entity construction\\r\\n\\r\\nSometimes we want to exert more control over how entity objects are constructed. We now describe several options for changing the default behavior when constructing entity objects.\\r\\n\\r\\n### Custom initialization on first access\\r\\n\\r\\nOccasionally we need to perform some special initialization before dispatching an operation to an entity that has never been accessed, or that has been deleted. To specify this behavior, one can add a conditional before the `DispatchAsync`:\\r\\n\\r\\n```csharp\\r\\n[FunctionName(nameof(Counter))]\\r\\npublic static Task Run([EntityTrigger] IDurableEntityContext ctx)\\r\\n{\\r\\n    if (!ctx.HasState)\\r\\n    {\\r\\n        ctx.SetState(...);\\r\\n    }\\r\\n    return ctx.DispatchAsync<Counter>();\\r\\n}\\r\\n```\\r\\n\\r\\n### Bindings in entity classes\\r\\n\\r\\nUnlike regular functions, entity class methods don\\'t have direct access to input and output bindings. Instead, binding data must be captured in the entry-point function declaration and then passed to the `DispatchAsync<T>` method. Any objects passed to `DispatchAsync<T>` is passed automatically to the entity class constructor as an argument.\\r\\n\\r\\nThe following example shows how a `CloudBlobContainer` reference from the [blob input binding](../functions-bindings-storage-blob-input) can be made available to a class-based entity.\\r\\n\\r\\n```csharp\\r\\npublic class BlobBackedEntity\\r\\n{\\r\\n    [JsonIgnore]\\r\\n    private readonly CloudBlobContainer container;\\r\\n\\r\\n    public BlobBackedEntity(CloudBlobContainer container)\\r\\n    {\\r\\n        this.container = container;\\r\\n    }\\r\\n\\r\\n    // ... entity methods can use this.container in their implementations ...\\r\\n\\r\\n    [FunctionName(nameof(BlobBackedEntity))]\\r\\n    public static Task Run(\\r\\n        [EntityTrigger] IDurableEntityContext context,\\r\\n        [Blob(\"my-container\", FileAccess.Read)] CloudBlobContainer container)\\r\\n    {\\r\\n        // passing the binding object as a parameter makes it available to the\\r\\n        // entity class constructor\\r\\n        return context.DispatchAsync<BlobBackedEntity>(container);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nFor more information on bindings in Azure Functions, see the [Azure Functions Triggers and Bindings](../functions-triggers-bindings) documentation.\\r\\n\\r\\n### Dependency injection in entity classes\\r\\n\\r\\nEntity classes support [Azure Functions Dependency Injection](../functions-dotnet-dependency-injection). The following example demonstrates how to register an `IHttpClientFactory` service into a class-based entity.\\r\\n\\r\\n```csharp\\r\\n[assembly: FunctionsStartup(typeof(MyNamespace.Startup))]\\r\\n\\r\\nnamespace MyNamespace\\r\\n{\\r\\n    public class Startup : FunctionsStartup\\r\\n    {\\r\\n        public override void Configure(IFunctionsHostBuilder builder)\\r\\n        {\\r\\n            builder.Services.AddHttpClient();\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nThe following snippet demonstrates how to incorporate the injected service into your entity class.\\r\\n\\r\\n```csharp\\r\\npublic class HttpEntity\\r\\n{\\r\\n    [JsonIgnore]\\r\\n    private readonly HttpClient client;\\r\\n\\r\\n    public HttpEntity(IHttpClientFactory factory)\\r\\n    {\\r\\n        this.client = factory.CreateClient();\\r\\n    }\\r\\n\\r\\n    public Task<int> GetAsync(string url)\\r\\n    {\\r\\n        using (var response = await this.client.GetAsync(url))\\r\\n        {\\r\\n            return (int)response.StatusCode;\\r\\n        }\\r\\n    }\\r\\n\\r\\n    [FunctionName(nameof(HttpEntity))]\\r\\n    public static Task Run([EntityTrigger] IDurableEntityContext ctx)\\r\\n        => ctx.DispatchAsync<HttpEntity>();\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nTo avoid issues with serialization, make sure to exclude fields meant to store injected values from the serialization.\\r\\n\\r\\nNote\\r\\n\\r\\nUnlike when using constructor injection in regular .NET Azure Functions, the functions entry point method for class-based entities *must* be declared `static`. Declaring a non-static function entry point can cause conflicts between the normal Azure Functions object initializer and the Durable Entities object initializer.\\r\\n\\r\\n## Function-based syntax\\r\\n\\r\\nSo far we have focused on the class-based syntax, as we expect it to be better suited for most applications. However, the function-based syntax can be appropriate for applications that wish to define or manage their own abstractions for entity state and operations. Also, it can be appropriate when implementing libraries that require genericity not currently supported by the class-based syntax.\\r\\n\\r\\nWith the function-based syntax, the Entity Function explicitly handles the operation dispatch, and explicitly manages the state of the entity. For example, the following code shows the *Counter* entity implemented using the function-based syntax.\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"Counter\")]\\r\\npublic static void Counter([EntityTrigger] IDurableEntityContext ctx)\\r\\n{\\r\\n    switch (ctx.OperationName.ToLowerInvariant())\\r\\n    {\\r\\n        case \"add\":\\r\\n            ctx.SetState(ctx.GetState<int>() + ctx.GetInput<int>());\\r\\n            break;\\r\\n        case \"reset\":\\r\\n            ctx.SetState(0);\\r\\n            break;\\r\\n        case \"get\":\\r\\n            ctx.Return(ctx.GetState<int>());\\r\\n            break;\\r\\n        case \"delete\":\\r\\n            ctx.DeleteState();\\r\\n            break;\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n### The entity context object\\r\\n\\r\\nEntity-specific functionality can be accessed via a context object of type `IDurableEntityContext`. This context object is available as a parameter to the entity function, and via the async-local property `Entity.Current`.\\r\\n\\r\\nThe following members provide information about the current operation, and allow us to specify a return value.\\r\\n\\r\\n- `EntityName`: the name of the currently executing entity.\\r\\n- `EntityKey`: the key of the currently executing entity.\\r\\n- `EntityId`: the ID of the currently executing entity (includes name and key).\\r\\n- `OperationName`: the name of the current operation.\\r\\n- `GetInput<TInput>()`: gets the input for the current operation.\\r\\n- `Return(arg)`: returns a value to the orchestration that called the operation.\\r\\n\\r\\nThe following members manage the state of the entity (create, read, update, delete).\\r\\n\\r\\n- `HasState`: whether the entity exists, that is, has some state.\\r\\n- `GetState<TState>()`: gets the current state of the entity. If it doesn\\'t already exist, it\\'s created.\\r\\n- `SetState(arg)`: creates or updates the state of the entity.\\r\\n- `DeleteState()`: deletes the state of the entity, if it exists.\\r\\n\\r\\nIf the state returned by `GetState` is an object, it can be directly modified by the application code. There\\'s no need to call `SetState` again at the end (but also no harm). If `GetState<TState>` is called multiple times, the same type must be used.\\r\\n\\r\\nFinally, the following members are used to signal other entities, or start new orchestrations:\\r\\n\\r\\n- `SignalEntity(EntityId, operation, input)`: sends a one-way message to an entity.\\r\\n- `CreateNewOrchestration(orchestratorFunctionName, input)`: starts a new orchestration.\\r\\n\\r\\n# Developer\\'s guide to durable entities in .NET (isolated)\\r\\n\\r\\nIn this article, we describe the available interfaces for developing durable entities with .NET in detail, including examples and general advice.\\r\\n\\r\\nEntity functions provide serverless application developers with a convenient way to organize application state as a collection of fine-grained entities. For more detail about the underlying concepts, see the [Durable Entities: Concepts](durable-functions-entities) article.\\r\\n\\r\\nWe currently offer two APIs for defining entities:\\r\\n\\r\\n- The **class-based syntax** represents entities and operations as classes and methods. This syntax produces easily readable code and allows operations to be invoked in a type-checked manner through interfaces.\\r\\n- The **function-based syntax** is a lower-level interface that represents entities as functions. It provides precise control over how the entity operations are dispatched, and how the entity state is managed.\\r\\n\\r\\nThis article focuses primarily on the class-based syntax, as we expect it to be better suited for most applications. However, the function-based syntax can be appropriate for applications that wish to define or manage their own abstractions for entity state and operations. Also, it can be appropriate for implementing libraries that require genericity not currently supported by the class-based syntax.\\r\\n\\r\\nNote\\r\\n\\r\\nThe class-based syntax is just a layer on top of the function-based syntax, so both variants can be used interchangeably in the same application.\\r\\n\\r\\n## Defining entity classes\\r\\n\\r\\nThe following example is an implementation of a `Counter` entity that stores a single value of type integer, and offers four operations `Add`, `Reset`, `Get`, and `Delete`.\\r\\n\\r\\nNote\\r\\n\\r\\nYou need [Azure Functions Core Tools](../functions-run-local) version `4.0.5455` or above to run entities in the isolated model.\\r\\n\\r\\nThere are two ways of defining an entity as a class in the C# isolated worker model. They produce entities with different state serialization structures.\\r\\n\\r\\nWith the following approach, the entire object is serialized when defining an entity.\\r\\n\\r\\n```csharp\\r\\npublic class Counter\\r\\n{\\r\\n    public int Value { get; set; }\\r\\n\\r\\n    public void Add(int amount) \\r\\n    {\\r\\n        this.Value += amount;\\r\\n    }\\r\\n\\r\\n    public Task Reset() \\r\\n    {\\r\\n        this.Value = 0;\\r\\n        return Task.CompletedTask;\\r\\n    }\\r\\n\\r\\n    public Task<int> Get() \\r\\n    {\\r\\n        return Task.FromResult(this.Value);\\r\\n    }\\r\\n\\r\\n    // Delete is implicitly defined when defining an entity this way\\r\\n\\r\\n    [Function(nameof(Counter))]\\r\\n    public static Task Run([EntityTrigger] TaskEntityDispatcher dispatcher)\\r\\n        => dispatcher.DispatchAsync<Counter>();\\r\\n}\\r\\n```\\r\\n\\r\\nA `TaskEntity<TState>`-based implementation, which makes it easy to use dependency injection. In this case, state is deserialized to the `State` property, and no other property is serialized/deserialized.\\r\\n\\r\\n```csharp\\r\\npublic class Counter : TaskEntity<int>\\r\\n{\\r\\n    readonly ILogger logger; \\r\\n\\r\\n    public Counter(ILogger<Counter> logger)\\r\\n    {\\r\\n        this.logger = logger; \\r\\n    }\\r\\n\\r\\n    public int Add(int amount) \\r\\n    {\\r\\n        this.State += amount;\\r\\n    }\\r\\n\\r\\n    public Reset() \\r\\n    {\\r\\n        this.State = 0;\\r\\n        return Task.CompletedTask;\\r\\n    }\\r\\n\\r\\n    public Task<int> Get() \\r\\n    {\\r\\n        return Task.FromResult(this.State);\\r\\n    }\\r\\n\\r\\n    // Delete is implicitly defined when defining an entity this way\\r\\n\\r\\n    [Function(nameof(Counter))]\\r\\n    public static Task Run([EntityTrigger] TaskEntityDispatcher dispatcher)\\r\\n        => dispatcher.DispatchAsync<Counter>();\\r\\n}\\r\\n```\\r\\n\\r\\nWarning\\r\\n\\r\\nWhen writing entities that derive from `ITaskEntity` or `TaskEntity<TState>`, it is important to **not** name your entity trigger method `RunAsync`. This will cause runtime errors when invoking the entity as there is an ambiguous match with the method name \"RunAsync\" due to `ITaskEntity` already defining an instance-level \"RunAsync\".\\r\\n\\r\\n### Deleting entities in the isolated model\\r\\n\\r\\nDeleting an entity in the isolated model is accomplished by setting the entity state to `null`. How this is accomplished depends on what entity implementation path is being used.\\r\\n\\r\\n- When deriving from `ITaskEntity` or using function based syntax, delete is accomplished by calling `TaskEntityOperation.State.SetState(null)`.\\r\\n- When deriving from `TaskEntity<TState>`, delete is implicitly defined. However, it can be overridden by defining a method `Delete` on the entity. State can also be deleted from any operation via `this.State = null`.\\r\\n    - To delete by setting state to null requires `TState` to be nullable.\\r\\n    - The implicitly defined delete operation deletes non-nullable `TState`.\\r\\n- When using a POCO as your state (not deriving from `TaskEntity<TState>`), delete is implicitly defined. It\\'s possible to override the delete operation by defining a method `Delete` on the POCO. However, there\\'s no way to set state to `null` in the POCO route so the implicitly defined delete operation is the only true delete.\\r\\n\\r\\n## Accessing entities directly\\r\\n\\r\\nClass-based entities can be accessed directly, using explicit string names for the entity and its operations. This section provides examples. For a deeper explanation of the underlying concepts (such as signals vs. calls), see the discussion in [Access entities](durable-functions-entities#access-entities).\\r\\n\\r\\n### Example: client signals entity\\r\\n\\r\\nThe following Azure Http Function implements a DELETE operation using REST conventions. It sends a delete signal to the counter entity whose key is passed in the URL path.\\r\\n\\r\\n```csharp\\r\\n[Function(\"DeleteCounter\")]\\r\\npublic static async Task<HttpResponseData> DeleteCounter(\\r\\n    [HttpTrigger(AuthorizationLevel.Function, \"delete\", Route = \"Counter/{entityKey}\")] HttpRequestData req,\\r\\n    [DurableClient] DurableTaskClient client, string entityKey)\\r\\n{\\r\\n    var entityId = new EntityInstanceId(\"Counter\", entityKey);\\r\\n    await client.Entities.SignalEntityAsync(entityId, \"Delete\");\\r\\n    return req.CreateResponse(HttpStatusCode.Accepted);\\r\\n}\\r\\n```\\r\\n\\r\\n### Example: client reads entity state\\r\\n\\r\\nThe following Azure Http Function implements a GET operation using REST conventions. It reads the current state of the counter entity whose key is passed in the URL path.\\r\\n\\r\\n```csharp\\r\\n[Function(\"GetCounter\")]\\r\\npublic static async Task<HttpResponseData> GetCounter(\\r\\n    [HttpTrigger(AuthorizationLevel.Function, \"get\", Route = \"Counter/{entityKey}\")] HttpRequestData req,\\r\\n    [DurableClient] DurableTaskClient client, string entityKey)\\r\\n{\\r\\n    var entityId = new EntityInstanceId(\"Counter\", entityKey);\\r\\n    EntityMetadata<int>? entity = await client.Entities.GetEntityAsync<int>(entityId);\\r\\n    HttpResponseData response = request.CreateResponse(HttpStatusCode.OK);\\r\\n    await response.WriteAsJsonAsync(entity.State);\\r\\n\\r\\n    return response;\\r\\n}\\r\\n```\\r\\n\\r\\n### Example: orchestration first signals then calls entity\\r\\n\\r\\nThe following orchestration signals a counter entity to increment it, and then calls the same entity to read its latest value.\\r\\n\\r\\n```csharp\\r\\n[Function(\"IncrementThenGet\")]\\r\\npublic static async Task<int> Run([OrchestrationTrigger] TaskOrchestrationContext context)\\r\\n{\\r\\n    var entityId = new EntityInstanceId(\"Counter\", \"myCounter\");\\r\\n\\r\\n    // One-way signal to the entity - does not await a response\\r\\n    await context.Entities.SignalEntityAsync(entityId, \"Add\", 1);\\r\\n\\r\\n    // Two-way call to the entity which returns a value - awaits the response\\r\\n    int currentValue = await context.Entities.CallEntityAsync<int>(entityId, \"Get\");\\r\\n\\r\\n    return currentValue; \\r\\n}\\r\\n```\\r\\n\\r\\n## Accessing entities through interfaces\\r\\n\\r\\nThis is currently not supported in the .NET isolated worker.\\r\\n\\r\\n## Entity serialization\\r\\n\\r\\nSerialization default behavior has changed from `Newtonsoft.Json` to` System.Text.Json`. For more information, see [here](durable-functions-serialization-and-persistence?tabs=csharp-isolated#customizing-serialization-and-deserialization).\\r\\n\\r\\n## Entity construction\\r\\n\\r\\nSometimes we want to exert more control over how entity objects are constructed. We now describe several options for changing the default behavior when constructing entity objects.\\r\\n\\r\\n### Custom initialization on first access\\r\\n\\r\\n```csharp\\r\\npublic class Counter : TaskEntity<int>\\r\\n{\\r\\n    protected override int InitializeState(TaskEntityOperation operation)\\r\\n    {\\r\\n        // This is called when state is null, giving a chance to customize first-access of entity.\\r\\n        return 10;\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n### Bindings in entity classes\\r\\n\\r\\nThe following example shows how to use a [blob input binding](../functions-bindings-storage-blob-input) in a class-based entity.\\r\\n\\r\\n```csharp\\r\\npublic class BlobBackedEntity : TaskEntity<object?>\\r\\n{\\r\\n    private BlobContainerClient Container { get; set; }\\r\\n\\r\\n    [Function(nameof(BlobBackedEntity))]\\r\\n    public Task DispatchAsync(\\r\\n        [EntityTrigger] TaskEntityDispatcher dispatcher, \\r\\n        [BlobInput(\"my-container\")] BlobContainerClient container)\\r\\n    {\\r\\n        this.Container = container;\\r\\n        return dispatcher.DispatchAsync(this);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nFor more information on bindings in Azure Functions, see the [Azure Functions Triggers and Bindings](../functions-triggers-bindings) documentation.\\r\\n\\r\\n### Dependency injection in entity classes\\r\\n\\r\\nEntity classes support [Azure Functions Dependency Injection](../dotnet-isolated-process-guide#dependency-injection).\\r\\n\\r\\nThe following demonstrates how to configure an `HttpClient` in the `program.cs` file to be imported later in the entity class.\\r\\n\\r\\n```csharp\\r\\npublic class Program\\r\\n{\\r\\n    public static void Main()\\r\\n    {\\r\\n        IHost host = new HostBuilder()\\r\\n            .ConfigureFunctionsWorkerDefaults((IFunctionsWorkerApplicationBuilder workerApplication) =>\\r\\n            {\\r\\n                workerApplication.Services.AddHttpClient<HttpEntity>()\\r\\n                    .ConfigureHttpClient(client => {/* configure http client here */});\\r\\n             })\\r\\n            .Build();\\r\\n\\r\\n        host.Run();\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nHere\\'s how to incorporate the injected service into your entity class.\\r\\n\\r\\n```csharp\\r\\npublic class HttpEntity : TaskEntity<object?>\\r\\n{\\r\\n    private readonly HttpClient client;\\r\\n\\r\\n     public HttpEntity(HttpClient client)\\r\\n    {\\r\\n        this.client = client;\\r\\n    }\\r\\n\\r\\n    public async Task<int> GetAsync(string url)\\r\\n    {\\r\\n        using var response = await this.client.GetAsync(url);\\r\\n        return (int)response.StatusCode;\\r\\n    }\\r\\n\\r\\n    [Function(nameof(HttpEntity))]\\r\\n    public static Task Run([EntityTrigger] TaskEntityDispatcher dispatcher)\\r\\n        => dispatcher.DispatchAsync<HttpEntity>();\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nTo avoid issues with serialization, make sure to exclude fields meant to store injected values from the serialization.\\r\\n\\r\\nNote\\r\\n\\r\\nUnlike when using constructor injection in regular .NET Azure Functions, the functions entry point method for class-based entities *must* be declared `static`. Declaring a non-static function entry point can cause conflicts between the normal Azure Functions object initializer and the Durable Entities object initializer.\\r\\n\\r\\n## Function-based syntax\\r\\n\\r\\nSo far we have focused on the class-based syntax, as we expect it to be better suited for most applications. However, the function-based syntax can be appropriate for applications that wish to define or manage their own abstractions for entity state and operations. Also, it can be appropriate when implementing libraries that require genericity not currently supported by the class-based syntax.\\r\\n\\r\\nWith the function-based syntax, the Entity Function explicitly handles the operation dispatch, and explicitly manages the state of the entity. For example, the following code shows the *Counter* entity implemented using the function-based syntax.\\r\\n\\r\\n```csharp\\r\\n[Function(nameof(Counter))]\\r\\npublic static Task DispatchAsync([EntityTrigger] TaskEntityDispatcher dispatcher)\\r\\n{\\r\\n    return dispatcher.DispatchAsync(operation =>\\r\\n    {\\r\\n        if (operation.State.GetState(typeof(int)) is null)\\r\\n        {\\r\\n            operation.State.SetState(0);\\r\\n        }\\r\\n\\r\\n        switch (operation.Name.ToLowerInvariant())\\r\\n        {\\r\\n            case \"add\":\\r\\n                int state = operation.State.GetState<int>();\\r\\n                state += operation.GetInput<int>();\\r\\n                operation.State.SetState(state);\\r\\n                return new(state);\\r\\n            case \"reset\":\\r\\n                operation.State.SetState(0);\\r\\n                break;\\r\\n            case \"get\":\\r\\n                return new(operation.State.GetState<int>());\\r\\n            case \"delete\": \\r\\n                operation.State.SetState(null);\\r\\n                break; \\r\\n        }\\r\\n\\r\\n        return default;\\r\\n    });\\r\\n}\\r\\n```',\n",
       "  'metadata': {}},\n",
       " {'_id': 'f342b1e6-9f4f-8f25-2fbe-b23d12b2cf8b',\n",
       "  'title': 'Overview of Durable Functions in the .NET isolated worker - Azure',\n",
       "  'text': '# Overview of Durable Functions in the .NET isolated worker\\r\\n\\r\\nThis article is an overview of Durable Functions in the [.NET isolated worker](../dotnet-isolated-process-guide). The isolated worker allows your Durable Functions app to run on a .NET version different than that of the Azure Functions host.\\r\\n\\r\\n## Why use Durable Functions in the .NET isolated worker?\\r\\n\\r\\nUsing this model lets you get all the great benefits that come with the Azure Functions .NET isolated worker process. For more information, see [Benefits of the isolated worker model](../dotnet-isolated-process-guide#benefits-of-the-isolated-worker-model). Additionally, this new SDK includes some new features.\\r\\n\\r\\n### Feature improvements over in-process Durable Functions\\r\\n\\r\\n- Orchestration input can be injected directly: `MyOrchestration([OrchestrationTrigger] TaskOrchestrationContext context, T input)`\\r\\n- Support for strongly typed calls and class-based activities and orchestrations (NOTE: in preview. For more information, see here.)\\r\\n- Plus all the benefits of the Azure Functions .NET isolated worker.\\r\\n\\r\\n### Source generator and class-based activities and orchestrations\\r\\n\\r\\n**Requirement**: add `<PackageReference Include=\"Microsoft.DurableTask.Generators\" Version=\"1.0.0-preview.1\" />` to your project.\\r\\n\\r\\nBy adding the source generator package, you get access to two new features:\\r\\n\\r\\n- **Class-based activities and orchestrations**, an alternative way to write Durable Functions. Instead of \"function-based\", you write strongly typed classes, which inherit types from the Durable SDK.\\r\\n- **Strongly typed extension methods** for invoking sub orchestrations and activities. These extension methods can also be used from \"function-based\" activities and orchestrations.\\r\\n\\r\\n#### Function-based example\\r\\n\\r\\n```csharp\\r\\npublic static class MyFunctions\\r\\n{\\r\\n    [Function(nameof(MyActivity))] \\r\\n    public static async Task<string> MyActivity([ActivityTrigger] string input)\\r\\n    {\\r\\n        // implementation\\r\\n    }\\r\\n\\r\\n    [Function(nameof(MyOrchestration))] \\r\\n    public static async Task<string> MyOrchestration([OrchestrationTrigger] TaskOrchestrationContext context, string input)\\r\\n    {\\r\\n        // implementation\\r\\n        return await context.CallActivityAsync(nameof(MyActivity), input);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n#### Class-based example\\r\\n\\r\\n```csharp\\r\\n[DurableTask(nameof(MyActivity))]\\r\\npublic class MyActivity : TaskActivity<string, string>\\r\\n{\\r\\n    private readonly ILogger logger;\\r\\n\\r\\n    public MyActivity(ILogger<MyActivity> logger) // activities have access to DI.\\r\\n    {\\r\\n        this.logger = logger;\\r\\n    }\\r\\n\\r\\n    public async override Task<string> RunAsync(TaskActivityContext context, string input)\\r\\n    {\\r\\n        // implementation\\r\\n    }\\r\\n}\\r\\n\\r\\n[DurableTask(nameof(MyOrchestration))]\\r\\npublic class MyOrchestration : TaskOrchestrator<string, string>\\r\\n{\\r\\n    public async override Task<string> RunAsync(TaskOrchestrationContext context, string input)\\r\\n    {\\r\\n        ILogger logger = context.CreateReplaySafeLogger<MyOrchestration>(); // orchestrations do NOT have access to DI.\\r\\n\\r\\n        // An extension method was generated for directly invoking \"MyActivity\".\\r\\n        return await context.CallMyActivityAsync(input);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n## Durable entities\\r\\n\\r\\nDurable entities are supported in the .NET isolated worker. See [developer\\'s guide](durable-functions-dotnet-entities).\\r\\n\\r\\n## Migration guide\\r\\n\\r\\nThis guide assumes you\\'re starting with a .NET Durable Functions 2.x project.\\r\\n\\r\\n### Update your project\\r\\n\\r\\nThe first step is to update your project to [Azure Functions .NET isolated](../migrate-version-3-version-4). Then, update your Durable Functions NuGet package references.\\r\\n\\r\\nOld:\\r\\n\\r\\n```xml\\r\\n<ItemGroup>\\r\\n  <PackageReference Include=\"Microsoft.Azure.WebJobs.Extensions.DurableTask\" Version=\"2.9.0\" />\\r\\n</ItemGroup>\\r\\n```\\r\\n\\r\\nNew:\\r\\n\\r\\n```xml\\r\\n<ItemGroup>\\r\\n  <PackageReference Include=\"Microsoft.Azure.Functions.Worker.Extensions.DurableTask\" Version=\"1.1.0\" />\\r\\n</ItemGroup>\\r\\n```\\r\\n\\r\\n### Update your code\\r\\n\\r\\nDurable Functions for .NET isolated worker is an entirely new package with different types and namespaces. There are required changes to your code as a result, but many of the APIs line up with no changes needed.\\r\\n\\r\\n#### Host.json schema\\r\\n\\r\\nThe schema for Durable Functions .NET isolated worker and Durable Functions 2.x has remained the same, no changes should be needed.\\r\\n\\r\\n#### Public API changes\\r\\n\\r\\nThis table isn\\'t an exhaustive list of changes.\\r\\n\\r\\n| 2.x | Isolated |\\r\\n| --- | --- |\\r\\n| `IDurableOrchestrationClient` | `DurableTaskClient` |\\r\\n| `IDurableOrchestrationClient.StartNewAsync` | `DurableTaskClient.ScheduleNewOrchestrationInstanceAsync` |\\r\\n| `IDurableEntityClient.SignalEntityAsync` | `DurableTaskClient.Entities.SignalEntityAsync` |\\r\\n| `IDurableEntityClient.ReadEntityStateAsync` | `DurableTaskClient.Entities.GetEntityAsync` |\\r\\n| `IDurableEntityClient.ListEntitiesAsync` | `DurableTaskClient.Entities.GetAllEntitiesAsync` |\\r\\n| `IDurableEntityClient.CleanEntityStorageAsync` | `DurableTaskClient.Entities.CleanEntityStorageAsync` |\\r\\n| `IDurableOrchestrationContext` | `TaskOrchestrationContext` |\\r\\n| `IDurableOrchestrationContext.GetInput<T>()` | `TaskOrchestrationContext.GetInput<T>()` or inject input as a parameter: `MyOrchestration([OrchestrationTrigger] TaskOrchestrationContext context, T input)` |\\r\\n| `DurableActivityContext` | No equivalent |\\r\\n| `DurableActivityContext.GetInput<T>()` | Inject input as a parameter `MyActivity([ActivityTrigger] T input)` |\\r\\n| `IDurableOrchestrationContext.CallActivityWithRetryAsync` | `TaskOrchestrationContext.CallActivityAsync`, include `TaskOptions` parameter with retry details. |\\r\\n| `IDurableOrchestrationContext.CallSubOrchestratorWithRetryAsync` | `TaskOrchestrationContext.CallSubOrchestratorAsync`, include `TaskOptions` parameter with retry details. |\\r\\n| `IDurableOrchestrationContext.CallHttpAsync` | `TaskOrchestrationContext.CallHttpAsync` |\\r\\n| `IDurableOrchestrationContext.CreateReplaySafeLogger(ILogger)` | `TaskOrchestrationContext.CreateReplaySafeLogger<T>()` or `TaskOrchestrationContext.CreateReplaySafeLogger(string)` |\\r\\n| `IDurableOrchestrationContext.CallEntityAsync` | `TaskOrchestrationContext.Entities.CallEntityAsync` |\\r\\n| `IDurableOrchestrationContext.SignalEntity` | `TaskOrchestrationContext.Entities.SignalEntityAsync` |\\r\\n| `IDurableOrchestrationContext.LockAsync` | `TaskOrchestrationContext.Entities.LockEntitiesAsync` |\\r\\n| `IDurableOrchestrationContext.IsLocked` | `TaskOrchestrationContext.Entities.InCriticalSection` |\\r\\n| `IDurableEntityContext` | `TaskEntityContext`. |\\r\\n| `IDurableEntityContext.EntityName` | `TaskEntityContext.Id.Name` |\\r\\n| `IDurableEntityContext.EntityKey` | `TaskEntityContext.Id.Key` |\\r\\n| `IDurableEntityContext.OperationName` | `TaskEntityOperation.Name` |\\r\\n| `IDurableEntityContext.FunctionBindingContext` | Removed, add `FunctionContext` as an input parameter |\\r\\n| `IDurableEntityContext.HasState` | `TaskEntityOperation.State.HasState` |\\r\\n| `IDurableEntityContext.BatchSize` | Removed |\\r\\n| `IDurableEntityContext.BatchPosition` | Removed |\\r\\n| `IDurableEntityContext.GetState` | `TaskEntityOperation.State.GetState` |\\r\\n| `IDurableEntityContext.SetState` | `TaskEntityOperation.State.SetState` |\\r\\n| `IDurableEntityContext.DeleteState` | `TaskEntityOperation.State.SetState(null)` |\\r\\n| `IDurableEntityContext.GetInput` | `TaskEntityOperation.GetInput` |\\r\\n| `IDurableEntityContext.Return` | Removed. Method return value used instead. |\\r\\n| `IDurableEntityContext.SignalEntity` | `TaskEntityContext.SignalEntity` |\\r\\n| `IDurableEntityContext.StartNewOrchestration` | `TaskEntityContext.ScheduleNewOrchestration` |\\r\\n| `IDurableEntityContext.DispatchAsync` | `TaskEntityDispatcher.DispatchAsync`. Constructor params removed. |\\r\\n| `IDurableOrchestrationClient.GetStatusAsync` | `DurableTaskClient.GetInstanceAsync` |\\r\\n\\r\\n#### Behavioral changes\\r\\n\\r\\n- Serialization default behavior has changed from `Newtonsoft.Json` to `System.Text.Json`. For more information, see [here](durable-functions-serialization-and-persistence).',\n",
       "  'metadata': {}},\n",
       " {'_id': '4001488c-8280-7d5a-eb77-f7e45bfdf4fb',\n",
       "  'title': 'Durable entities - Azure Functions',\n",
       "  'text': '# Entity functions (csharp)\\r\\n\\r\\nEntity functions define operations for reading and updating small pieces of state, known as *durable entities*. Like orchestrator functions, entity functions are functions with a special trigger type, the *entity trigger*. Unlike orchestrator functions, entity functions manage the state of an entity explicitly, rather than implicitly representing state via control flow. Entities provide a means for scaling out applications by distributing the work across many entities, each with a modestly sized state.\\r\\n\\r\\nNote\\r\\n\\r\\nEntity functions and related functionality are only available in [Durable Functions 2.0](durable-functions-versions#migrate-from-1x-to-2x) and above. They are currently supported in .NET in-proc, .NET isolated worker, JavaScript, and Python, but not in PowerShell or Java. Furthermore, entity functions for .NET Isolated are supported when using the Azure Storage or Netherite state providers, but not when using the MSSQL state provider.\\r\\n\\r\\n## General concepts\\r\\n\\r\\nEntities behave a bit like tiny services that communicate via messages. Each entity has a unique identity and an internal state (if it exists). Like services or objects, entities perform operations when prompted to do so. When an operation executes, it might update the internal state of the entity. It might also call external services and wait for a response. Entities communicate with other entities, orchestrations, and clients by using messages that are implicitly sent via reliable queues.\\r\\n\\r\\nTo prevent conflicts, all operations on a single entity are guaranteed to execute serially, that is, one after another.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen an entity is invoked, it processes its payload to completion and then schedules a new execution to activate once future inputs arrive. As a result, your entity execution logs might show an extra execution after each entity invocation; this is expected.\\r\\n\\r\\n### Entity ID\\r\\n\\r\\nEntities are accessed via a unique identifier, the *entity ID*. An entity ID is simply a pair of strings that uniquely identifies an entity instance. It consists of an:\\r\\n\\r\\n- **Entity name**, which is a name that identifies the type of the entity. An example is \"Counter.\" This name must match the name of the entity function that implements the entity. It isn\\'t sensitive to case.\\r\\n- **Entity key**, which is a string that uniquely identifies the entity among all other entities of the same name. An example is a GUID.\\r\\n\\r\\nFor example, a `Counter` entity function might be used for keeping score in an online game. Each instance of the game has a unique entity ID, such as `@Counter@Game1` and `@Counter@Game2`. All operations that target a particular entity require specifying an entity ID as a parameter.\\r\\n\\r\\n### Entity operations\\r\\n\\r\\nTo invoke an operation on an entity, specify the:\\r\\n\\r\\n- **Entity ID** of the target entity.\\r\\n- **Operation name**, which is a string that specifies the operation to perform. For example, the `Counter` entity could support `add`, `get`, or `reset` operations.\\r\\n- **Operation input**, which is an optional input parameter for the operation. For example, the add operation can take an integer amount as the input.\\r\\n- **Scheduled time**, which is an optional parameter for specifying the delivery time of the operation. For example, an operation can be reliably scheduled to run several days in the future.\\r\\n\\r\\nOperations can return a result value or an error result, such as a JavaScript error or a .NET exception. This result or error occurs in orchestrations that called the operation.\\r\\n\\r\\nAn entity operation can also create, read, update, and delete the state of the entity. The state of the entity is always durably persisted in storage.\\r\\n\\r\\n## Define entities\\r\\n\\r\\nCurrently, there are two distinct APIs for defining entities in .NET:\\r\\n\\r\\n**Function-based syntax**\\r\\nWhen you use a function-based syntax, entities are represented as functions and operations are explicitly dispatched by the application. This syntax works well for entities with simple state, few operations, or a dynamic set of operations like in application frameworks. This syntax can be tedious to maintain because it doesn\\'t catch type errors at compile time.\\r\\n\\r\\n**Class-based syntax**\\r\\nWhen you use a class-based syntax, .NET classes and methods represent entities and operations. This syntax produces more easily readable code and allows operations to be invoked in a type-safe way. The class-based syntax is a thin layer on top of the function-based syntax, so both variants can be used interchangeably in the same application.\\r\\n\\r\\nThe specific APIs depend on whether your C# functions run in an *isolated worker process* (recommended) or in the same process as the host.\\r\\n\\r\\n**In-process**\\r\\nThe following code is an example of a simple `Counter` entity implemented as a durable function. This function defines three operations, `add`, `reset`, and `get`, each of which operates on an integer state.\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"Counter\")]\\r\\npublic static void Counter([EntityTrigger] IDurableEntityContext ctx)\\r\\n{\\r\\n    switch (ctx.OperationName.ToLowerInvariant())\\r\\n    {\\r\\n        case \"add\":\\r\\n            ctx.SetState(ctx.GetState<int>() + ctx.GetInput<int>());\\r\\n            break;\\r\\n        case \"reset\":\\r\\n            ctx.SetState(0);\\r\\n            break;\\r\\n        case \"get\":\\r\\n            ctx.Return(ctx.GetState<int>());\\r\\n            break;\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nFor more information on the function-based syntax and how to use it, see [Function-based syntax](durable-functions-dotnet-entities#function-based-syntax).\\r\\n\\r\\n**In-process**\\r\\nThe following example is an equivalent implementation of the `Counter` entity using classes and methods.\\r\\n\\r\\n```csharp\\r\\n[JsonObject(MemberSerialization.OptIn)]\\r\\npublic class Counter\\r\\n{\\r\\n    [JsonProperty(\"value\")]\\r\\n    public int CurrentValue { get; set; }\\r\\n\\r\\n    public void Add(int amount) => this.CurrentValue += amount;\\r\\n\\r\\n    public void Reset() => this.CurrentValue = 0;\\r\\n\\r\\n    public int Get() => this.CurrentValue;\\r\\n\\r\\n    [FunctionName(nameof(Counter))]\\r\\n    public static Task Run([EntityTrigger] IDurableEntityContext ctx)\\r\\n        => ctx.DispatchAsync<Counter>();\\r\\n}\\r\\n```\\r\\n\\r\\nThe state of this entity is an object of type `Counter`, which contains a field that stores the current value of the counter. To persist this object in storage, it\\'s serialized and deserialized by the [Json.NET](https://www.newtonsoft.com/json) library.\\r\\n\\r\\nFor more information on the class-based syntax and how to use it, see [Defining entity classes](durable-functions-dotnet-entities#defining-entity-classes).\\r\\n\\r\\n**Isolated worker process**\\r\\n\\r\\n```csharp\\r\\n[Function(nameof(Counter))]\\r\\npublic static Task DispatchAsync([EntityTrigger] TaskEntityDispatcher dispatcher)\\r\\n{\\r\\n    return dispatcher.DispatchAsync(operation =>\\r\\n    {\\r\\n        if (operation.State.GetState(typeof(int)) is null)\\r\\n        {\\r\\n            operation.State.SetState(0);\\r\\n        }\\r\\n\\r\\n        switch (operation.Name.ToLowerInvariant())\\r\\n        {\\r\\n            case \"add\":\\r\\n                int state = operation.State.GetState<int>();\\r\\n                state += operation.GetInput<int>();\\r\\n                operation.State.SetState(state);\\r\\n                return new(state);\\r\\n            case \"reset\":\\r\\n                operation.State.SetState(0);\\r\\n                break;\\r\\n            case \"get\":\\r\\n                return new(operation.State.GetState<int>());\\r\\n            case \"delete\": \\r\\n                operation.State.SetState(null);\\r\\n                break; \\r\\n        }\\r\\n\\r\\n        return default;\\r\\n    });\\r\\n}\\r\\n```\\r\\n\\r\\n**Isolated worker process**\\r\\nThe following example shows the implementation of the `Counter` entity using classes and methods.\\r\\n\\r\\n```csharp\\r\\npublic class Counter\\r\\n{\\r\\n    public int CurrentValue { get; set; }\\r\\n\\r\\n    public void Add(int amount) => this.CurrentValue += amount;\\r\\n\\r\\n    public void Reset() => this.CurrentValue = 0;\\r\\n\\r\\n    public int Get() => this.CurrentValue;\\r\\n\\r\\n    [Function(nameof(Counter))]\\r\\n    public static Task RunEntityAsync([EntityTrigger] TaskEntityDispatcher dispatcher)\\r\\n    {\\r\\n        return dispatcher.DispatchAsync<Counter>();\\r\\n    }\\r\\n}\\r\\n\\r\\n```\\r\\n\\r\\nThe following example implements a `Counter` entity by directly implementing `TaskEntity<TState>`, which gives the added benefit of being able to use Dependency Injection.\\r\\n\\r\\n```csharp\\r\\npublic class Counter : TaskEntity<int>\\r\\n{\\r\\n    readonly ILogger logger;\\r\\n\\r\\n    public Counter(ILogger<Counter> logger)\\r\\n    {\\r\\n        this.logger = logger; \\r\\n    }\\r\\n\\r\\n    public void Add(int amount) => this.State += amount;\\r\\n\\r\\n    public void Reset() => this.State = 0;\\r\\n\\r\\n    public int Get() => this.State;\\r\\n\\r\\n    [Function(nameof(Counter))]\\r\\n    public Task RunEntityAsync([EntityTrigger] TaskEntityDispatcher dispatcher)\\r\\n    {\\r\\n        return dispatcher.DispatchAsync(this);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nYou can also dispatch by using a static method.\\r\\n\\r\\n```csharp\\r\\n[Function(nameof(Counter))]\\r\\npublic static Task RunEntityStaticAsync([EntityTrigger] TaskEntityDispatcher dispatcher)\\r\\n{\\r\\n    return dispatcher.DispatchAsync<Counter>();\\r\\n}\\r\\n```\\r\\n\\r\\n## Access entities\\r\\n\\r\\nEntities can be accessed using one-way or two-way communication. The following terminology distinguishes the two forms of communication:\\r\\n\\r\\n- **Calling** an entity uses two-way (round-trip) communication. You send an operation message to the entity, and then wait for the response message before you continue. The response message can provide a result value or an error result, such as a JavaScript error or a .NET exception. This result or error is then observed by the caller.\\r\\n- **Signaling** an entity uses one-way (fire and forget) communication. You send an operation message but don\\'t wait for a response. While the message is guaranteed to be delivered eventually, the sender doesn\\'t know when and can\\'t observe any result value or errors.\\r\\n\\r\\nEntities can be accessed from within client functions, from within orchestrator functions, or from within entity functions. Not all forms of communication are supported by all contexts:\\r\\n\\r\\n- From within clients, you can signal entities and you can read the entity state.\\r\\n- From within orchestrations, you can signal entities and you can call entities.\\r\\n- From within entities, you can signal entities.\\r\\n\\r\\nThe following examples illustrate these various ways of accessing entities.\\r\\n\\r\\n### Example: Client signals an entity\\r\\n\\r\\nTo access entities from an ordinary Azure Function, which is also known as a client function, use the [entity client binding](durable-functions-bindings#entity-client). The following example shows a queue-triggered function signaling an entity using this binding.\\r\\n\\r\\n**In-process**\\r\\nNote\\r\\n\\r\\nFor simplicity, the following examples show the loosely typed syntax for accessing entities. In general, we recommend that you [access entities through interfaces](durable-functions-dotnet-entities#accessing-entities-through-interfaces) because it provides more type checking.\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"AddFromQueue\")]\\r\\npublic static Task Run(\\r\\n    [QueueTrigger(\"durable-function-trigger\")] string input,\\r\\n    [DurableClient] IDurableEntityClient client)\\r\\n{\\r\\n    // Entity operation input comes from the queue message content.\\r\\n    var entityId = new EntityId(nameof(Counter), \"myCounter\");\\r\\n    int amount = int.Parse(input);\\r\\n    return client.SignalEntityAsync(entityId, \"Add\", amount);\\r\\n}\\r\\n```\\r\\n\\r\\n**Isolated worker process**\\r\\n\\r\\n```csharp\\r\\n[Function(\"AddFromQueue\")]\\r\\npublic static Task Run(\\r\\n    [QueueTrigger(\"durable-function-trigger\")] string input, [DurableClient] DurableTaskClient client)\\r\\n{\\r\\n    // Entity operation input comes from the queue message content. \\r\\n    var entityId = new EntityInstanceId(nameof(Counter), \"myCounter\");\\r\\n    int amount = int.Parse(input);\\r\\n    return client.Entities.SignalEntityAsync(entityId, \"Add\", amount);\\r\\n}\\r\\n```\\r\\n\\r\\nThe term *signal* means that the entity API invocation is one-way and asynchronous. It\\'s not possible for a client function to know when the entity has processed the operation. Also, the client function can\\'t observe any result values or exceptions.\\r\\n\\r\\n### Example: Client reads an entity state\\r\\n\\r\\nClient functions can also query the state of an entity, as shown in the following example:\\r\\n\\r\\n**In-process**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"QueryCounter\")]\\r\\npublic static async Task<HttpResponseMessage> Run(\\r\\n    [HttpTrigger(AuthorizationLevel.Function)] HttpRequestMessage req,\\r\\n    [DurableClient] IDurableEntityClient client)\\r\\n{\\r\\n    var entityId = new EntityId(nameof(Counter), \"myCounter\");\\r\\n    EntityStateResponse<JObject> stateResponse = await client.ReadEntityStateAsync<JObject>(entityId);\\r\\n    return req.CreateResponse(HttpStatusCode.OK, stateResponse.EntityState);\\r\\n}\\r\\n```\\r\\n\\r\\n**Isolated worker process**\\r\\n\\r\\n```csharp\\r\\n[Function(\"QueryCounter\")]\\r\\npublic static async Task<HttpResponseData> Run(\\r\\n    [HttpTrigger(AuthorizationLevel.Function)] HttpRequestData req,\\r\\n    [DurableClient] DurableTaskClient client)\\r\\n{\\r\\n    var entityId = new EntityInstanceId(nameof(Counter), \"myCounter\");\\r\\n    EntityMetadata<int>? entity = await client.Entities.GetEntityAsync<int>(entityId);\\r\\n\\r\\n    if (entity is null)\\r\\n    {\\r\\n        return req.CreateResponse(HttpStatusCode.NotFound);\\r\\n    }\\r\\n\\r\\n    HttpResponseData response = req.CreateResponse(HttpStatusCode.OK);\\r\\n    await response.WriteAsJsonAsync(entity);\\r\\n\\r\\n    return response;\\r\\n}\\r\\n```\\r\\n\\r\\nEntity state queries are sent to the Durable tracking store and return the entity\\'s most recently persisted state. This state is always a \"committed\" state, that is, it\\'s never a temporary intermediate state assumed in the middle of executing an operation. However, it\\'s possible that this state is stale compared to the entity\\'s in-memory state. Only orchestrations can read an entity\\'s in-memory state, as described in the following section.\\r\\n\\r\\n### Example: Orchestration signals and calls an entity\\r\\n\\r\\nOrchestrator functions can access entities by using APIs on the [orchestration trigger binding](durable-functions-bindings#orchestration-trigger). The following example code shows an orchestrator function calling and signaling a `Counter` entity.\\r\\n\\r\\n**In-process**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"CounterOrchestration\")]\\r\\npublic static async Task Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    var entityId = new EntityId(nameof(Counter), \"myCounter\");\\r\\n\\r\\n    // Two-way call to the entity which returns a value - awaits the response\\r\\n    int currentValue = await context.CallEntityAsync<int>(entityId, \"Get\");\\r\\n    if (currentValue < 10)\\r\\n    {\\r\\n        // One-way signal to the entity which updates the value - does not await a response\\r\\n        context.SignalEntity(entityId, \"Add\", 1);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n**Isolated worker process**\\r\\n\\r\\n```csharp\\r\\n[Function(\"CounterOrchestration\")]\\r\\npublic static async Task Run([OrchestrationTrigger] TaskOrchestrationContext context)\\r\\n{\\r\\n    var entityId = new EntityInstanceId(nameof(Counter), \"myCounter\");\\r\\n\\r\\n    // Two-way call to the entity which returns a value - awaits the response\\r\\n    int currentValue = await context.Entities.CallEntityAsync<int>(entityId, \"Get\");\\r\\n\\r\\n    if (currentValue < 10)\\r\\n    {\\r\\n        // One-way signal to the entity which updates the value - does not await a response\\r\\n        await context.Entities.SignalEntityAsync(entityId, \"Add\", 1);\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nOnly orchestrations are capable of calling entities and getting a response, which could be either a return value or an exception. Client functions that use the [client binding](durable-functions-bindings#entity-client) can only signal entities.\\r\\n\\r\\nNote\\r\\n\\r\\nCalling an entity from an orchestrator function is similar to calling an [activity function](durable-functions-types-features-overview#activity-functions) from an orchestrator function. The main difference is that entity functions are durable objects with an address, which is the entity ID. Entity functions support specifying an operation name. Activity functions, on the other hand, are stateless and don\\'t have the concept of operations.\\r\\n\\r\\n### Example: Entity signals an entity\\r\\n\\r\\nAn entity function can send signals to other entities, or even itself, while it executes an operation. For example, we can modify the previous `Counter` entity example so that it sends a \"milestone-reached\" signal to some monitor entity when the counter reaches the value 100.\\r\\n\\r\\n**In-process**\\r\\n\\r\\n```csharp\\r\\n   case \"add\":\\r\\n        var currentValue = ctx.GetState<int>();\\r\\n        var amount = ctx.GetInput<int>();\\r\\n        if (currentValue < 100 && currentValue + amount >= 100)\\r\\n        {\\r\\n            ctx.SignalEntity(new EntityId(\"MonitorEntity\", \"\"), \"milestone-reached\", ctx.EntityKey);\\r\\n        }\\r\\n\\r\\n        ctx.SetState(currentValue + amount);\\r\\n        break;\\r\\n```\\r\\n\\r\\n**Isolated worker process**\\r\\n\\r\\n```csharp\\r\\ncase \"add\":\\r\\n    var currentValue = operation.State.GetState<int>();\\r\\n    var amount = operation.GetInput<int>();\\r\\n    if (currentValue < 100 && currentValue + amount >= 100)\\r\\n    {\\r\\n        operation.Context.SignalEntity(new EntityInstanceId(\"MonitorEntity\", \"\"), \"milestone-reached\", operation.Context.EntityInstanceId);\\r\\n    }\\r\\n\\r\\n    operation.State.SetState(currentValue + amount);\\r\\n    break;\\r\\n```\\r\\n\\r\\n## Entity coordination\\r\\n\\r\\nThere might be times when you need to coordinate operations across multiple entities. For example, in a banking application, you might have entities that represent individual bank accounts. When you transfer funds from one account to another, you must ensure that the source account has sufficient funds. You also must ensure that updates to both the source and destination accounts are done in a transactionally consistent way.\\r\\n\\r\\n### Example: Transfer funds\\r\\n\\r\\nThe following example code transfers funds between two account entities by using an orchestrator function. Coordinating entity updates requires using the `LockAsync` method to create a *critical section* in the orchestration.\\r\\n\\r\\nNote\\r\\n\\r\\nFor simplicity, this example reuses the `Counter` entity defined previously. In a real application, it would be better to define a more detailed `BankAccount` entity.\\r\\n\\r\\n```csharp\\r\\n// This is a method called by an orchestrator function\\r\\npublic static async Task<bool> TransferFundsAsync(\\r\\n    string sourceId,\\r\\n    string destinationId,\\r\\n    int transferAmount,\\r\\n    IDurableOrchestrationContext context)\\r\\n{\\r\\n    var sourceEntity = new EntityId(nameof(Counter), sourceId);\\r\\n    var destinationEntity = new EntityId(nameof(Counter), destinationId);\\r\\n\\r\\n    // Create a critical section to avoid race conditions.\\r\\n    // No operations can be performed on either the source or\\r\\n    // destination accounts until the locks are released.\\r\\n    using (await context.LockAsync(sourceEntity, destinationEntity))\\r\\n    {\\r\\n        ICounter sourceProxy = \\r\\n            context.CreateEntityProxy<ICounter>(sourceEntity);\\r\\n        ICounter destinationProxy =\\r\\n            context.CreateEntityProxy<ICounter>(destinationEntity);\\r\\n\\r\\n        int sourceBalance = await sourceProxy.Get();\\r\\n\\r\\n        if (sourceBalance >= transferAmount)\\r\\n        {\\r\\n            await sourceProxy.Add(-transferAmount);\\r\\n            await destinationProxy.Add(transferAmount);\\r\\n\\r\\n            // the transfer succeeded\\r\\n            return true;\\r\\n        }\\r\\n        else\\r\\n        {\\r\\n            // the transfer failed due to insufficient funds\\r\\n            return false;\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nIn .NET, `LockAsync` returns `IDisposable`, which ends the critical section when disposed. This `IDisposable` result can be used together with a `using` block to get a syntactic representation of the critical section.\\r\\n\\r\\nIn the preceding example, an orchestrator function transfers funds from a source entity to a destination entity. The `LockAsync` method locked both the source and destination account entities. This locking ensured that no other client could query or modify the state of either account until the orchestration logic exited the critical section at the end of the `using` statement. This behavior prevents the possibility of overdrafting from the source account.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen an orchestration terminates, either normally or with an error, any critical sections in progress are implicitly ended and all locks are released.\\r\\n\\r\\n### Critical section behavior\\r\\n\\r\\nThe `LockAsync` method creates a critical section in an orchestration. These critical sections prevent other orchestrations from making overlapping changes to a specified set of entities. Internally, the `LockAsync` API sends \"lock\" operations to the entities and returns when it receives a \"lock acquired\" response message from each of these same entities. Both lock and unlock are built-in operations supported by all entities.\\r\\n\\r\\nNo operations from other clients are allowed on an entity while it\\'s in a locked state. This behavior ensures that only one orchestration instance can lock an entity at a time. If a caller tries to invoke an operation on an entity while it\\'s locked by an orchestration, that operation is placed in a pending operation queue. No pending operations are processed until after the holding orchestration releases its lock.\\r\\n\\r\\nNote\\r\\n\\r\\nThis behavior is slightly different from synchronization primitives used in most programming languages, such as the `lock` statement in C#. For example, in C#, the `lock` statement must be used by all threads to ensure proper synchronization across multiple threads. Entities, however, don\\'t require all callers to explicitly lock an entity. If any caller locks an entity, all other operations on that entity are blocked and queued behind that lock.\\r\\n\\r\\nLocks on entities are durable, so they persist even if the executing process is recycled. Locks are internally persisted as part of an entity\\'s durable state.\\r\\n\\r\\nUnlike transactions, critical sections don\\'t automatically roll back changes when errors occur. Instead, any error handling, such as roll-back or retry, must be explicitly coded, for example by catching errors or exceptions. This design choice is intentional. Automatically rolling back all the effects of an orchestration is difficult or impossible in general, because orchestrations might run activities and make calls to external services that can\\'t be rolled back. Also, attempts to roll back might themselves fail and require further error handling.\\r\\n\\r\\n### Critical section rules\\r\\n\\r\\nUnlike low-level locking primitives in most programming languages, critical sections are *guaranteed not to deadlock*. To prevent deadlocks, we enforce the following restrictions:\\r\\n\\r\\n- Critical sections can\\'t be nested.\\r\\n- Critical sections can\\'t create suborchestrations.\\r\\n- Critical sections can call only entities they have locked.\\r\\n- Critical sections can\\'t call the same entity using multiple parallel calls.\\r\\n- Critical sections can signal only entities they haven\\'t locked.\\r\\n\\r\\nAny violations of these rules cause a runtime error, such as `LockingRulesViolationException` in .NET, which includes a message that explains what rule was broken.\\r\\n\\r\\n## Comparison with virtual actors\\r\\n\\r\\nMany of the durable entities features are inspired by the [actor model](https://en.wikipedia.org/wiki/Actor_model). If you\\'re already familiar with actors, you might recognize many of the concepts described in this article. Durable entities are similar to [virtual actors](https://research.microsoft.com/projects/orleans/), or grains, as popularized by the [Orleans project](http://dotnet.github.io/orleans/). For example:\\r\\n\\r\\n- Durable entities are addressable via an entity ID.\\r\\n- Durable entity operations execute serially, one at a time, to prevent race conditions.\\r\\n- Durable entities are created implicitly when they\\'re called or signaled.\\r\\n- Durable entities are silently unloaded from memory when not executing operations.\\r\\n\\r\\nThere are some important differences that are worth noting:\\r\\n\\r\\n- Durable entities prioritize durability over latency, and so might not be appropriate for applications with strict latency requirements.\\r\\n- Durable entities don\\'t have built-in timeouts for messages. In Orleans, all messages time out after a configurable time. The default is 30 seconds.\\r\\n- Messages sent between entities are delivered reliably and in order. In Orleans, reliable or ordered delivery is supported for content sent through streams, but isn\\'t guaranteed for all messages between grains.\\r\\n- Request-response patterns in entities are limited to orchestrations. From within entities, only one-way messaging (also known as signaling) is permitted, as in the original actor model, and unlike grains in Orleans.\\r\\n- Durable entities don\\'t deadlock. In Orleans, deadlocks can occur and don\\'t resolve until messages time out.\\r\\n- Durable entities can be used with durable orchestrations and support distributed locking mechanisms.\\r\\n\\r\\n# Entity functions (javascript)\\r\\n\\r\\nEntity functions define operations for reading and updating small pieces of state, known as *durable entities*. Like orchestrator functions, entity functions are functions with a special trigger type, the *entity trigger*. Unlike orchestrator functions, entity functions manage the state of an entity explicitly, rather than implicitly representing state via control flow. Entities provide a means for scaling out applications by distributing the work across many entities, each with a modestly sized state.\\r\\n\\r\\nNote\\r\\n\\r\\nEntity functions and related functionality are only available in [Durable Functions 2.0](durable-functions-versions#migrate-from-1x-to-2x) and above. They are currently supported in .NET in-proc, .NET isolated worker, JavaScript, and Python, but not in PowerShell or Java. Furthermore, entity functions for .NET Isolated are supported when using the Azure Storage or Netherite state providers, but not when using the MSSQL state provider.\\r\\n\\r\\n## General concepts\\r\\n\\r\\nEntities behave a bit like tiny services that communicate via messages. Each entity has a unique identity and an internal state (if it exists). Like services or objects, entities perform operations when prompted to do so. When an operation executes, it might update the internal state of the entity. It might also call external services and wait for a response. Entities communicate with other entities, orchestrations, and clients by using messages that are implicitly sent via reliable queues.\\r\\n\\r\\nTo prevent conflicts, all operations on a single entity are guaranteed to execute serially, that is, one after another.\\r\\n\\r\\nNote\\r\\n\\r\\nWhen an entity is invoked, it processes its payload to completion and then schedules a new execution to activate once future inputs arrive. As a result, your entity execution logs might show an extra execution after each entity invocation; this is expected.\\r\\n\\r\\n### Entity ID\\r\\n\\r\\nEntities are accessed via a unique identifier, the *entity ID*. An entity ID is simply a pair of strings that uniquely identifies an entity instance. It consists of an:\\r\\n\\r\\n- **Entity name**, which is a name that identifies the type of the entity. An example is \"Counter.\" This name must match the name of the entity function that implements the entity. It isn\\'t sensitive to case.\\r\\n- **Entity key**, which is a string that uniquely identifies the entity among all other entities of the same name. An example is a GUID.\\r\\n\\r\\nFor example, a `Counter` entity function might be used for keeping score in an online game. Each instance of the game has a unique entity ID, such as `@Counter@Game1` and `@Counter@Game2`. All operations that target a particular entity require specifying an entity ID as a parameter.\\r\\n\\r\\n### Entity operations\\r\\n\\r\\nTo invoke an operation on an entity, specify the:\\r\\n\\r\\n- **Entity ID** of the target entity.\\r\\n- **Operation name**, which is a string that specifies the operation to perform. For example, the `Counter` entity could support `add`, `get`, or `reset` operations.\\r\\n- **Operation input**, which is an optional input parameter for the operation. For example, the add operation can take an integer amount as the input.\\r\\n- **Scheduled time**, which is an optional parameter for specifying the delivery time of the operation. For example, an operation can be reliably scheduled to run several days in the future.\\r\\n\\r\\nOperations can return a result value or an error result, such as a JavaScript error or a .NET exception. This result or error occurs in orchestrations that called the operation.\\r\\n\\r\\nAn entity operation can also create, read, update, and delete the state of the entity. The state of the entity is always durably persisted in storage.\\r\\n\\r\\n## Define entities\\r\\n\\r\\nYou define entities using a function-based syntax, where entities are represented as functions and operations are explicitly dispatched by the application.\\r\\n\\r\\nDurable entities are available in JavaScript starting with version **1.3.0** of the `durable-functions` npm package. The following code is the `Counter` entity implemented as a durable function written in JavaScript.\\r\\n\\r\\n**Counter/function.json**\\r\\n\\r\\n```json\\r\\n{\\r\\n  \"bindings\": [\\r\\n    {\\r\\n      \"name\": \"context\",\\r\\n      \"type\": \"entityTrigger\",\\r\\n      \"direction\": \"in\"\\r\\n    }\\r\\n  ],\\r\\n  \"disabled\": false\\r\\n}\\r\\n```\\r\\n\\r\\n**Counter/index.js**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.entity(function(context) {\\r\\n    const currentValue = context.df.getState(() => 0);\\r\\n    switch (context.df.operationName) {\\r\\n        case \"add\":\\r\\n            const amount = context.df.getInput();\\r\\n            context.df.setState(currentValue + amount);\\r\\n            break;\\r\\n        case \"reset\":\\r\\n            context.df.setState(0);\\r\\n            break;\\r\\n        case \"get\":\\r\\n            context.df.return(currentValue);\\r\\n            break;\\r\\n    }\\r\\n});\\r\\n```\\r\\n\\r\\n## Access entities\\r\\n\\r\\nEntities can be accessed using one-way or two-way communication. The following terminology distinguishes the two forms of communication:\\r\\n\\r\\n- **Calling** an entity uses two-way (round-trip) communication. You send an operation message to the entity, and then wait for the response message before you continue. The response message can provide a result value or an error result, such as a JavaScript error or a .NET exception. This result or error is then observed by the caller.\\r\\n- **Signaling** an entity uses one-way (fire and forget) communication. You send an operation message but don\\'t wait for a response. While the message is guaranteed to be delivered eventually, the sender doesn\\'t know when and can\\'t observe any result value or errors.\\r\\n\\r\\nEntities can be accessed from within client functions, from within orchestrator functions, or from within entity functions. Not all forms of communication are supported by all contexts:\\r\\n\\r\\n- From within clients, you can signal entities and you can read the entity state.\\r\\n- From within orchestrations, you can signal entities and you can call entities.\\r\\n- From within entities, you can signal entities.\\r\\n\\r\\nThe following examples illustrate these various ways of accessing entities.\\r\\n\\r\\n### Example: Client signals an entity\\r\\n\\r\\nTo access entities from an ordinary Azure Function, which is also known as a client function, use the [entity client binding](durable-functions-bindings#entity-client). The following example shows a queue-triggered function signaling an entity using this binding.\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = async function (context) {\\r\\n    const client = df.getClient(context);\\r\\n    const entityId = new df.EntityId(\"Counter\", \"myCounter\");\\r\\n    await client.signalEntity(entityId, \"add\", 1);\\r\\n};\\r\\n```\\r\\n\\r\\nThe term *signal* means that the entity API invocation is one-way and asynchronous. It\\'s not possible for a client function to know when the entity has processed the operation. Also, the client function can\\'t observe any result values or exceptions.\\r\\n\\r\\n### Example: Client reads an entity state\\r\\n\\r\\nClient functions can also query the state of an entity, as shown in the following example:\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = async function (context) {\\r\\n    const client = df.getClient(context);\\r\\n    const entityId = new df.EntityId(\"Counter\", \"myCounter\");\\r\\n    const stateResponse = await client.readEntityState(entityId);\\r\\n    return stateResponse.entityState;\\r\\n};\\r\\n```\\r\\n\\r\\nEntity state queries are sent to the Durable tracking store and return the entity\\'s most recently persisted state. This state is always a \"committed\" state, that is, it\\'s never a temporary intermediate state assumed in the middle of executing an operation. However, it\\'s possible that this state is stale compared to the entity\\'s in-memory state. Only orchestrations can read an entity\\'s in-memory state, as described in the following section.\\r\\n\\r\\n### Example: Orchestration signals and calls an entity\\r\\n\\r\\nOrchestrator functions can access entities by using APIs on the [orchestration trigger binding](durable-functions-bindings#orchestration-trigger). The following example code shows an orchestrator function calling and signaling a `Counter` entity.\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context){\\r\\n    const entityId = new df.EntityId(\"Counter\", \"myCounter\");\\r\\n\\r\\n    // Two-way call to the entity which returns a value - awaits the response\\r\\n    currentValue = yield context.df.callEntity(entityId, \"get\");\\r\\n});\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nJavaScript does not currently support signaling an entity from an orchestrator. Use `callEntity` instead.\\r\\n\\r\\nOnly orchestrations are capable of calling entities and getting a response, which could be either a return value or an exception. Client functions that use the [client binding](durable-functions-bindings#entity-client) can only signal entities.\\r\\n\\r\\nNote\\r\\n\\r\\nCalling an entity from an orchestrator function is similar to calling an [activity function](durable-functions-types-features-overview#activity-functions) from an orchestrator function. The main difference is that entity functions are durable objects with an address, which is the entity ID. Entity functions support specifying an operation name. Activity functions, on the other hand, are stateless and don\\'t have the concept of operations.\\r\\n\\r\\n### Example: Entity signals an entity\\r\\n\\r\\nAn entity function can send signals to other entities, or even itself, while it executes an operation. For example, we can modify the previous `Counter` entity example so that it sends a \"milestone-reached\" signal to some monitor entity when the counter reaches the value 100.\\r\\n\\r\\n```javascript\\r\\n    case \"add\":\\r\\n        const amount = context.df.getInput();\\r\\n        if (currentValue < 100 && currentValue + amount >= 100) {\\r\\n            const entityId = new df.EntityId(\"MonitorEntity\", \"\");\\r\\n            context.df.signalEntity(entityId, \"milestone-reached\", context.df.instanceId);\\r\\n        }\\r\\n        context.df.setState(currentValue + amount);\\r\\n        break;\\r\\n```\\r\\n\\r\\n## Comparison with virtual actors\\r\\n\\r\\nMany of the durable entities features are inspired by the [actor model](https://en.wikipedia.org/wiki/Actor_model). If you\\'re already familiar with actors, you might recognize many of the concepts described in this article. Durable entities are similar to [virtual actors](https://research.microsoft.com/projects/orleans/), or grains, as popularized by the [Orleans project](http://dotnet.github.io/orleans/). For example:\\r\\n\\r\\n- Durable entities are addressable via an entity ID.\\r\\n- Durable entity operations execute serially, one at a time, to prevent race conditions.\\r\\n- Durable entities are created implicitly when they\\'re called or signaled.\\r\\n- Durable entities are silently unloaded from memory when not executing operations.\\r\\n\\r\\nThere are some important differences that are worth noting:\\r\\n\\r\\n- Durable entities prioritize durability over latency, and so might not be appropriate for applications with strict latency requirements.\\r\\n- Durable entities don\\'t have built-in timeouts for messages. In Orleans, all messages time out after a configurable time. The default is 30 seconds.\\r\\n- Messages sent between entities are delivered reliably and in order. In Orleans, reliable or ordered delivery is supported for content sent through streams, but isn\\'t guaranteed for all messages between grains.\\r\\n- Request-response patterns in entities are limited to orchestrations. From within entities, only one-way messaging (also known as signaling) is permitted, as in the original actor model, and unlike grains in Orleans.\\r\\n- Durable entities don\\'t deadlock. In Orleans, deadlocks can occur and don\\'t resolve until messages time out.\\r\\n- Durable entities can be used with durable orchestrations and support distributed locking mechanisms.\\r\\n\\r\\n# Entity functions (python)\\r\\n\\r\\nEntity functions define operations for reading and updating small pieces of state, known as *durable entities*. Like orchestrator functions, entity functions are functions with a special trigger type, the *entity trigger*. Unlike orchestrator functions, entity functions manage the state of an entity explicitly, rather than implicitly representing state via control flow. Entities provide a means for scaling out applications by distributing the work across many entities, each with a modestly sized state.\\r\\n\\r\\nNote\\r\\n\\r\\nEntity functions and related functionality are only available in [Durable Functions 2.0](durable-functions-versions#migrate-from-1x-to-2x) and above. They are currently supported in .NET in-proc, .NET isolated worker, JavaScript, and Python, but not in PowerShell or Java. Furthermore, entity functions for .NET Isolated are supported when using the Azure Storage or Netherite state providers, but not when using the MSSQL state provider.\\r\\n\\r\\n## General concepts\\r\\n\\r\\nEntities behave a bit like tiny services that communicate via messages. Each entity has a unique identity and an internal state (if it exists). Like services or objects, entities perform operations when prompted to do so. When an operation executes, it might update the internal state of the entity. It might also call external services and wait for a response. Entities communicate with other entities, orchestrations,',\n",
       "  'metadata': {}},\n",
       " {'_id': 'c61a3bad-8095-3bb1-7d3f-97416a464010',\n",
       "  'title': 'Handling errors in Durable Functions - Azure',\n",
       "  'text': '# Handling errors in Durable Functions (Azure Functions)\\r\\n\\r\\nDurable Function orchestrations are implemented in code and can use the programming language\\'s built-in error-handling features. There really aren\\'t any new concepts you need to learn to add error handling and compensation into your orchestrations. However, there are a few behaviors that you should be aware of.\\r\\n\\r\\nNote\\r\\n\\r\\nVersion 4 of the Node.js programming model for Azure Functions is generally available. The new v4 model is designed to have a more flexible and intuitive experience for JavaScript and TypeScript developers. Learn more about the differences between v3 and v4 in the [migration guide](../functions-node-upgrade-v4).\\r\\n\\r\\nIn the following code snippets, JavaScript (PM4) denotes programming model V4, the new experience.\\r\\n\\r\\n## Errors in activity functions\\r\\n\\r\\nAny exception that is thrown in an activity function is marshaled back to the orchestrator function and thrown as a `FunctionFailedException`. You can write error handling and compensation code that suits your needs in the orchestrator function.\\r\\n\\r\\nFor example, consider the following orchestrator function that transfers funds from one account to another:\\r\\n\\r\\n**C# (InProc)**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"TransferFunds\")]\\r\\npublic static async Task Run([OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    var transferDetails = context.GetInput<TransferOperation>();\\r\\n\\r\\n    await context.CallActivityAsync(\"DebitAccount\",\\r\\n        new\\r\\n        {\\r\\n            Account = transferDetails.SourceAccount,\\r\\n            Amount = transferDetails.Amount\\r\\n        });\\r\\n\\r\\n    try\\r\\n    {\\r\\n        await context.CallActivityAsync(\"CreditAccount\",\\r\\n            new\\r\\n            {\\r\\n                Account = transferDetails.DestinationAccount,\\r\\n                Amount = transferDetails.Amount\\r\\n            });\\r\\n    }\\r\\n    catch (Exception)\\r\\n    {\\r\\n        // Refund the source account.\\r\\n        // Another try/catch could be used here based on the needs of the application.\\r\\n        await context.CallActivityAsync(\"CreditAccount\",\\r\\n            new\\r\\n            {\\r\\n                Account = transferDetails.SourceAccount,\\r\\n                Amount = transferDetails.Amount\\r\\n            });\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous C# examples are for Durable Functions 2.x. For Durable Functions 1.x, you must use `DurableOrchestrationContext` instead of `IDurableOrchestrationContext`. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n**C# (Isolated)**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"TransferFunds\")]\\r\\npublic static async Task Run(\\r\\n    [OrchestrationTrigger] TaskOrchestrationContext context, TransferOperation transferDetails)\\r\\n{\\r\\n    await context.CallActivityAsync(\"DebitAccount\",\\r\\n        new\\r\\n        {\\r\\n            Account = transferDetails.SourceAccount,\\r\\n            Amount = transferDetails.Amount\\r\\n        });\\r\\n\\r\\n    try\\r\\n    {\\r\\n        await context.CallActivityAsync(\"CreditAccount\",\\r\\n            new\\r\\n            {\\r\\n                Account = transferDetails.DestinationAccount,\\r\\n                Amount = transferDetails.Amount\\r\\n            });\\r\\n    }\\r\\n    catch (Exception)\\r\\n    {\\r\\n        // Refund the source account.\\r\\n        // Another try/catch could be used here based on the needs of the application.\\r\\n        await context.CallActivityAsync(\"CreditAccount\",\\r\\n            new\\r\\n            {\\r\\n                Account = transferDetails.SourceAccount,\\r\\n                Amount = transferDetails.Amount\\r\\n            });\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript (PM3)**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function* (context) {\\r\\n    const transferDetails = context.df.getInput();\\r\\n\\r\\n    yield context.df.callActivity(\"DebitAccount\", {\\r\\n        account: transferDetails.sourceAccount,\\r\\n        amount: transferDetails.amount,\\r\\n    });\\r\\n\\r\\n    try {\\r\\n        yield context.df.callActivity(\"CreditAccount\", {\\r\\n            account: transferDetails.destinationAccount,\\r\\n            amount: transferDetails.amount,\\r\\n        });\\r\\n    } catch (error) {\\r\\n        // Refund the source account.\\r\\n        // Another try/catch could be used here based on the needs of the application.\\r\\n        yield context.df.callActivity(\"CreditAccount\", {\\r\\n            account: transferDetails.sourceAccount,\\r\\n            amount: transferDetails.amount,\\r\\n        });\\r\\n    }\\r\\n})\\r\\n```\\r\\n\\r\\n**JavaScript (PM4)**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\ndf.app.orchestration(\"transferFunds\", function* (context) {\\r\\n    const transferDetails = context.df.getInput();\\r\\n\\r\\n    yield context.df.callActivity(\"debitAccount\", {\\r\\n        account: transferDetails.sourceAccount,\\r\\n        amount: transferDetails.amount,\\r\\n    });\\r\\n\\r\\n    try {\\r\\n        yield context.df.callActivity(\"creditAccount\", {\\r\\n            account: transferDetails.destinationAccount,\\r\\n            amount: transferDetails.amount,\\r\\n        });\\r\\n    } catch (error) {\\r\\n        // Refund the source account.\\r\\n        // Another try/catch could be used here based on the needs of the application.\\r\\n        yield context.df.callActivity(\"creditAccount\", {\\r\\n            account: transferDetails.sourceAccount,\\r\\n            amount: transferDetails.amount,\\r\\n        });\\r\\n    }\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    transfer_details = context.get_input()\\r\\n\\r\\n    yield context.call_activity(\\'DebitAccount\\', {\\r\\n         \\'account\\': transfer_details[\\'sourceAccount\\'],\\r\\n         \\'amount\\' : transfer_details[\\'amount\\']\\r\\n    })\\r\\n\\r\\n    try:\\r\\n        yield context.call_activity(\\'CreditAccount\\', {\\r\\n                \\'account\\': transfer_details[\\'destinationAccount\\'],\\r\\n                \\'amount\\': transfer_details[\\'amount\\'],\\r\\n            })\\r\\n    except:\\r\\n        yield context.call_activity(\\'CreditAccount\\', {\\r\\n            \\'account\\': transfer_details[\\'sourceAccount\\'],\\r\\n            \\'amount\\': transfer_details[\\'amount\\']\\r\\n        })\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\nBy default, cmdlets in PowerShell do not raise exceptions that can be caught using try/catch blocks. You have two options for changing this behavior:\\r\\n\\r\\n1. Use the `-ErrorAction Stop` flag when invoking cmdlets, such as `Invoke-DurableActivity`.\\r\\n2. Set the [`$ErrorActionPreference`](/en-us/powershell/module/microsoft.powershell.core/about/about_preference_variables#erroractionpreference) preference variable to `\"Stop\"` in the orchestrator function before invoking cmdlets.\\r\\n\\r\\n```powershell\\r\\nparam($Context)\\r\\n\\r\\n$ErrorActionPreference = \"Stop\"\\r\\n\\r\\n$transferDetails = $Context.Input\\r\\n\\r\\nInvoke-DurableActivity -FunctionName \\'DebitAccount\\' -Input @{ account = transferDetails.sourceAccount; amount = transferDetails.amount }\\r\\n\\r\\ntry {\\r\\n    Invoke-DurableActivity -FunctionName \\'CreditAccount\\' -Input @{ account = transferDetails.destinationAccount; amount = transferDetails.amount }\\r\\n} catch {\\r\\n    Invoke-DurableActivity -FunctionName \\'CreditAccount\\' -Input @{ account = transferDetails.sourceAccount; amount = transferDetails.amount }\\r\\n}\\r\\n```\\r\\n\\r\\nFor more information on error handling in PowerShell, see the [Try-Catch-Finally](/en-us/powershell/module/microsoft.powershell.core/about/about_try_catch_finally) PowerShell documentation.\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"TransferFunds\")\\r\\npublic void transferFunds(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    TransferOperation transfer = ctx.getInput(TransferOperation.class);\\r\\n    ctx.callActivity(\\r\\n        \"DebitAccount\", \\r\\n        new OperationArgs(transfer.sourceAccount, transfer.amount)).await();\\r\\n    try {\\r\\n        ctx.callActivity(\\r\\n            \"CreditAccount\", \\r\\n            new OperationArgs(transfer.destinationAccount, transfer.amount)).await();\\r\\n    } catch (TaskFailedException ex) {\\r\\n        // Refund the source account on failure\\r\\n        ctx.callActivity(\\r\\n            \"CreditAccount\", \\r\\n            new OperationArgs(transfer.sourceAccount, transfer.amount)).await();\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nIf the first **CreditAccount** function call fails, the orchestrator function compensates by crediting the funds back to the source account.\\r\\n\\r\\n## Automatic retry on failure\\r\\n\\r\\nWhen you call activity functions or sub-orchestration functions, you can specify an automatic retry policy. The following example attempts to call a function up to three times and waits 5 seconds between each retry:\\r\\n\\r\\n**C# (InProc)**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"TimerOrchestratorWithRetry\")]\\r\\npublic static async Task Run([OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    var retryOptions = new RetryOptions(\\r\\n        firstRetryInterval: TimeSpan.FromSeconds(5),\\r\\n        maxNumberOfAttempts: 3);\\r\\n\\r\\n    await context.CallActivityWithRetryAsync(\"FlakyFunction\", retryOptions, null);\\r\\n\\r\\n    // ...\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous C# examples are for Durable Functions 2.x. For Durable Functions 1.x, you must use `DurableOrchestrationContext` instead of `IDurableOrchestrationContext`. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n**C# (Isolated)**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"TimerOrchestratorWithRetry\")]\\r\\npublic static async Task Run([OrchestrationTrigger] TaskOrchestrationContext context)\\r\\n{\\r\\n    var options = TaskOptions.FromRetryPolicy(new RetryPolicy(\\r\\n        maxNumberOfAttempts: 3,\\r\\n        firstRetryInterval: TimeSpan.FromSeconds(5)));\\r\\n\\r\\n    await context.CallActivityAsync(\"FlakyFunction\", options: options);\\r\\n\\r\\n    // ...\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript (PM3)**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context) {\\r\\n    const firstRetryIntervalInMilliseconds = 5000;\\r\\n    const maxNumberOfAttempts = 3;\\r\\n\\r\\n    const retryOptions = \\r\\n        new df.RetryOptions(firstRetryIntervalInMilliseconds, maxNumberOfAttempts);\\r\\n\\r\\n    yield context.df.callActivityWithRetry(\"FlakyFunction\", retryOptions);\\r\\n\\r\\n    // ...\\r\\n});\\r\\n```\\r\\n\\r\\n**JavaScript (PM4)**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\ndf.app.orchestration(\"callActivityWithRetry\", function* (context) {\\r\\n    const firstRetryIntervalInMilliseconds = 5000;\\r\\n    const maxNumberOfAttempts = 3;\\r\\n\\r\\n    const retryOptions = new df.RetryOptions(firstRetryIntervalInMilliseconds, maxNumberOfAttempts);\\r\\n\\r\\n    yield context.df.callActivityWithRetry(\"flakyFunction\", retryOptions);\\r\\n\\r\\n    // ...\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    first_retry_interval_in_milliseconds = 5000\\r\\n    max_number_of_attempts = 3\\r\\n\\r\\n    retry_options = df.RetryOptions(first_retry_interval_in_milliseconds, max_number_of_attempts)\\r\\n\\r\\n    yield context.call_activity_with_retry(\\'FlakyFunction\\', retry_options)\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n```powershell\\r\\nparam($Context)\\r\\n\\r\\n$retryOptions = New-DurableRetryOptions `\\r\\n                    -FirstRetryInterval (New-TimeSpan -Seconds 5) `\\r\\n                    -MaxNumberOfAttempts 3\\r\\n\\r\\nInvoke-DurableActivity -FunctionName \\'FlakyFunction\\' -RetryOptions $retryOptions\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"TimerOrchestratorWithRetry\")\\r\\npublic void timerOrchestratorWithRetry(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    final int maxAttempts = 3;\\r\\n    final Duration firstRetryInterval = Duration.ofSeconds(5);\\r\\n    RetryPolicy policy = new RetryPolicy(maxAttempts, firstRetryInterval);\\r\\n    TaskOptions options = new TaskOptions(policy);\\r\\n    ctx.callActivity(\"FlakeyFunction\", options).await();\\r\\n    // ...\\r\\n}\\r\\n```\\r\\n\\r\\nThe activity function call in the previous example takes a parameter for configuring an automatic retry policy. There are several options for customizing the automatic retry policy:\\r\\n\\r\\n- **Max number of attempts**: The maximum number of attempts. If set to 1, there will be no retry.\\r\\n- **First retry interval**: The amount of time to wait before the first retry attempt.\\r\\n- **Backoff coefficient**: The coefficient used to determine rate of increase of backoff. Defaults to 1.\\r\\n- **Max retry interval**: The maximum amount of time to wait in between retry attempts.\\r\\n- **Retry timeout**: The maximum amount of time to spend doing retries. The default behavior is to retry indefinitely.\\r\\n\\r\\n## Custom retry handlers\\r\\n\\r\\nWhen using the .NET or Java, you also have the option to implement retry handlers in code. This is useful when declarative retry policies are not expressive enough. For languages that don\\'t support custom retry handlers, you still have the option of implementing retry policies using loops, exception handling, and timers for injecting delays between retries.\\r\\n\\r\\n**C# (InProc)**\\r\\n\\r\\n```csharp\\r\\nRetryOptions retryOptions = new RetryOptions(\\r\\n    firstRetryInterval: TimeSpan.FromSeconds(5),\\r\\n    maxNumberOfAttempts: int.MaxValue)\\r\\n    {\\r\\n        Handle = exception =>\\r\\n        {\\r\\n            // True to handle and try again, false to not handle and throw.\\r\\n            if (exception is TaskFailedException failure)\\r\\n            {\\r\\n                // Exceptions from TaskActivities are always this type. Inspect the\\r\\n                // inner Exception to get more details.\\r\\n            }\\r\\n\\r\\n            return false;\\r\\n        };\\r\\n    }\\r\\n\\r\\nawait ctx.CallActivityWithRetryAsync(\"FlakeyActivity\", retryOptions, null);\\r\\n```\\r\\n\\r\\n**C# (Isolated)**\\r\\n\\r\\n```csharp\\r\\nTaskOptions retryOptions = TaskOptions.FromRetryHandler(retryContext =>\\r\\n{\\r\\n    // Don\\'t retry anything that derives from ApplicationException\\r\\n    if (retryContext.LastFailure.IsCausedBy<ApplicationException>())\\r\\n    {\\r\\n        return false;\\r\\n    }\\r\\n\\r\\n    // Quit after N attempts\\r\\n    return retryContext.LastAttemptNumber < 3;\\r\\n});\\r\\n\\r\\ntry\\r\\n{\\r\\n    await ctx.CallActivityAsync(\"FlakeyActivity\", options: retryOptions);\\r\\n}\\r\\ncatch (TaskFailedException)\\r\\n{\\r\\n    // Case when the retry handler returns false...\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript (PM3)**\\r\\nJavaScript doesn\\'t currently support custom retry handlers. However, you still have the option of implementing retry logic directly in the orchestrator function using loops, exception handling, and timers for injecting delays between retries.\\r\\n\\r\\n**JavaScript (PM4)**\\r\\nJavaScript doesn\\'t currently support custom retry handlers. However, you still have the option of implementing retry logic directly in the orchestrator function using loops, exception handling, and timers for injecting delays between retries.\\r\\n\\r\\n**Python**\\r\\nPython doesn\\'t currently support custom retry handlers. However, you still have the option of implementing retry logic directly in the orchestrator function using loops, exception handling, and timers for injecting delays between retries.\\r\\n\\r\\n**PowerShell**\\r\\nPowerShell doesn\\'t currently support custom retry handlers. However, you still have the option of implementing retry logic directly in the orchestrator function using loops, exception handling, and timers for injecting delays between retries.\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\nRetryHandler retryHandler = retryCtx -> {\\r\\n    // Don\\'t retry anything that derives from RuntimeException\\r\\n    if (retryCtx.getLastFailure().isCausedBy(RuntimeException.class)) {\\r\\n        return false;\\r\\n    }\\r\\n\\r\\n    // Quit after N attempts\\r\\n    return retryCtx.getLastAttemptNumber() < 3;\\r\\n};\\r\\n\\r\\nTaskOptions options = new TaskOptions(retryHandler);\\r\\ntry {\\r\\n    ctx.callActivity(\"FlakeyActivity\", options).await();\\r\\n} catch (TaskFailedException ex) {\\r\\n    // Case when the retry handler returns false...\\r\\n}\\r\\n```\\r\\n\\r\\n## Function timeouts\\r\\n\\r\\nYou might want to abandon a function call within an orchestrator function if it\\'s taking too long to complete. The proper way to do this today is by creating a [durable timer](durable-functions-timers) with an \"any\" task selector, as in the following example:\\r\\n\\r\\n**C# (InProc)**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"TimerOrchestrator\")]\\r\\npublic static async Task<bool> Run([OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    TimeSpan timeout = TimeSpan.FromSeconds(30);\\r\\n    DateTime deadline = context.CurrentUtcDateTime.Add(timeout);\\r\\n\\r\\n    using (var cts = new CancellationTokenSource())\\r\\n    {\\r\\n        Task activityTask = context.CallActivityAsync(\"FlakyFunction\");\\r\\n        Task timeoutTask = context.CreateTimer(deadline, cts.Token);\\r\\n\\r\\n        Task winner = await Task.WhenAny(activityTask, timeoutTask);\\r\\n        if (winner == activityTask)\\r\\n        {\\r\\n            // success case\\r\\n            cts.Cancel();\\r\\n            return true;\\r\\n        }\\r\\n        else\\r\\n        {\\r\\n            // timeout case\\r\\n            return false;\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous C# examples are for Durable Functions 2.x. For Durable Functions 1.x, you must use `DurableOrchestrationContext` instead of `IDurableOrchestrationContext`. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n**C# (Isolated)**\\r\\n\\r\\n```csharp\\r\\n[Function(\"TimerOrchestrator\")]\\r\\npublic static async Task<bool> Run([OrchestrationTrigger] TaskOrchestrationContext context)\\r\\n{\\r\\n    TimeSpan timeout = TimeSpan.FromSeconds(30);\\r\\n    DateTime deadline = context.CurrentUtcDateTime.Add(timeout);\\r\\n\\r\\n    using (var cts = new CancellationTokenSource())\\r\\n    {\\r\\n        Task activityTask = context.CallActivityAsync(\"FlakyFunction\");\\r\\n        Task timeoutTask = context.CreateTimer(deadline, cts.Token);\\r\\n\\r\\n        Task winner = await Task.WhenAny(activityTask, timeoutTask);\\r\\n        if (winner == activityTask)\\r\\n        {\\r\\n            // success case\\r\\n            cts.Cancel();\\r\\n            return true;\\r\\n        }\\r\\n        else\\r\\n        {\\r\\n            // timeout case\\r\\n            return false;\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\n**JavaScript (PM3)**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\nconst moment = require(\"moment\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context) {\\r\\n    const deadline = moment.utc(context.df.currentUtcDateTime).add(30, \"s\");\\r\\n\\r\\n    const activityTask = context.df.callActivity(\"FlakyFunction\");\\r\\n    const timeoutTask = context.df.createTimer(deadline.toDate());\\r\\n\\r\\n    const winner = yield context.df.Task.any([activityTask, timeoutTask]);\\r\\n    if (winner === activityTask) {\\r\\n        // success case\\r\\n        timeoutTask.cancel();\\r\\n        return true;\\r\\n    } else {\\r\\n        // timeout case\\r\\n        return false;\\r\\n    }\\r\\n});\\r\\n```\\r\\n\\r\\n**JavaScript (PM4)**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\nconst { DateTime } = require(\"luxon\");\\r\\n\\r\\ndf.app.orchestration(\"timerOrchestrator\", function* (context) {\\r\\n    const deadline = DateTime.fromJSDate(context.df.currentUtcDateTime).plus({ seconds: 30 });\\r\\n\\r\\n    const activityTask = context.df.callActivity(\"flakyFunction\");\\r\\n    const timeoutTask = context.df.createTimer(deadline.toJSDate());\\r\\n\\r\\n    const winner = yield context.df.Task.any([activityTask, timeoutTask]);\\r\\n    if (winner === activityTask) {\\r\\n        // success case\\r\\n        timeoutTask.cancel();\\r\\n        return true;\\r\\n    } else {\\r\\n        // timeout case\\r\\n        return false;\\r\\n    }\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\nfrom datetime import datetime, timedelta\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    deadline = context.current_utc_datetime + timedelta(seconds = 30)\\r\\n    \\r\\n    activity_task = context.call_activity(\\'FlakyFunction\\')\\r\\n    timeout_task = context.create_timer(deadline)\\r\\n\\r\\n    winner = yield context.task_any(activity_task, timeout_task)\\r\\n    if winner == activity_task:\\r\\n        timeout_task.cancel()\\r\\n        return True\\r\\n    else:\\r\\n        return False\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\n\\r\\n```powershell\\r\\nparam($Context)\\r\\n\\r\\n$expiryTime = New-TimeSpan -Seconds 30\\r\\n\\r\\n$activityTask = Invoke-DurableActivity -FunctionName \\'FlakyFunction\\'-NoWait\\r\\n$timerTask = Start-DurableTimer -Duration $expiryTime -NoWait\\r\\n\\r\\n$winner = Wait-DurableTask -Task @($activityTask, $timerTask) -NoWait\\r\\n\\r\\nif ($winner -eq $activityTask) {\\r\\n    Stop-DurableTimerTask -Task $timerTask\\r\\n    return $True\\r\\n}\\r\\nelse {\\r\\n    return $False\\r\\n}\\r\\n```\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"TimerOrchestrator\")\\r\\npublic boolean timerOrchestrator(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    Task<Void> activityTask = ctx.callActivity(\"SlowFunction\");\\r\\n    Task<Void> timeoutTask = ctx.createTimer(Duration.ofMinutes(30));\\r\\n\\r\\n    Task<?> winner = ctx.anyOf(activityTask, timeoutTask).await();\\r\\n    if (winner == activityTask) {\\r\\n        // success case\\r\\n        return true;\\r\\n    } else {\\r\\n        // timeout case\\r\\n        return false;\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThis mechanism does not actually terminate in-progress activity function execution. Rather, it simply allows the orchestrator function to ignore the result and move on. For more information, see the [Timers](durable-functions-timers#usage-for-timeout) documentation.\\r\\n\\r\\n## Unhandled exceptions\\r\\n\\r\\nIf an orchestrator function fails with an unhandled exception, the details of the exception are logged and the instance completes with a `Failed` status.',\n",
       "  'metadata': {}},\n",
       " {'_id': '56da57d2-85a1-2dbc-c7bc-a9d848a93363',\n",
       "  'title': 'Eternal orchestrations in Durable Functions - Azure',\n",
       "  'text': '# Eternal orchestrations in Durable Functions (Azure Functions)\\r\\n\\r\\n*Eternal orchestrations* are orchestrator functions that never end. They are useful when you want to use [Durable Functions](durable-functions-overview) for aggregators and any scenario that requires an infinite loop.\\r\\n\\r\\n## Orchestration history\\r\\n\\r\\nAs explained in the [orchestration history](durable-functions-orchestrations#orchestration-history) topic, the Durable Task Framework keeps track of the history of each function orchestration. This history grows continuously as long as the orchestrator function continues to schedule new work. If the orchestrator function goes into an infinite loop and continuously schedules work, this history could grow critically large and cause significant performance problems. The *eternal orchestration* concept was designed to mitigate these kinds of problems for applications that need infinite loops.\\r\\n\\r\\n## Resetting and restarting\\r\\n\\r\\nInstead of using infinite loops, orchestrator functions reset their state by calling the *continue-as-new* method of the [orchestration trigger binding](durable-functions-bindings#orchestration-trigger). This method takes a JSON-serializable parameter, which becomes the new input for the next orchestrator function generation.\\r\\n\\r\\nWhen *continue-as-new* is called, the orchestration instance restarts itself with the new input value. The same instance ID is kept, but the orchestrator function\\'s history is reset.\\r\\n\\r\\nNote\\r\\n\\r\\nThe Durable Task Framework maintains the same instance ID but internally creates a new *execution ID* for the orchestrator function that gets reset by *continue-as-new*. This execution ID is not exposed externally, but it may be useful to know about when debugging orchestration execution.\\r\\n\\r\\n## Periodic work example\\r\\n\\r\\nOne use case for eternal orchestrations is code that needs to do periodic work indefinitely.\\r\\n\\r\\n**C#**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"Periodic_Cleanup_Loop\")]\\r\\npublic static async Task Run(\\r\\n    [OrchestrationTrigger] IDurableOrchestrationContext context)\\r\\n{\\r\\n    await context.CallActivityAsync(\"DoCleanup\", null);\\r\\n\\r\\n    // sleep for one hour between cleanups\\r\\n    DateTime nextCleanup = context.CurrentUtcDateTime.AddHours(1);\\r\\n    await context.CreateTimer(nextCleanup, CancellationToken.None);\\r\\n\\r\\n    context.ContinueAsNew(null);\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous C# example is for Durable Functions 2.x. For Durable Functions 1.x, you must use `DurableOrchestrationContext` instead of `IDurableOrchestrationContext`. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\nconst moment = require(\"moment\");\\r\\n\\r\\nmodule.exports = df.orchestrator(function*(context) {\\r\\n    yield context.df.callActivity(\"DoCleanup\");\\r\\n\\r\\n    // sleep for one hour between cleanups\\r\\n    const nextCleanup = moment.utc(context.df.currentUtcDateTime).add(1, \"h\");\\r\\n    yield context.df.createTimer(nextCleanup.toDate());\\r\\n\\r\\n    yield context.df.continueAsNew(undefined);\\r\\n});\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nimport azure.functions as func\\r\\nimport azure.durable_functions as df\\r\\nfrom datetime import datetime, timedelta\\r\\n\\r\\ndef orchestrator_function(context: df.DurableOrchestrationContext):\\r\\n    yield context.call_activity(\"DoCleanup\")\\r\\n\\r\\n    # sleep for one hour between cleanups\\r\\n    next_cleanup = context.current_utc_datetime + timedelta(hours = 1)\\r\\n    yield context.create_timer(next_cleanup)\\r\\n\\r\\n    context.continue_as_new(None)\\r\\n\\r\\nmain = df.Orchestrator.create(orchestrator_function)\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\nPowerShell doesn\\'t support *continue-as-new*.\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"Periodic_Cleanup_Loop\")\\r\\npublic void periodicCleanupLoop(\\r\\n        @DurableOrchestrationTrigger(name = \"ctx\") TaskOrchestrationContext ctx) {\\r\\n    ctx.callActivity(\"DoCleanup\").await();\\r\\n\\r\\n    ctx.createTimer(Duration.ofHours(1)).await();\\r\\n\\r\\n    ctx.continueAsNew(null);\\r\\n}\\r\\n```\\r\\n\\r\\nThe difference between this example and a timer-triggered function is that cleanup trigger times here are not based on a schedule. For example, a CRON schedule that executes a function every hour will execute it at 1:00, 2:00, 3:00 etc. and could potentially run into overlap issues. In this example, however, if the cleanup takes 30 minutes, then it will be scheduled at 1:00, 2:30, 4:00, etc. and there is no chance of overlap.\\r\\n\\r\\n## Starting an eternal orchestration\\r\\n\\r\\nUse the *start-new* or *schedule-new* durable client method to start an eternal orchestration, just like you would any other orchestration function.\\r\\n\\r\\nNote\\r\\n\\r\\nIf you need to ensure a singleton eternal orchestration is running, it\\'s important to maintain the same instance `id` when starting the orchestration. For more information, see [Instance Management](durable-functions-instance-management).\\r\\n\\r\\n**C#**\\r\\n\\r\\n```csharp\\r\\n[FunctionName(\"Trigger_Eternal_Orchestration\")]\\r\\npublic static async Task<HttpResponseMessage> OrchestrationTrigger(\\r\\n    [HttpTrigger(AuthorizationLevel.Function, \"post\", Route = null)] HttpRequestMessage request,\\r\\n    [DurableClient] IDurableOrchestrationClient client)\\r\\n{\\r\\n    string instanceId = \"StaticId\";\\r\\n\\r\\n    await client.StartNewAsync(\"Periodic_Cleanup_Loop\", instanceId); \\r\\n    return client.CreateCheckStatusResponse(request, instanceId);\\r\\n}\\r\\n```\\r\\n\\r\\nNote\\r\\n\\r\\nThe previous code is for Durable Functions 2.x. For Durable Functions 1.x, you must use `OrchestrationClient` attribute instead of the `DurableClient` attribute, and you must use the `DurableOrchestrationClient` parameter type instead of `IDurableOrchestrationClient`. For more information about the differences between versions, see the [Durable Functions versions](durable-functions-versions) article.\\r\\n\\r\\n**JavaScript**\\r\\n\\r\\n```javascript\\r\\nconst df = require(\"durable-functions\");\\r\\n\\r\\nmodule.exports = async function (context, req) {\\r\\n    const client = df.getClient(context);\\r\\n    const instanceId = \"StaticId\";\\r\\n    \\r\\n    // null is used as the input, since there is no input in \"Periodic_Cleanup_Loop\".\\r\\n    await client.startNew(\"Periodic_Cleanup_Loop\", instanceId, null);\\r\\n\\r\\n    context.log(`Started orchestration with ID = \\'${instanceId}\\'.`);\\r\\n    return client.createCheckStatusResponse(context.bindingData.req, instanceId);\\r\\n};\\r\\n```\\r\\n\\r\\n**Python**\\r\\n\\r\\n```python\\r\\nasync def main(req: func.HttpRequest, starter: str) -> func.HttpResponse:\\r\\n    client = df.DurableOrchestrationClient(starter)\\r\\n    instance_id = \\'StaticId\\'\\r\\n\\r\\n    await client.start_new(\\'Periodic_Cleanup_Loop\\', instance_id, None)\\r\\n\\r\\n    logging.info(f\"Started orchestration with ID = \\'{instance_id}\\'.\")\\r\\n    return client.create_check_status_response(req, instance_id)\\r\\n\\r\\n```\\r\\n\\r\\n**PowerShell**\\r\\nPowerShell doesn\\'t support *continue-as-new*.\\r\\n\\r\\n**Java**\\r\\n\\r\\n```java\\r\\n@FunctionName(\"Trigger_Eternal_Orchestration\")\\r\\npublic HttpResponseMessage triggerEternalOrchestration(\\r\\n        @HttpTrigger(name = \"req\") HttpRequestMessage<?> req,\\r\\n        @DurableClientInput(name = \"durableContext\") DurableClientContext durableContext) {\\r\\n\\r\\n    String instanceID = \"StaticID\";\\r\\n    DurableTaskClient client = durableContext.getClient();\\r\\n    client.scheduleNewOrchestrationInstance(\"Periodic_Cleanup_Loop\", null, instanceID);\\r\\n    return durableContext.createCheckStatusResponse(req, instanceID);\\r\\n}\\r\\n```\\r\\n\\r\\n## Exit from an eternal orchestration\\r\\n\\r\\nIf an orchestrator function needs to eventually complete, then all you need to do is *not* call `ContinueAsNew` and let the function exit.\\r\\n\\r\\nIf an orchestrator function is in an infinite loop and needs to be stopped, use the *terminate* API of the [orchestration client binding](durable-functions-bindings#orchestration-client) to stop it. For more information, see [Instance Management](durable-functions-instance-management).',\n",
       "  'metadata': {}}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import AzureOpenAI # verified in Python version 3.12.3, 3.12.4 \n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "import json\n",
    " \n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    " \n",
    "client = AzureOpenAI(\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    api_version=\"2024-10-21\",\n",
    "    azure_endpoint=\"https://discovery-eastus.openai.azure.com/\",\n",
    ")\n",
    "\n",
    "class QAPair(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class QAExtraction(BaseModel):\n",
    "    identified_content: list[str]\n",
    "    qa_pairs: list[QAPair]\n",
    "\n",
    "def generate_qa_from_gpt(\n",
    "    content: str,\n",
    ") -> QAExtraction:\n",
    "    \n",
    "    system_message = f\"\"\"  \n",
    "        Given a domain-specific document sourced from Windows public or private knowledge-base, your task is to generate question-and-answer pairs according to the following guidelines:  \n",
    "    \n",
    "        1. Thoroughly read and understand the document.  \n",
    "        \n",
    "        2. Identify and note ALL potential issues, questions, or common problems that can be inferred from the content and context of the document. These elements will serve as the basis for generating general troubleshooting queries, assistance requests, or informational inquiries relevant to customers.  \n",
    "        \n",
    "        3. Create question-and-answer pairs based on the given document and identified content:  \n",
    "            - **Questions** should represent typical customer inquiries, focusing on general troubleshooting, seeking assistance, or looking for information. They should be straightforward and avoid overly technical terms.  \n",
    "            - **Answers** should be detailed and informative. Each answer must begin with a reasoning process that provides background and domain knowledge related to the question, explaining the context and how to approach the issue. This helps the support team understand the problem. Follow this with insights and step-by-step instructions to address the issue, and conclude with the final solution based on the reasoning process.  \n",
    "            - Do not include information that is not present in the document.  \n",
    "            - Ensure that the question-and-answer pairs comprehensively cover ALL aspects of the document without redundancy. No additional pairs should be generated if they would introduce redundancy.  \n",
    "        \n",
    "        4. Respond in JSON format with the following structure:  \n",
    "            - Include the identified potential issues, questions, or common problems for general troubleshooting queries in a list under the key `identified_content`.  \n",
    "            - Include the question-and-answer pairs in a list under the key `qa_pairs`, using `question` and `answer` keys for each pair.  \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    # document_content = f\"\"\"\n",
    "    # App Service on Linux supports a number of language-specific built-in images. Just deploy your code. Supported languages include: Node.js, Java (Tomcat, JBoss, or with an embedded web server), PHP, Python, and .NET Core. Run az webapp list-runtimes --os linux to view the latest languages and supported versions. If the runtime your application requires isn't supported in the built-in images, you can deploy it with a custom container.\n",
    "\n",
    "    # Outdated runtimes are periodically removed from the Web Apps Create and Configuration blades in the portal. These runtimes are hidden from the portal when they're deprecated by the maintaining organization or found to have significant vulnerabilities. These options are hidden to guide customers to the latest runtimes, where they'll be the most successful.\n",
    "\n",
    "    # When an outdated runtime is hidden from the portal, any of your existing sites using that version will continue to run. If a runtime is fully removed from the App Service platform, your Azure subscription owner(s) will receive an email notice before the removal.\n",
    "\n",
    "    # If you need to create another web app with an outdated runtime version that's no longer shown on the portal, see the language configuration guides for instructions on how to get the runtime version of your site. You can use the Azure CLI to create another site with the same runtime. Alternatively, you can use the Export Template button on the web app blade in the portal to export an ARM template of the site. You can reuse this template to deploy a new site with the same runtime and configuration.\n",
    "    # \"\"\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini-2024-07-18-global\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ], \n",
    "        response_format=QAExtraction,\n",
    "    )\n",
    "\n",
    "    # print(completion.model_dump_json(indent=2))\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "queries = []\n",
    "index = 0\n",
    "for item in corpus:\n",
    "    # call gpt completions to generate questions\n",
    "    qa_result = generate_qa_from_gpt(item['text'])\n",
    "    for qa in qa_result.qa_pairs:\n",
    "        queries.append({\"_id\": f\"{index}\", \"text\": qa.question, \"metadata\": {f\"{item['_id']}\": [{\"sentences\":qa.answer, \"label\": \"\"}]}})\n",
    "        index += 1\n",
    "# save the queries to a jsonl file\n",
    "with open('./datasets/learncorpus/queries.jsonl', 'w') as f:\n",
    "    for item in queries:\n",
    "        # convert item to json \n",
    "        json.dump(item, f)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV file './datasets/learncorpus/qrels/test.tsv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# File name\n",
    "file_name = \"./datasets/learncorpus/qrels/test.tsv\"\n",
    "\n",
    "with open(file_name, 'w', encoding='utf8', newline='') as tsv_file:\n",
    "        tsv_writer = csv.writer(tsv_file, delimiter='\\t', lineterminator='\\n')\n",
    "        tsv_writer.writerow([\"query-id\", \"corpus-id\", \"score\"])\n",
    "        for queryItem in queries:\n",
    "            tsv_writer.writerow([queryItem['_id'], list(queryItem['metadata'].keys())[0], 1])\n",
    "\n",
    "print(f\"TSV file '{file_name}' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
